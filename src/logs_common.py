#
# This file is part of smlprover.
#
# Copyright 2020 Franz Brau√üe <franz.brausse@manchester.ac.uk>
# See the LICENSE file for terms of distribution.

# TODO !!!:  inst actually contains info required for logging, so maybe all feilds and paths/filenames
# definitions need to be moved into a logger module/class (which does not exist now).

import os, sys, json
import logging
print(logging.__path__)
from utils_common import str_to_bool

class DataFileInstance:
    # data_file_prefix is the name of the data file including the full path to
    #                  the file but excluding the .csv suffix
    # run_prefix is a uique identifier string for each run and is supposed to be
    #            used as a prefix of the reports/files written during the run
    # _filename_prefix is a string including the output directory and is supposed 
    #                  to be used as a path + prefix of all reports produced by a run
    # _out_dir defines directory where all reports / output files will be written.
    #          When it is not provided, the directory from which data was loaded
    #          will be used as the output directory
    # _new_data_file_prefix is path and filename of new data (unseen during training) w/o .csv suffix.
    #
    # We divide the files generated by an SMLP run into (1) reports and (2) saved model info.
    # The reports will all have the same prefix and different suffixes. As default, the prefix used
    # for the reports is generated as a concatination of the run_prefix and data_name_prefix, and
    # new_data_prefix if new data is provided. As default, the model name is used as the prefix in all
    # file names that record the saved model related information required to re-run the saved model
    # on a new data in the future (without using the training data used to generate the saved model). 
    # Besides the models themselves, these model related files include ones that record dataset scaling
    # info, features used in the model, levels of categorical features used in the model, and more.
    # (1) if data_name_prefix is provided and model_name is not, use data_name_prefix as the model_name
    # when generating names of files that store saved model information.
    # (2) if model_name is provided and data_name_prefix is not, use model_name as the data_name_prefix
    # to genrate names of files for all reports.
    # In each run of SMLP, at least one of data_name_prefix and model_name must be provided; otherwise 
    # an error is generated. If both are provided, than as described above data_name_prefix is used as the
    # prefix in report filenames and model_name is used as the prefix for the saved model related filenames.
    def __init__(self, data_file_prefix : str, run_prefix : str, output_directory : str=None, 
            new_data_file_prefix : str=None, model_name : str=None):
        # Define _data_dir and _data_name_prefix. data_file_prefix can be None say when
        # one wants to reuse a trained model for prediction on new data, or perform an
        # optimization or validation task on a trained model.
        if not data_file_prefix is None:
            self._data_dir, self._data_name_prefix = os.path.split(data_file_prefix)
        else:
            self._data_dir, self._data_name_prefix = None, None
        
        # Define _model_dir and _model_name_prefix. 
        if not model_name is None:
            self._model_dir, self._model_name_prefix = os.path.split(model_name)
        else:
            self._model_dir, self._model_name_prefix = None, None
        
        # assert that a training data file or a model should be provided'    
        if data_file_prefix is None and model_name is None:
            raise Exception('A training data file or a model should be provided')
        
        # define _output_dir: if it is not specified explicitly, reuse _data_dir or _model_dir
        self._out_dir = output_directory
        if self._out_dir is None:
            if not self._data_dir is None:
                self._out_dir = self._data_dir
            elif not self._model_dir is None:
                self._out_dir = self._model_dir
            else:
                raise Exception('A training data file or a model should be provided')      
        
        # define _run_prefix; it should be provided for each SMLP run, but if it is not provided
        # an error will not be issued and instead str(None) will be used as the prefix
        self._run_prefix = str(run_prefix)
        
        # define _new_data_file_prefix
        self._new_data_file_prefix = new_data_file_prefix
        
        # record model_name 
        self._model_name = model_name
        
        # define _model_name_prefix to be used as suffix in names of all files used to save the model related info
        if self._model_name is None:
            assert not self._data_name_prefix is None
            self._model_name_prefix = os.path.join(self._out_dir, self._run_prefix + '_' + self._data_name_prefix)
        else:
            self._model_name_prefix = os.path.join(self._out_dir, self._model_name)
        
        # define _report_name_prefix to be used as a prefix in SMLP report filenames
        if self._data_name_prefix is None:
            assert not self._model_name is None
            _, model_name = os.path.split(self._model_name)
            self._report_name_prefix = os.path.join(self._out_dir, self._run_prefix + '_' + model_name)
        else:
            self._report_name_prefix = os.path.join(self._out_dir, self._run_prefix + '_' + self._data_name_prefix)
                                             
        # if new_data is not None, its name is added to self._filename_prefix
        if not self._new_data_file_prefix is None:
            _, new_data_fname = os.path.split(self._new_data_file_prefix)
            self._report_name_prefix = self._report_name_prefix + '_' + new_data_fname
        
    # TODO !!!: self._report_name_prefix is used in train_caret.py despite the parameter name start with _
    # define and use an interface function instead: get_report_name_prefix(self, response, algo).
    # used als in train_common.py, train_sklearn.py
    def get_report_name_prefix(self, responses=None, algo=None):
        res = self._report_name_prefix
        if not responses is None:
            if len(responses) == 1:
                res = res + '_' + str(responses[0])
        if not algo is None:
            res = res + '_' + str(algo)
        return res

    def get_model_name_prefix(self, responses=None, algo=None):
        res = self._model_name_prefix
        if not responses is None:
            if len(responses) == 1:
                res = res + '_' + str(responses[0])
        if not algo is None:
            res = res + '_' + str(algo)
        return res
        '''
        if not data_file_prefix is None:
            self._dir, self._pre = os.path.split(data_file_prefix)
        elif not model_file_prefix is None:
            self._dir, self._pre = os.path.split(model_file_prefix)
        else:
            raise Exception('A training data file or a model should be provided')
        self._run_prefix = run_prefix
        self._out_dir = output_directory
        self._new_data_file_prefix = new_data_file_prefix
        
        if not self._out_dir is None:
            self._filename_prefix = os.path.join(self._out_dir, self._run_prefix + '_' + self._pre)
        else:
            self._filename_prefix = os.path.join(self._dir, self._run_prefix + '_' + self._pre)
        # if new_data is not None, its name is added to self._filename_prefix
        if not self._new_data_file_prefix is None:
            _, new_data_fname = os.path.split(self._new_data_file_prefix)
            self._filename_prefix = self._filename_prefix + '_' + new_data_fname
        '''
    '''
    # input/training data file name (including the directory path)
    @property
    def data_fname(self):
        return os.path.join(self._dir, self._pre + ".csv")
    '''
    @property
    def data_fname(self):
        if not self._data_dir is None and not self._data_name_prefix is None:
            return os.path.join(self._data_dir, self._data_name_prefix + ".csv")
        else:
            return None

    # new (unseen during training) data file name (including the directory path)
    @property
    def new_data_fname(self):
        if self._new_data_file_prefix is None:
            return None
        else:
            return self._new_data_file_prefix + ".csv"
    
    # filename with full path for logging verbosity and events during execution
    @property
    def log_file(self):
        return self._report_name_prefix + ".txt"
    
    # filename with full path for logging error message before aborting
    @property
    def error_file(self):
        return self._report_name_prefix + "_error.txt"
    
    # Saved neural network model in a specific tensorflow format (weights, metadata, etc.).
    # Generated using Keras model.save utility
    # The model can be also partially trained, and training can resume from that point.
    #@property
    #def model_file(self):
    #    return self._filename_prefix + "_model_complete.h5"
    #
    # saved sklearn or caret model filename - -saved using pickle.dump()
    #@property
    #def pickle_model_file(self):
    #    return self._filename_prefix + "_model_complete.pkl"
    
    # file to save NN Keras/tensorflow training / error convergence info, known as checkpoints
    @property
    def model_checkpoint_pattern(self):
        return self._model_name_prefix + "_model_checkpoint.h5"

    # Currently unused -- genrated in train_keras.py but not consumed.
    # Saved neural network model in json format -- alternative to info saved in model_fname).
    # Generated using Keras model.to_json() utility (alternative to Keras model.save utility)
    # The model can be also partially trained, and training can resume from that point.
    #@property
    #def model_config_file(self):
    #    return self._filename_prefix + "_model_config.json"

    # min/max info of all columns (features and reponses) in input data
    # (labeled data used for training and testing ML models)
    @property
    def data_bounds_file(self):
        return self._model_name_prefix + "_data_bounds.json"

    # TODO !!!: add description
    @property
    def model_gen_file(self):
        return self._model_name_prefix + "_model_gen.json"
    
    # TODO !!!: add description
    #@property
    #def poly_model_reg_file(self):
    #    return self._model_name_prefix + "_poly_model_reg.pkl"
    
    # saved min-max scaler filename for features
    @property
    def features_scaler_file(self):
        return self._model_name_prefix + "_features_scaler.pkl"
    
    # saved min-max scaler filename for responses
    @property
    def responses_scaler_file(self):
        return self._model_name_prefix + "_responses_scaler.pkl"
    
    # dictionary containing categorica feature names as keys and correponding levels as values.
    # Includes only categorical features that will be used in traing a model (a subset of original.
    # fetaures and features engineered from these features). This info is part of the saved model.
    @property
    def model_levels_dict_file(self):
        return self._model_name_prefix + "_model_levels_dict.json"
    
    @property
    def model_features_dict_file(self):
        return self._model_name_prefix + "_model_features_dict.json"
    
    # required for generating file names of the reports containing model prediction results;
    # might cover multiple models (algorithms like NN, DT, RF) as well as multiple responses
    def predictions_summary_filename(self, data_version):
        return self._report_name_prefix + '_' + data_version + "_predictions_summary.csv"
    
    # required for generating file names of the reports containing model precision (rmse, R2, etc.) reults;
    # might cover multiple models (algorithms like NN, DT, RF) as well as multiple responses
    def prediction_precisions_filename(self, data_version):
        return self._report_name_prefix + '_' + data_version + "_prediction_precisions.csv"
        
    # saved model file name -- model_algo is the training algo name like nn_keras, sklearn_dt;
    # model_format is the format used to save the model -- for nn_keras .h5 format is used,
    # (implies suffix .h5); for sklearn and caret models pickle format is used (implies suffix .pkl)
    def model_fname(self, model_algo, model_format, response=None):
        if response is None:
            return self._model_name_prefix + '_' + model_algo + "_model_complete" + model_format
        else:
            return self._model_name_prefix + '_' + str(response) + '_' + model_algo + "_model_complete" + model_format
        
    
# LOGGER sources:
# https://docs.python.org/3/howto/logging.html
# https://stackoverflow.com/questions/29087297/is-there-a-way-to-change-the-filemode-for-a-logger-object-that-is-not-configured/29087645#29087645
# create python logger object with desired configuration ; define the default configuration logger_params_dict
class SmlpLogger:
    def __init__(self):
        self._DEF_LOGGER_LEVEL = 'warning'
        self._DEF_LOGGER_FMODE = 'w'
        self._DEF_LOGGER_TIME = 'true'

        self.logger_params_dict = {
            'log_level': {'abbr':'log_level', 'default':'info', 'type':str,
                'help':'The logger level or severity of the events they are used to track. The standard levels ' + 
                    'are (in increasing order of severity): notset, debug, info, warning, error, critical; ' +
                    'only events of this level and above will be tracked [default {}]'.format(self._DEF_LOGGER_LEVEL)}, 
            'log_mode': {'abbr':'log_mode', 'default':'w', 'type':str,
                'help':'The logger filemode for logging into log file [default {}]'.format(self._DEF_LOGGER_FMODE)},
            'log_time': {'abbr':'log_time', 'default':self._DEF_LOGGER_TIME, 'type':str_to_bool,
                'help':'Should time stamp be logged along with every message issued by logger [default {}]'.format(self._DEF_LOGGER_TIME)}}

    # create python logger object with desired configuration 
    def create_logger(self, logger_name, log_file, log_level, log_mode, log_time):
        # create logger for an application called logger_name
        logger = logging.getLogger(logger_name)

        def log_level_to_level_object(level_str):
            if level_str == 'critical':
                return logging.CRITICAL
            elif level_str == 'error':
                return logging.ERROR
            elif level_str == 'warning':
                return logging.WARNING
            elif level_str == 'info':
                return logging.INFO
            elif level_str == 'debug':
                return logging.DEBUG
            elif level_str == 'notset':
                return logging.NOTSET
            else:
                raise Exception('Unsupported logging level {}'.format(log_level))

        log_level_object = log_level_to_level_object(log_level)
        # set the logging level 
        logger.setLevel(log_level_object)

        # create file handler which logs even debug messages
        fh = logging.FileHandler(log_file, mode=log_mode)
        fh.setLevel(log_level_object)

        # create console handler with a higher log level
        ch = logging.StreamHandler(stream=sys.stdout)
        ch.setLevel(log_level_object)

        # create formatter and add it to the handlers
        if log_time:
            formatter = logging.Formatter('\n%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        else:
            formatter = logging.Formatter('\n%(name)s - %(levelname)s - %(message)s')
        #formatter = logging.Formatter('[%(asctime)s] %(levelname)8s --- %(message)s ' +
        #                             '(%(filename)s:%(lineno)s)',datefmt='%Y-%m-%d %H:%M:%S')
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)

        # add the handlers to the logger
        logger.addHandler(ch)
        logger.addHandler(fh)

        return logger


