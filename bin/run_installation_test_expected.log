Script is running inside a Docker container.
2026-02-08 10:39:55.640415: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2026-02-08 10:39:56.909453: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2026-02-08 10:39:56.909584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-02-08 10:39:57.061866: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2026-02-08 10:39:57.645863: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2026-02-08 10:39:57.649740: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-02-08 10:40:00.557679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
usage: smlp/src/run_smlp.py [-h] [-model MODEL] [-save_model SAVE_MODEL]
                            [-use_model USE_MODEL] [-model_name MODEL_NAME]
                            [-save_model_config SAVE_MODEL_RERUN_CONFIGURATION]
                            [-model_per_response MODEL_PER_RESPONSE]
                            [-pred_plots PREDICTION_PLOTS]
                            [-nn_keras_layers NN_KERAS_LAYERS]
                            [-nn_keras_epochs NN_KERAS_EPOCHS]
                            [-nn_keras_batch NN_KERAS_BATCH_SIZE]
                            [-nn_keras_optimizer NN_KERAS_OPTIMIZER]
                            [-nn_keras_learning_rate NN_KERAS_LEARNING_RATE]
                            [-nn_keras_loss NN_KERAS_LOSS_FUNCTION]
                            [-nn_keras_metrics NN_KERAS_METRICS]
                            [-nn_keras_hid_activation NN_KERAS_HID_ACTIVATION]
                            [-nn_keras_out_activation NN_KERAS_OUT_ACTIVATION]
                            [-nn_keras_seq_api NN_KERAS_SEQUENTIAL_API]
                            [-nn_keras_weights_precision NN_KERAS_WEIGHTS_PRECISION]
                            [-nn_keras_tuner NN_KERAS_TUNER_ALGO]
                            [-nn_keras_layers_grid NN_KERAS_LAYERS_GRID]
                            [-nn_keras_batches_grid NN_KERAS_BATCHES_GRID]
                            [-nn_keras_lrates_grid NN_KERAS_LEARNING_RATES_GRID]
                            [-nn_keras_losses_grid NN_KERAS_LOSS_FUNCTIONS_GRID]
                            [-poly_sklearn_degree POLY_SKLEARN_DEGREE]
                            [-poly_sklearn_fit_intercept POLY_SKLEARN_FIT_INTERCEPT]
                            [-poly_sklearn_copy_X POLY_SKLEARN_COPY_X]
                            [-poly_sklearn_n_jobs POLY_SKLEARN_N_JOBS]
                            [-poly_sklearn_positive POLY_SKLEARN_POSITIVE]
                            [-dt_sklearn_splitter DT_SKLEARN_SPLITTER]
                            [-dt_sklearn_max_features DT_SKLEARN_MAX_FEATURES]
                            [-dt_sklearn_rand_state DT_SKLEARN_RANDOM_STATE]
                            [-dt_sklearn_criterion DT_SKLEARN_CRITERION]
                            [-dt_sklearn_max_depth DT_SKLEARN_MAX_DEPTH]
                            [-dt_sklearn_min_samples_split DT_SKLEARN_MIN_SAMPLES_SPLIT]
                            [-dt_sklearn_min_samples_leaf DT_SKLEARN_MIN_SAMPLES_LEAF]
                            [-dt_sklearn_min_weight_fraction_leaf DT_SKLEARN_MIN_WEIGHT_FRACTION_LEAF]
                            [-dt_sklearn_max_leaf_nodes DT_SKLEARN_MAX_LEAF_NODES]
                            [-dt_sklearn_min_impurity_decrease DT_SKLEARN_MIN_IMPURITY_DECREASE]
                            [-dt_sklearn_ccp_alpha DT_SKLEARN_CCP_ALPHA]
                            [-rf_sklearn_n_estimators RF_SKLEARN_N_ESTIMATORS]
                            [-rf_sklearn_max_features RF_SKLEARN_MAX_FEATURES]
                            [-rf_sklearn_bootstrap RF_SKLEARN_BOOTSTRAP]
                            [-rf_sklearn_verbose RF_SKLEARN_VERBOSE]
                            [-rf_sklearn_warm_start RF_SKLEARN_WARM_START]
                            [-rf_sklearn_max_samples RF_SKLEARN_MAX_SAMPLES]
                            [-rf_sklearn_rand_state RF_SKLEARN_RANDOM_STATE]
                            [-rf_sklearn_criterion RF_SKLEARN_CRITERION]
                            [-rf_sklearn_max_depth RF_SKLEARN_MAX_DEPTH]
                            [-rf_sklearn_min_samples_split RF_SKLEARN_MIN_SAMPLES_SPLIT]
                            [-rf_sklearn_min_samples_leaf RF_SKLEARN_MIN_SAMPLES_LEAF]
                            [-rf_sklearn_min_weight_fraction_leaf RF_SKLEARN_MIN_WEIGHT_FRACTION_LEAF]
                            [-rf_sklearn_max_leaf_nodes RF_SKLEARN_MAX_LEAF_NODES]
                            [-rf_sklearn_min_impurity_decrease RF_SKLEARN_MIN_IMPURITY_DECREASE]
                            [-rf_sklearn_ccp_alpha RF_SKLEARN_CCP_ALPHA]
                            [-et_sklearn_n_estimators ET_SKLEARN_N_ESTIMATORS]
                            [-et_sklearn_max_features ET_SKLEARN_MAX_FEATURES]
                            [-et_sklearn_bootstrap ET_SKLEARN_BOOTSTRAP]
                            [-et_sklearn_verbose ET_SKLEARN_VERBOSE]
                            [-et_sklearn_warm_start ET_SKLEARN_WARM_START]
                            [-et_sklearn_max_samples ET_SKLEARN_MAX_SAMPLES]
                            [-et_sklearn_rand_state ET_SKLEARN_RANDOM_STATE]
                            [-et_sklearn_criterion ET_SKLEARN_CRITERION]
                            [-et_sklearn_max_depth ET_SKLEARN_MAX_DEPTH]
                            [-et_sklearn_min_samples_split ET_SKLEARN_MIN_SAMPLES_SPLIT]
                            [-et_sklearn_min_samples_leaf ET_SKLEARN_MIN_SAMPLES_LEAF]
                            [-et_sklearn_min_weight_fraction_leaf ET_SKLEARN_MIN_WEIGHT_FRACTION_LEAF]
                            [-et_sklearn_max_leaf_nodes ET_SKLEARN_MAX_LEAF_NODES]
                            [-et_sklearn_min_impurity_decrease ET_SKLEARN_MIN_IMPURITY_DECREASE]
                            [-et_sklearn_ccp_alpha ET_SKLEARN_CCP_ALPHA]
                            [-setup_caret_session_id SETUP_CARET_SESSION_ID]
                            [-setup_caret_fold SETUP_CARET_FOLD]
                            [-setup_caret_data_split_shuffle SETUP_CARET_DATA_SPLIT_SHUFFLE]
                            [-setup_caret_verbose SETUP_CARET_VERBOSE]
                            [-model_caret_cross_validation MODEL_CARET_CROSS_VALIDATION]
                            [-model_caret_verbose MODEL_CARET_VERBOSE]
                            [-model_caret_return_train_score MODEL_CARET_RETURN_TRAIN_SCORE]
                            [-tuner_caret_search_algo TUNER_CARET_SEARCH_ALGORITHM]
                            [-tuner_caret_tuner_verbose TUNER_CARET_TUNER_VERBOSE]
                            [-resp RESPONSE] [-feat FEATURES]
                            [-keep_feat KEEP_FEATURES] [-new_data NEW_DATA]
                            [-data_scaler DATA_SCALER]
                            [-scale_feat SCALE_FEATURES]
                            [-scale_resp SCALE_RESPONSES]
                            [-impute_resp IMPUTE_RESPONSES]
                            [-split SPLIT_TEST] [-train_rand TRAIN_RANDOM_N]
                            [-train_first TRAIN_FIRST_N]
                            [-train_unif TRAIN_UNIFORM_N]
                            [-sw_coef SAMPLE_WEIGHTS_COEF]
                            [-sw_exp SAMPLE_WEIGHTS_EXPONENT]
                            [-sw_int SAMPLE_WEIGHTS_INTERCEPT]
                            [-respmap RESPONSE_MAP] [-resp2b RESPONSE_TO_BOOL]
                            [-pos_val POSITIVE_VALUE]
                            [-neg_val NEGATIVE_VALUE]
                            [-resp_plots RESPONSE_PLOTS]
                            [-mrmr_pred MRMR_FEAT_COUNT_FOR_PREDICTION]
                            [-mrmr_corr MRMR_FEAT_COUNT_FOR_CORRELATION]
                            [-data LABELED_DATA] [-mode ANALYTICS_MODE]
                            [-plots INTERACTIVE_PLOTS] [-seed SEED]
                            [-pref LOG_FILES_PREFIX]
                            [-out_dir OUTPUT_DIRECTORY]
                            [-save_config SAVE_CONFIGURATION]
                            [-config LOAD_CONFIGURATION]
                            [-log_level LOG_LEVEL] [-log_mode LOG_MODE]
                            [-log_time LOG_TIME] [-doe_algo DOE_ALGO]
                            [-doe_factor_level_ranges DOE_FACTOR_LEVEL_RANGES]
                            [-doe_samples DOE_NUM_SAMPLES]
                            [-doe_resolution DOE_DESIGN_RESOLUTION]
                            [-doe_spec DOE_SPEC_FILE]
                            [-doe_bb_centers DOE_BOX_BEHNKEN_CENTERS]
                            [-doe_cc_center DOE_CENTRAL_COMPOSITE_CENTER]
                            [-doe_cc_alpha DOE_CENTRAL_COMPOSITE_ALPHA]
                            [-doe_cc_face DOE_CENTRAL_COMPOSITE_FACE]
                            [-doe_prob_distr DOE_PROB_DISTRIBUTION]
                            [-discr_algo DISCRETIZATION_ALGO]
                            [-discr_bins DISCRETIZATION_BINS]
                            [-discr_labels DISCRETIZATION_LABELS]
                            [-discr_type DISCRETIZATION_TYPE]
                            [-mi_method MUTUAL_INFORMATION_METHOD]
                            [-corr_and_mi CORRELATIONS_AND_MUTUAL_INFORMATION]
                            [-discret_num DISCRETIZE_NUMERIC_FEATURES]
                            [-cont_est CONTINUOUS_CORRELATION_ESTIMATORS]
                            [-psg_quality PSG_QUALITY_TARGET]
                            [-psg_dim PSG_MAX_DIMENSION]
                            [-psg_top PSG_TOP_RANKED] [-spec SPEC]
                            [-delta_rel DELTA_RELATIVE]
                            [-delta_abs DELTA_ABSOLUTE]
                            [-rad_rel RADIUS_RELATIVE]
                            [-rad_abs RADIUS_ABSOLUTE] [-alpha ALPHA]
                            [-beta BETA] [-eta ETA]
                            [-compress_rules COMPRESS_RULES]
                            [-simplify_terms SIMPLIFY_TERMS]
                            [-tree_encoding TREE_ENCODING]
                            [-nnet_encoding NNET_ENCODING]
                            [-trace_runtime TRACE_RUNTIME]
                            [-trace_prec TRACE_PRECISION]
                            [-trace_anonym TRACE_ANONYMIZE]
                            [-quer_names QUERY_NAMES]
                            [-quer_exprs QUERY_EXPRESSIONS]
                            [-lemma_prec LEMMA_PRECISION]
                            [-asrt_names ASSERTIONS_NAMES]
                            [-asrt_exprs ASSERTIONS_EXPRESSIONS]
                            [-epsilon EPSILON] [-center_offset CENTER_OFFSET]
                            [-objv_names OBJECTIVES_NAMES]
                            [-objv_exprs OBJECTIVES_EXPRESSIONS]
                            [-scale_objv SCALE_OBJECTIVES]
                            [-pareto OPTIMIZE_PARETO]
                            [-frac_aprox APPROXIMATE_FRACTIONS]
                            [-frac_prec FRACTION_PRECISION]
                            [-vacuity VACUITY_CHECK]
                            [-opt_strategy OPTIMIZATION_STRATEGY]
                            [-solver SOLVER] [-solver_path SOLVER_PATH]
                            [-solver_logic SOLVER_LOGIC]

options:
  -h, --help            show this help message and exit
  -model MODEL, --model MODEL
                        Type of model to train (NN, Poly, ... [default: none]
  -save_model SAVE_MODEL, --save_model SAVE_MODEL
                        Should the trained models be saved for future use?
                        [default: True]
  -use_model USE_MODEL, --use_model USE_MODEL
                        Should the saved models be reused (and training
                        skipped)? [default: False]
  -model_name MODEL_NAME, --model_name MODEL_NAME
                        Name of saved model. If not specified, the name is
                        defined as follows: filename_prefix + "_" + model_algo
                        + "_model_complete" + model_format where
                        filename_prefix is concatenation of the output
                        directory and the prefix identifying the run,
                        model_algo is the training algo name and model_format
                        is .h5 for nn_keras and .pkl for models trained using
                        sklearn and keras packages.
  -save_model_config SAVE_MODEL_RERUN_CONFIGURATION, --save_model_rerun_configuration SAVE_MODEL_RERUN_CONFIGURATION
                        Should a config file enabling to re-run a saved model
                        be written out? [default: True]
  -model_per_response MODEL_PER_RESPONSE, --model_per_response MODEL_PER_RESPONSE
                        Should a separate model, possible with a different,
                        dedicated feature set, be built per response (as
                        opposite to building one multi-response
                        model)?[default: False]
  -pred_plots PREDICTION_PLOTS, --prediction_plots PREDICTION_PLOTS
                        Should response distribution plots and plots comparing
                        response values in data with the predicted values be
                        generated? A related option interactive_plots controls
                        whether the generated plots should be displayed
                        interactively during runtime [default: True]
  -nn_keras_layers NN_KERAS_LAYERS, --nn_keras_layers NN_KERAS_LAYERS
                        specify number and sizes of the hidden layers of the
                        NN as non-empty, comma-separated list of positive
                        fractions in the number of input features in, e.g.
                        "0.5,0.25" specifies the second layer of half input
                        size, third layer of quarter input size (the input
                        layer has one node per input) [default: 2,1]
  -nn_keras_epochs NN_KERAS_EPOCHS, --nn_keras_epochs NN_KERAS_EPOCHS
                        epochs for NN [default: 2000]
  -nn_keras_batch NN_KERAS_BATCH_SIZE, --nn_keras_batch_size NN_KERAS_BATCH_SIZE
                        batch_size for NN [default: not exposed]
  -nn_keras_optimizer NN_KERAS_OPTIMIZER, --nn_keras_optimizer NN_KERAS_OPTIMIZER
                        optimizer for NN [default: adam]
  -nn_keras_learning_rate NN_KERAS_LEARNING_RATE, --nn_keras_learning_rate NN_KERAS_LEARNING_RATE
                        optimizer for NN [default: 0.001]
  -nn_keras_loss NN_KERAS_LOSS_FUNCTION, --nn_keras_loss_function NN_KERAS_LOSS_FUNCTION
                        The loss function for NN training convergence.
                        Possible options are: "mse" (MeanSquaredError), "mae"
                        (MeanAbsoluteError), "mspe"
                        (MeanAbsolutePercentageError) "msle"
                        (MeanSquaredLogarithmicError), "huber" (Huber),
                        "logcosh" (LogCosh) [default: mse]
  -nn_keras_metrics NN_KERAS_METRICS, --nn_keras_metrics NN_KERAS_METRICS
                        The metrics for NN training convergence. Possible
                        options are: "rmse (RootMeanSquaredError), "mse"
                        (MeanSquaredError), "mae" (MeanAbsoluteError), "mspe"
                        (MeanAbsolutePercentageError) "msle"
                        (MeanSquaredLogarithmicError), "logcosh"
                        (LogCoshError), and "cosine" (CosineSimilarity)
                        [default: ['mse']]
  -nn_keras_hid_activation NN_KERAS_HID_ACTIVATION, --nn_keras_hid_activation NN_KERAS_HID_ACTIVATION
                        hidden layer activation for NN [default: relu]
  -nn_keras_out_activation NN_KERAS_OUT_ACTIVATION, --nn_keras_out_activation NN_KERAS_OUT_ACTIVATION
                        output layer activation for NN [default: linear]
  -nn_keras_seq_api NN_KERAS_SEQUENTIAL_API, --nn_keras_sequential_api NN_KERAS_SEQUENTIAL_API
                        Should sequential api be used building NN layers or
                        should functional api be used instead? [default: True]
  -nn_keras_weights_precision NN_KERAS_WEIGHTS_PRECISION, --nn_keras_weights_precision NN_KERAS_WEIGHTS_PRECISION
                        Decimal precison (theat is, decimal points after the
                        dot) to use for rounding model weights (after a NN
                        model has been trained). The default value {} implies
                        that weight will not be rounded [default: linear]
  -nn_keras_tuner NN_KERAS_TUNER_ALGO, --nn_keras_tuner_algo NN_KERAS_TUNER_ALGO
                        NN Keras tuner algorithm to be invoked. Supported
                        options are hyperband (Hyperband), bayesian
                        (BayesianOptimization) and random (RandomSearch). The
                        option value None indicates that keras tuner will not
                        be invoked [default: None]
  -nn_keras_layers_grid NN_KERAS_LAYERS_GRID, --nn_keras_layers_grid NN_KERAS_LAYERS_GRID
                        Semicolon separated list of NN Keras layers
                        specifications, to be used by Keras tuner. Each such
                        specification itself is a comma separated list of
                        numbers, see the layers options for a detailed
                        description [default: None]
  -nn_keras_batches_grid NN_KERAS_BATCHES_GRID, --nn_keras_batches_grid NN_KERAS_BATCHES_GRID
                        Comma separated list of NN Keras batch sizes, to be
                        used by Keras tuner. [default: None]
  -nn_keras_lrates_grid NN_KERAS_LEARNING_RATES_GRID, --nn_keras_learning_rates_grid NN_KERAS_LEARNING_RATES_GRID
                        Comma separated list of NN Keras learning rates, to be
                        used by Keras tuner. [default: None]
  -nn_keras_losses_grid NN_KERAS_LOSS_FUNCTIONS_GRID, --nn_keras_loss_functions_grid NN_KERAS_LOSS_FUNCTIONS_GRID
                        Comma separated list of NN Keras loss functions, to be
                        used by Keras tuner. It can be a subset of loss
                        functions mse, mae, mape, msle, huber, logcosh.
                        [default: None]
  -poly_sklearn_degree POLY_SKLEARN_DEGREE, --poly_sklearn_degree POLY_SKLEARN_DEGREE
                        Degree of the polynomial to train [default: 2]
  -poly_sklearn_fit_intercept POLY_SKLEARN_FIT_INTERCEPT, --poly_sklearn_fit_intercept POLY_SKLEARN_FIT_INTERCEPT
                        Whether to calculate the intercept for this model. If
                        set to False, no intercept will be used in
                        calculations (i.e. data is expected to be centered).
                        [default: True]
  -poly_sklearn_copy_X POLY_SKLEARN_COPY_X, --poly_sklearn_copy_X POLY_SKLEARN_COPY_X
                        If True, X will be copied; else, it may be
                        overwritten. [default: True]
  -poly_sklearn_n_jobs POLY_SKLEARN_N_JOBS, --poly_sklearn_n_jobs POLY_SKLEARN_N_JOBS
                        The number of jobs to use for the computation. This
                        will only provide speedup in case of sufficiently
                        large problems, that is if firstly n_targets > 1 and
                        secondly X is sparse or if positive is set to True.
                        None means 1 unless in a joblib.parallel_backend
                        context. -1 means using all processors [default: None]
  -poly_sklearn_positive POLY_SKLEARN_POSITIVE, --poly_sklearn_positive POLY_SKLEARN_POSITIVE
                        When set to True, forces the coefficients to be
                        positive. This option is only supported for dense
                        arrays. [default: False]
  -dt_sklearn_splitter DT_SKLEARN_SPLITTER, --dt_sklearn_splitter DT_SKLEARN_SPLITTER
                        The strategy used to choose the split at each node.
                        Supported strategies are “best” to choose the best
                        split and “random” to choose the best random split
                        [default: best]
  -dt_sklearn_max_features DT_SKLEARN_MAX_FEATURES, --dt_sklearn_max_features DT_SKLEARN_MAX_FEATURES
                        The number of features to consider when looking for
                        the best split: If int, then consider max_features
                        features at each split. If float, max_features is a
                        fraction and max(1, int(max_features *
                        n_features_in_)) features are considered at each
                        split. If “sqrt”, then max_features=sqrt(n_features).
                        If “log2”, then max_features=log2(n_features). If
                        None, then max_features=n_features. [default: None]
  -dt_sklearn_rand_state DT_SKLEARN_RANDOM_STATE, --dt_sklearn_random_state DT_SKLEARN_RANDOM_STATE
                        Controls the randomness of the estimator. The features
                        are always randomly permuted at each split, even if
                        splitter is set to "best". When max_features <
                        n_features, the algorithm will select max_features at
                        random at each split before finding the best split
                        among them. But the best found split may vary across
                        different runs, even if max_features=n_features. That
                        is the case, if the improvement of the criterion is
                        identical for several splits and one split has to be
                        selected at random. To obtain a deterministic
                        behaviour during fitting, random_state has to be fixed
                        to an integer. [default: None]
  -dt_sklearn_criterion DT_SKLEARN_CRITERION, --dt_sklearn_criterion DT_SKLEARN_CRITERION
                        The function to measure the quality of a split.
                        Supported criteria are “squared_error” for the mean
                        squared error, which is equal to variance reduction as
                        feature selection criterion and minimizes the L2 loss
                        using the mean of each terminal node, “friedman_mse”,
                        which uses mean squared error with Friedman’s
                        improvement score for potential splits,
                        “absolute_error” for the mean absolute error, which
                        minimizes the L1 loss using the median of each
                        terminal node, and “poisson” which uses reduction in
                        Poisson deviance to find splits. Training using
                        “absolute_error” is slower than when using
                        “squared_error”. [default: squared_error]
  -dt_sklearn_max_depth DT_SKLEARN_MAX_DEPTH, --dt_sklearn_max_depth DT_SKLEARN_MAX_DEPTH
                        The maximum depth of the tree. If None, then nodes are
                        expanded until all leaves are pure or until all leaves
                        contain less than min_samples_split samples. [default:
                        None]
  -dt_sklearn_min_samples_split DT_SKLEARN_MIN_SAMPLES_SPLIT, --dt_sklearn_min_samples_split DT_SKLEARN_MIN_SAMPLES_SPLIT
                        The minimum number of samples required to split an
                        internal node.If int, then consider min_samples_split
                        as the minimum number. If float, min_samples_split is
                        a fraction and ceil(min_samples_split * n_samples) is
                        the minimum number of samples for each split.
                        [default: 2]
  -dt_sklearn_min_samples_leaf DT_SKLEARN_MIN_SAMPLES_LEAF, --dt_sklearn_min_samples_leaf DT_SKLEARN_MIN_SAMPLES_LEAF
                        The minimum number of samples required to be at a leaf
                        node. If int, then consider min_samples_leaf as the
                        minimum number. If float, min_samples_leaf is a
                        fraction and ceil(min_samples_leaf * n_samples) is the
                        minimum number of samples for each node. [default: 1]
  -dt_sklearn_min_weight_fraction_leaf DT_SKLEARN_MIN_WEIGHT_FRACTION_LEAF, --dt_sklearn_min_weight_fraction_leaf DT_SKLEARN_MIN_WEIGHT_FRACTION_LEAF
                        The minimum weighted fraction of the sum total of
                        weights (of all the input samples) required to be at a
                        leaf node. Samples have equal weight when
                        sample_weight is not provided. [default: 0.0]
  -dt_sklearn_max_leaf_nodes DT_SKLEARN_MAX_LEAF_NODES, --dt_sklearn_max_leaf_nodes DT_SKLEARN_MAX_LEAF_NODES
                        Grow a tree with max_leaf_nodes in best-first fashion.
                        Best nodes are defined as relative reduction in
                        impurity. If None then unlimited number of leaf nodes
                        [default: None]
  -dt_sklearn_min_impurity_decrease DT_SKLEARN_MIN_IMPURITY_DECREASE, --dt_sklearn_min_impurity_decrease DT_SKLEARN_MIN_IMPURITY_DECREASE
                        A node will be split if this split induces a decrease
                        of the impurity greater than or equal to this value
                        N_t / N * (impurity - N_t_R / N_t * right_impurity -
                        N_t_L / N_t * left_impurity), where N is the total
                        number of samples, N_t is the number of samples at the
                        current node, N_t_L is the number of samples in the
                        left child, and N_t_R is the number of samples in the
                        right child. N, N_t, N_t_R and N_t_L all refer to the
                        weighted sum, if sample_weight is passed. [default:
                        0.0]
  -dt_sklearn_ccp_alpha DT_SKLEARN_CCP_ALPHA, --dt_sklearn_ccp_alpha DT_SKLEARN_CCP_ALPHA
                        Complexity parameter used for Minimal Cost-Complexity
                        Pruning. The subtree with the largest cost complexity
                        that is smaller than ccp_alpha will be chosen. By
                        default, no pruning is performed. [default: 0.0]
  -rf_sklearn_n_estimators RF_SKLEARN_N_ESTIMATORS, --rf_sklearn_n_estimators RF_SKLEARN_N_ESTIMATORS
                        The number of trees in the forest. [default: 100]
  -rf_sklearn_max_features RF_SKLEARN_MAX_FEATURES, --rf_sklearn_max_features RF_SKLEARN_MAX_FEATURES
                        The number of features to consider when looking for
                        the best split: If int, then consider max_features
                        features at each split. If float, max_features is a
                        fraction and max(1, int(max_features *
                        n_features_in_)) features are considered at each
                        split, where n_features_in_ is the number of features
                        seen during fit. If “sqrt”, then
                        max_features=sqrt(n_features). If “log2”, then
                        max_features=log2(n_features). If None or 1.0, then
                        max_features=n_features. [default: 1.0]
  -rf_sklearn_bootstrap RF_SKLEARN_BOOTSTRAP, --rf_sklearn_bootstrap RF_SKLEARN_BOOTSTRAP
                        Whether bootstrap samples are used when building
                        trees. If False, the whole dataset is used to build
                        each tree [default: True]
  -rf_sklearn_verbose RF_SKLEARN_VERBOSE, --rf_sklearn_verbose RF_SKLEARN_VERBOSE
                        Controls the verbosity when fitting and predicting.
                        [default: 0]
  -rf_sklearn_warm_start RF_SKLEARN_WARM_START, --rf_sklearn_warm_start RF_SKLEARN_WARM_START
                        When set to True, reuse the solution of the previous
                        call to fit and add more estimators to the ensemble,
                        otherwise, just fit a whole new forest [default:
                        False]
  -rf_sklearn_max_samples RF_SKLEARN_MAX_SAMPLES, --rf_sklearn_max_samples RF_SKLEARN_MAX_SAMPLES
                        If bootstrap is True, the number of samples to draw
                        from X to train each base estimator. If None
                        (default), then draw X.shape[0] samples. If int, then
                        draw max_samples samples.If float, then draw
                        max(round(n_samples * max_samples), 1) samples. Thus,
                        max_samples should be in the interval (0.0, 1.0].
                        [default: None]
  -rf_sklearn_rand_state RF_SKLEARN_RANDOM_STATE, --rf_sklearn_random_state RF_SKLEARN_RANDOM_STATE
                        Controls both the randomness of the bootstrapping of
                        the samples used when building trees (if
                        bootstrap=True) and the sampling of the features to
                        consider when looking for the best split at each node
                        (if max_features < n_features). [default: None]
  -rf_sklearn_criterion RF_SKLEARN_CRITERION, --rf_sklearn_criterion RF_SKLEARN_CRITERION
                        The function to measure the quality of a split.
                        Supported criteria are “squared_error” for the mean
                        squared error, which is equal to variance reduction as
                        feature selection criterion and minimizes the L2 loss
                        using the mean of each terminal node, “friedman_mse”,
                        which uses mean squared error with Friedman’s
                        improvement score for potential splits,
                        “absolute_error” for the mean absolute error, which
                        minimizes the L1 loss using the median of each
                        terminal node, and “poisson” which uses reduction in
                        Poisson deviance to find splits. Training using
                        “absolute_error” is slower than when using
                        “squared_error”. [default: squared_error]
  -rf_sklearn_max_depth RF_SKLEARN_MAX_DEPTH, --rf_sklearn_max_depth RF_SKLEARN_MAX_DEPTH
                        The maximum depth of the tree. If None, then nodes are
                        expanded until all leaves are pure or until all leaves
                        contain less than min_samples_split samples. [default:
                        None]
  -rf_sklearn_min_samples_split RF_SKLEARN_MIN_SAMPLES_SPLIT, --rf_sklearn_min_samples_split RF_SKLEARN_MIN_SAMPLES_SPLIT
                        The minimum number of samples required to split an
                        internal node.If int, then consider min_samples_split
                        as the minimum number. If float, min_samples_split is
                        a fraction and ceil(min_samples_split * n_samples) is
                        the minimum number of samples for each split.
                        [default: 2]
  -rf_sklearn_min_samples_leaf RF_SKLEARN_MIN_SAMPLES_LEAF, --rf_sklearn_min_samples_leaf RF_SKLEARN_MIN_SAMPLES_LEAF
                        The minimum number of samples required to be at a leaf
                        node. If int, then consider min_samples_leaf as the
                        minimum number. If float, min_samples_leaf is a
                        fraction and ceil(min_samples_leaf * n_samples) is the
                        minimum number of samples for each node. [default: 1]
  -rf_sklearn_min_weight_fraction_leaf RF_SKLEARN_MIN_WEIGHT_FRACTION_LEAF, --rf_sklearn_min_weight_fraction_leaf RF_SKLEARN_MIN_WEIGHT_FRACTION_LEAF
                        The minimum weighted fraction of the sum total of
                        weights (of all the input samples) required to be at a
                        leaf node. Samples have equal weight when
                        sample_weight is not provided. [default: 0.0]
  -rf_sklearn_max_leaf_nodes RF_SKLEARN_MAX_LEAF_NODES, --rf_sklearn_max_leaf_nodes RF_SKLEARN_MAX_LEAF_NODES
                        Grow a tree with max_leaf_nodes in best-first fashion.
                        Best nodes are defined as relative reduction in
                        impurity. If None then unlimited number of leaf nodes
                        [default: None]
  -rf_sklearn_min_impurity_decrease RF_SKLEARN_MIN_IMPURITY_DECREASE, --rf_sklearn_min_impurity_decrease RF_SKLEARN_MIN_IMPURITY_DECREASE
                        A node will be split if this split induces a decrease
                        of the impurity greater than or equal to this value
                        N_t / N * (impurity - N_t_R / N_t * right_impurity -
                        N_t_L / N_t * left_impurity), where N is the total
                        number of samples, N_t is the number of samples at the
                        current node, N_t_L is the number of samples in the
                        left child, and N_t_R is the number of samples in the
                        right child. N, N_t, N_t_R and N_t_L all refer to the
                        weighted sum, if sample_weight is passed. [default:
                        0.0]
  -rf_sklearn_ccp_alpha RF_SKLEARN_CCP_ALPHA, --rf_sklearn_ccp_alpha RF_SKLEARN_CCP_ALPHA
                        Complexity parameter used for Minimal Cost-Complexity
                        Pruning. The subtree with the largest cost complexity
                        that is smaller than ccp_alpha will be chosen. By
                        default, no pruning is performed. [default: 0.0]
  -et_sklearn_n_estimators ET_SKLEARN_N_ESTIMATORS, --et_sklearn_n_estimators ET_SKLEARN_N_ESTIMATORS
                        The number of trees in the forest. [default: 100]
  -et_sklearn_max_features ET_SKLEARN_MAX_FEATURES, --et_sklearn_max_features ET_SKLEARN_MAX_FEATURES
                        The number of features to consider when looking for
                        the best split: If int, then consider max_features
                        features at each split. If float, max_features is a
                        fraction and max(1, int(max_features *
                        n_features_in_)) features are considered at each
                        split, where n_features_in_ is the number of features
                        seen during fit. If “sqrt”, then
                        max_features=sqrt(n_features). If “log2”, then
                        max_features=log2(n_features). If None, then
                        max_features=n_features. [default: 1.0]
  -et_sklearn_bootstrap ET_SKLEARN_BOOTSTRAP, --et_sklearn_bootstrap ET_SKLEARN_BOOTSTRAP
                        Whether bootstrap samples are used when building
                        trees. If False, the whole dataset is used to build
                        each tree [default: True]
  -et_sklearn_verbose ET_SKLEARN_VERBOSE, --et_sklearn_verbose ET_SKLEARN_VERBOSE
                        Controls the verbosity when fitting and predicting.
                        [default: 0]
  -et_sklearn_warm_start ET_SKLEARN_WARM_START, --et_sklearn_warm_start ET_SKLEARN_WARM_START
                        When set to True, reuse the solution of the previous
                        call to fit and add more estimators to the ensemble,
                        otherwise, just fit a whole new forest [default:
                        False]
  -et_sklearn_max_samples ET_SKLEARN_MAX_SAMPLES, --et_sklearn_max_samples ET_SKLEARN_MAX_SAMPLES
                        If bootstrap is True, the number of samples to draw
                        from X to train each base estimator. If None
                        (default), then draw X.shape[0] samples. If int, then
                        draw max_samples samples.If float, then draw
                        max(round(n_samples * max_samples), 1) samples. Thus,
                        max_samples should be in the interval (0.0, 1.0].
                        [default: None]
  -et_sklearn_rand_state ET_SKLEARN_RANDOM_STATE, --et_sklearn_random_state ET_SKLEARN_RANDOM_STATE
                        Used to pick randomly the max_features used at each
                        split. Note that the mere presence of random_state
                        doesn’t mean that randomization is always used, as it
                        may be dependent on another parameter, e.g. shuffle,
                        being set. [default: None]
  -et_sklearn_criterion ET_SKLEARN_CRITERION, --et_sklearn_criterion ET_SKLEARN_CRITERION
                        The function to measure the quality of a split.
                        Supported criteria are “squared_error” for the mean
                        squared error, which is equal to variance reduction as
                        feature selection criterion and minimizes the L2 loss
                        using the mean of each terminal node, “friedman_mse”,
                        which uses mean squared error with Friedman’s
                        improvement score for potential splits,
                        “absolute_error” for the mean absolute error, which
                        minimizes the L1 loss using the median of each
                        terminal node, and “poisson” which uses reduction in
                        Poisson deviance to find splits. Training using
                        “absolute_error” is slower than when using
                        “squared_error”. [default: squared_error]
  -et_sklearn_max_depth ET_SKLEARN_MAX_DEPTH, --et_sklearn_max_depth ET_SKLEARN_MAX_DEPTH
                        The maximum depth of the tree. If None, then nodes are
                        expanded until all leaves are pure or until all leaves
                        contain less than min_samples_split samples. [default:
                        None]
  -et_sklearn_min_samples_split ET_SKLEARN_MIN_SAMPLES_SPLIT, --et_sklearn_min_samples_split ET_SKLEARN_MIN_SAMPLES_SPLIT
                        The minimum number of samples required to split an
                        internal node.If int, then consider min_samples_split
                        as the minimum number. If float, min_samples_split is
                        a fraction and ceil(min_samples_split * n_samples) is
                        the minimum number of samples for each split.
                        [default: 2]
  -et_sklearn_min_samples_leaf ET_SKLEARN_MIN_SAMPLES_LEAF, --et_sklearn_min_samples_leaf ET_SKLEARN_MIN_SAMPLES_LEAF
                        The minimum number of samples required to be at a leaf
                        node. If int, then consider min_samples_leaf as the
                        minimum number. If float, min_samples_leaf is a
                        fraction and ceil(min_samples_leaf * n_samples) is the
                        minimum number of samples for each node. [default: 1]
  -et_sklearn_min_weight_fraction_leaf ET_SKLEARN_MIN_WEIGHT_FRACTION_LEAF, --et_sklearn_min_weight_fraction_leaf ET_SKLEARN_MIN_WEIGHT_FRACTION_LEAF
                        The minimum weighted fraction of the sum total of
                        weights (of all the input samples) required to be at a
                        leaf node. Samples have equal weight when
                        sample_weight is not provided. [default: 0.0]
  -et_sklearn_max_leaf_nodes ET_SKLEARN_MAX_LEAF_NODES, --et_sklearn_max_leaf_nodes ET_SKLEARN_MAX_LEAF_NODES
                        Grow a tree with max_leaf_nodes in best-first fashion.
                        Best nodes are defined as relative reduction in
                        impurity. If None then unlimited number of leaf nodes
                        [default: None]
  -et_sklearn_min_impurity_decrease ET_SKLEARN_MIN_IMPURITY_DECREASE, --et_sklearn_min_impurity_decrease ET_SKLEARN_MIN_IMPURITY_DECREASE
                        A node will be split if this split induces a decrease
                        of the impurity greater than or equal to this value
                        N_t / N * (impurity - N_t_R / N_t * right_impurity -
                        N_t_L / N_t * left_impurity), where N is the total
                        number of samples, N_t is the number of samples at the
                        current node, N_t_L is the number of samples in the
                        left child, and N_t_R is the number of samples in the
                        right child. N, N_t, N_t_R and N_t_L all refer to the
                        weighted sum, if sample_weight is passed. [default:
                        0.0]
  -et_sklearn_ccp_alpha ET_SKLEARN_CCP_ALPHA, --et_sklearn_ccp_alpha ET_SKLEARN_CCP_ALPHA
                        Complexity parameter used for Minimal Cost-Complexity
                        Pruning. The subtree with the largest cost complexity
                        that is smaller than ccp_alpha will be chosen. By
                        default, no pruning is performed. [default: 0.0]
  -setup_caret_session_id SETUP_CARET_SESSION_ID, --setup_caret_session_id SETUP_CARET_SESSION_ID
                        Controls the randomness of experiment. It is
                        equivalent to ‘random_state’ in scikit-learn. When
                        None, a pseudo random number is generated. This can be
                        used for later reproducibility of the entire
                        experiment [default: None]
  -setup_caret_fold SETUP_CARET_FOLD, --setup_caret_fold SETUP_CARET_FOLD
                        Controls cross-validation. If None, the CV generator
                        in the fold_strategy parameter of the setup function
                        is used. When an integer is passed, it is interpreted
                        as the ‘n_splits’ parameter of the CV generator in the
                        setup function. [default: 0]
  -setup_caret_data_split_shuffle SETUP_CARET_DATA_SPLIT_SHUFFLE, --setup_caret_data_split_shuffle SETUP_CARET_DATA_SPLIT_SHUFFLE
                        When set to False, prevents shuffling of rows during
                        ‘train_test_split’. [default: True]
  -setup_caret_verbose SETUP_CARET_VERBOSE, --setup_caret_verbose SETUP_CARET_VERBOSE
                        When set to False, Information grid is not printed.
                        [default: True]
  -model_caret_cross_validation MODEL_CARET_CROSS_VALIDATION, --model_caret_cross_validation MODEL_CARET_CROSS_VALIDATION
                        When set to False, metrics are evaluated on holdout
                        set. fold param is ignored when cross_validation is
                        set to False. [default: True]
  -model_caret_verbose MODEL_CARET_VERBOSE, --model_caret_verbose MODEL_CARET_VERBOSE
                        Score grid is not printed when verbose is set to
                        False. [default: True]
  -model_caret_return_train_score MODEL_CARET_RETURN_TRAIN_SCORE, --model_caret_return_train_score MODEL_CARET_RETURN_TRAIN_SCORE
                        If False, returns the CV Validation scores only. If
                        True, returns the CV training scores along with the CV
                        validation scores. This is useful when the user wants
                        to do bias-variance tradeoff. A high CV training score
                        with a low corresponding CV validation score indicates
                        overfitting. [default: False]
  -tuner_caret_search_algo TUNER_CARET_SEARCH_ALGORITHM, --tuner_caret_search_algorithm TUNER_CARET_SEARCH_ALGORITHM
                        The search algorithm depends on the search_library
                        parameter. If None, will use search library-specific
                        default algorithm. Other possible values are ‘random’
                        : random grid search (default) and ‘grid’ : grid
                        search [default: random]
  -tuner_caret_tuner_verbose TUNER_CARET_TUNER_VERBOSE, --tuner_caret_tuner_verbose TUNER_CARET_TUNER_VERBOSE
                        If True or above 0, will print messages from the
                        tuner. Ignored when verbose param is False. [default:
                        True]
  -resp RESPONSE, --response RESPONSE
                        Names of response variables, must be provided [default
                        None]
  -feat FEATURES, --features FEATURES
                        Names of input features (can be computed from data)
                        [default None]
  -keep_feat KEEP_FEATURES, --keep_features KEEP_FEATURES
                        Names of input features that should be used in model
                        training: feature selection or other heuristics for
                        selecting features that will be used in model training
                        cannot drop these input features [default []]
  -new_data NEW_DATA, --new_data NEW_DATA
                        Path excluding the .csv suffix to new data file
                        [default: None]
  -data_scaler DATA_SCALER, --data_scaler DATA_SCALER
                        Should features and responses be scaled and with which
                        scaling optionton? Value "none" implies no scaling;
                        the only other supported option in "min_max" scaler
                        [default: min_max]
  -scale_feat SCALE_FEATURES, --scale_features SCALE_FEATURES
                        Should features be scaled using scaler specified
                        through option "data_scaler"? [default: True]
  -scale_resp SCALE_RESPONSES, --scale_responses SCALE_RESPONSES
                        Should responses be scaled using scaler specified
                        through option "data_scaler"? [default: True]
  -impute_resp IMPUTE_RESPONSES, --impute_responses IMPUTE_RESPONSES
                        Should missing values in responses be imputed? Might
                        make sense when there are multiple responses and
                        different responses have missing values in different
                        samples: this might be a better alternative compared
                        to dropping rows where at least one response has a
                        missing value [default: False]
  -split SPLIT_TEST, --split_test SPLIT_TEST
                        Fraction in (0,1] of data samples to split from
                        training data for testing; when the option value is
                        1,the dataset will be used both for training and
                        testing [default: 0.2]
  -train_rand TRAIN_RANDOM_N, --train_random_n TRAIN_RANDOM_N
                        Subset random n rows from training data to use for
                        training [default: 0]
  -train_first TRAIN_FIRST_N, --train_first_n TRAIN_FIRST_N
                        Subset first n rows from training data to use for
                        training [default: 0]
  -train_unif TRAIN_UNIFORM_N, --train_uniform_n TRAIN_UNIFORM_N
                        Subset random n rows from training data with close to
                        uniform distribution to use for training [default: 0]
  -sw_coef SAMPLE_WEIGHTS_COEF, --sample_weights_coef SAMPLE_WEIGHTS_COEF
                        Coefficient in range ]-1, 1[ to compute sample weights
                        for model training; weights are defined as [sw_coef *
                        (v - mid_range) + 1 for v in resp_vals] where
                        resp_vals is the response value vector or vector of
                        mean values of all responses per sample, and mid_range
                        is the mid point of the range of resp_vals. The value
                        of sw_coef is chosen positive (resp. negative) when
                        one wants to assign higher weights to samples with
                        high (resp. low) values in resp_vals. As an example,
                        sw_coef = 0.2 assigns weight=1.2 to samples with
                        max(resp_vals) and weight=0.8 to samples with
                        min(resp_vals), and sw_coef = 0 implies weight=1 for
                        each sample [default: 0]
  -sw_exp SAMPLE_WEIGHTS_EXPONENT, --sample_weights_exponent SAMPLE_WEIGHTS_EXPONENT
                        The Exponent to compute sample weights for model
                        training; weights are defined as [sw_int + sw_coef
                        *((v - mn)/(mx-mn))**sw_exp for v in resp_vals ] where
                        resp_vals is the response value vector or vector of
                        mean values of all responses per sample, and mn and mx
                        are respectively the min and max of resp_vals. The
                        value of sw_coef is chosen non-negative to make sure
                        all weights are non-negative [default: 0]
  -sw_int SAMPLE_WEIGHTS_INTERCEPT, --sample_weights_intercept SAMPLE_WEIGHTS_INTERCEPT
                        The intercept to compute sample weights for model
                        training; weights are defined as [sw_int + sw_coef
                        *((v - mn)/(mx-mn))**sw_exp for v in resp_vals ] where
                        resp_vals is the response value vector or vector of
                        mean values of all responses per sample, and mn and mx
                        are respectively the min and max of resp_vals. The
                        value of sw_coef is chosen non-negative to make sure
                        all weights are non-negative [default: 0]
  -respmap RESPONSE_MAP, --response_map RESPONSE_MAP
                        Python expression with just one variable x to be
                        applied as lambda function to the values of each of
                        the responses, as part of preprocessing. This
                        transformation is applied before the transformation of
                        responses specified using option resp2b, and the user
                        responsibility to ensure these two transformations in
                        the described order achieve the right transformation
                        of the response columns [default: None]
  -resp2b RESPONSE_TO_BOOL, --response_to_bool RESPONSE_TO_BOOL
                        Semicolon seperated list of conditions to be applied
                        to the responses in the order the responses are
                        specified, to convert them into binary responses as
                        part of data preprocessing. The conditions define when
                        each response is positive. Say a condition resp1 > 5
                        transforms response called resp1 into a binary 1/0
                        response that has value 1 for each data sample (row)
                        where resp1 is greater than 5 and value 0 for the
                        remaining samples [default: None]
  -pos_val POSITIVE_VALUE, --positive_value POSITIVE_VALUE
                        Value that represents positive values in a binary
                        categorical response in the original input data
                        (before any data processing has been applied)
                        [default: 1]
  -neg_val NEGATIVE_VALUE, --negative_value NEGATIVE_VALUE
                        Value that represents negative values in a binary
                        categorical response in the original input data
                        (before any data processing has been applied)
                        [default: 0]
  -resp_plots RESPONSE_PLOTS, --response_plots RESPONSE_PLOTS
                        Should response value distribution plots be genrated
                        during data processing? A related option
                        interactive_plots controls whether the generated plots
                        should be displayed interactively during runtime
                        [default: True]
  -mrmr_pred MRMR_FEAT_COUNT_FOR_PREDICTION, --mrmr_feat_count_for_prediction MRMR_FEAT_COUNT_FOR_PREDICTION
                        Count of features selected by MRMR algorithm for
                        predictive models [default: 15]
  -mrmr_corr MRMR_FEAT_COUNT_FOR_CORRELATION, --mrmr_feat_count_for_correlation MRMR_FEAT_COUNT_FOR_CORRELATION
                        Count of features selected by MRMR algorithm for
                        correlation analysis [default: 15]
  -data LABELED_DATA, --labeled_data LABELED_DATA
                        Path, possibly excluding the .csv, or including gz or
                        bz2 suffix, to input training data file containing
                        labels [default None]
  -mode ANALYTICS_MODE, --analytics_mode ANALYTICS_MODE
                        What kind of analysis should be performed; the
                        supported modes are: "train", "predict", "subgroups",
                        "doe", "discretize", "optimize", "verify", "query",
                        "optsyn" [default: None]
  -plots INTERACTIVE_PLOTS, --interactive_plots INTERACTIVE_PLOTS
                        Should plots be displayed interactively (or only
                        saved)?[default: True]
  -seed SEED, --seed SEED
                        Initial random seed [default None]
  -pref LOG_FILES_PREFIX, --log_files_prefix LOG_FILES_PREFIX
                        String to be used as prefix for the output files
                        [default: None]
  -out_dir OUTPUT_DIRECTORY, --output_directory OUTPUT_DIRECTORY
                        Output directory where all reports and output files
                        will be written [default: the same directory from
                        which data is loaded]
  -save_config SAVE_CONFIGURATION, --save_configuration SAVE_CONFIGURATION
                        Should tool run parameters be saved into a a
                        configuration file? [default: False]
  -config LOAD_CONFIGURATION, --load_configuration LOAD_CONFIGURATION
                        Json config file name, to load tool parameter values
                        from, or None. Paramters specified through command
                        line will override the correponding config file values
                        if they are specified there as well [default: None]
  -log_level LOG_LEVEL, --log_level LOG_LEVEL
                        The logger level or severity of the events they are
                        used to track. The standard levels are (in increasing
                        order of severity): notset, debug, info, warning,
                        error, critical; only events of this level and above
                        will be tracked [default warning]
  -log_mode LOG_MODE, --log_mode LOG_MODE
                        The logger filemode for logging into log file [default
                        w]
  -log_time LOG_TIME, --log_time LOG_TIME
                        Should time stamp be logged along with every message
                        issued by logger [default true]
  -doe_algo DOE_ALGO, --doe_algo DOE_ALGO
                        Design of experiment (DOE) algorithm from doepy
                        package. The supported algorithms are:
                        "full_factorial, fractional_factorial,
                        plackett_burman, sukharev_grid, box_behnken,
                        box_wilson, latin_hypercube, latin_hypercube_sf,
                        halton_sequence, uniform_random_matrix"
  -doe_factor_level_ranges DOE_FACTOR_LEVEL_RANGES, --doe_factor_level_ranges DOE_FACTOR_LEVEL_RANGES
                        A dictionary of levels per feature for building
                        experiments for all supported DOE algorithms. Here
                        experiments are lists feature-value assignments
                        [(feature_1, value_1),...,(feature_n, value_n)], and
                        they are rows of the matrix of experiments returned by
                        the supported DOE algorithms. The features are integer
                        features (thus the levels (values) are integers). The
                        keys in that dictionary are names of features and the
                        associated values are lists [val_1, .., val_k] from
                        which value for that feature are selected to build an
                        experiment. Example:
                        {"Pressure":[50,60,70],"Temperature":[290, 320,
                        350],"Flow rate":[0.9,1.0]}. DOE algorithms that work
                        with two levels only treat these levels as the min and
                        max of the rage of a numeric variable. [default: None]
  -doe_samples DOE_NUM_SAMPLES, --doe_num_samples DOE_NUM_SAMPLES
                        Number of samples (experiments) to be generated
                        [default: None]
  -doe_resolution DOE_DESIGN_RESOLUTION, --doe_design_resolution DOE_DESIGN_RESOLUTION
                        Desired design resolution. The resolution of a design
                        is defined as the length of the shortest word in the
                        defining relation. The resolution describes the level
                        of confounding between factors and interaction
                        effects, where higher resolution indicates lower
                        degree of confounding. For example, consider the
                        2^4-1-design defined by gen = "a b c ab" The factor
                        "d" is defined by "ab" with defining relation I="abd",
                        where I is the unit vector. In this simple example the
                        shortest word is "abd" meaning that this is a
                        resolution III-design. In practice resolution III-,
                        IV- and V-designs are most commonly applied. * III:
                        Main effects may be confounded with two-factor
                        interactions. * IV: Main effects are unconfounded by
                        two-factor interactions, but two-factor interactions
                        may be confounded with each other. * V: Main effects
                        unconfounded with up to four-factor interactions, two-
                        factor interactions unconfounded with up to three-
                        factor interactions. Three-factor interactions may be
                        confounded with each other. [default: Half of the
                        total feature count in doe_factor_level_ranges]
  -doe_spec DOE_SPEC_FILE, --doe_spec_file DOE_SPEC_FILE
                        File in csv format that specifies factor, level ranges
                        used for building design of experiment (DOE) samples
                        using function sample_doepy(). If not provided, a
                        dictionary of factor / level ranges must be supplied
                        to sample_doepy() directly instead of the file.
  -doe_bb_centers DOE_BOX_BEHNKEN_CENTERS, --doe_box_behnken_centers DOE_BOX_BEHNKEN_CENTERS
                        Number of center points to include in the final design
                        [default: 1]
  -doe_cc_center DOE_CENTRAL_COMPOSITE_CENTER, --doe_central_composite_center DOE_CENTRAL_COMPOSITE_CENTER
                        A 1-by-2 array of integers, the number of center
                        points in each block of the design. [default]
  -doe_cc_alpha DOE_CENTRAL_COMPOSITE_ALPHA, --doe_central_composite_alpha DOE_CENTRAL_COMPOSITE_ALPHA
                        A string describing the effect of alpha has on the
                        variance. "alpha" can take on two values: "orthogonal"
                        or "o", and "rotatable" or "r" [default o]
  -doe_cc_face DOE_CENTRAL_COMPOSITE_FACE, --doe_central_composite_face DOE_CENTRAL_COMPOSITE_FACE
                        The relation between the start points and the corner
                        (factorial) points. There are three options for this
                        input: 1. "circumscribed" or "ccc": This is the
                        original form of the central composite design. The
                        star points are at some distance "alpha" from the
                        center, based on the properties desired for the
                        design. The start points establish new extremes for
                        the low and high settings for all factors. These
                        designs have circular, spherical, or hyperspherical
                        symmetry and require 5 levels for each factor.
                        Augmenting an existing factorial or resolution V
                        fractional factorial design with star points can
                        produce this design. 2. "inscribed" or "cci": For
                        those situations in which the limits specified for
                        factor settings are truly limits, the CCI design uses
                        the factors settings as the star points and creates a
                        factorial or fractional factorial design within those
                        limits (in other words, a CCI design is a scaled down
                        CCC design with each factor level of the CCC design
                        divided by "alpha" to generate the CCI design). This
                        design also requires 5 levels of each factor. 3.
                        "faced" or "ccf": In this design, the star points are
                        at the center of each face of the factorial space, so
                        alpha" = 1. This variety requires 3 levels of each
                        factor. Augmenting an existing factorial or resolution
                        V design with appropriate star points can also produce
                        this design. [default 2,2]
  -doe_prob_distr DOE_PROB_DISTRIBUTION, --doe_prob_distribution DOE_PROB_DISTRIBUTION
                        Analytical probability distribution to be applied over
                        the randomized sampling. Takes strings: "Normal",
                        "Poisson", "Exponential", "Beta", "Gamma" [default
                        Normal]
  -discr_algo DISCRETIZATION_ALGO, --discretization_algo DISCRETIZATION_ALGO
                        Discretization algorithm to use. The possible options
                        are: * "uniform": constracts constant-width bins; *
                        "quantile": uses the quantiles values to have equally
                        populated bins in each feature; * "kmeans": defines
                        bins based on a k-means clustering performed on each
                        feature independently; * "jenks": implements the
                        Fisher-Jenks Natural Breaks algorithm; * "ordinals":
                        converts the feature values into ordinals correponding
                        to the location of the correponding value in the
                        ascending sorted list of unique values in that
                        feature. * "ranks": converts the feature values into
                        ranks (ranks used in Spearman's rank correlation)
                        [default uniform]
  -discr_bins DISCRETIZATION_BINS, --discretization_bins DISCRETIZATION_BINS
                        Number of required bins in a discretization algorithm
                        [default 10]
  -discr_labels DISCRETIZATION_LABELS, --discretization_labels DISCRETIZATION_LABELS
                        If true, string labels (e.g., "Bin2") will be used to
                        denote levels of the categorical feature resulting
                        from discretization; othewise integers (e.g., 2) will
                        be used to represent the levels [default True]
  -discr_type DISCRETIZATION_TYPE, --discretization_type DISCRETIZATION_TYPE
                        The type of the categorical feature resulting from
                        discretization. Possible values are: * "object": the
                        feature will be of type "object" -- with strings as
                        values; * "category": the feature will be of pandas
                        type "category" -- with levels unordered; these
                        correspond to factors in statistics (and in R lamguage
                        terminology) * "ordered": the feature will be of
                        pandas type "category" -- with levels ordered; these
                        correspond to ordered factors in statistics (and in R
                        language terminology) * "integer": The feature will be
                        of type int, its values will be the resulting bin
                        numbers when enumerating the bins from left to right.
                        [default category]
  -mi_method MUTUAL_INFORMATION_METHOD, --mutual_information_method MUTUAL_INFORMATION_METHOD
                        The mutual information method to be used when
                        computing feature correlation scores with responses.
                        Supported options are "shannon", "normalized", and
                        "adjusted", for Shannon's mutual information, for the
                        normalized mutual information, and the adjusted mutual
                        information, respectively; in addition, with the
                        option value "correlation", the mutual information is
                        computed from a correlation coefficient corr between
                        the feature and response using equation mi = -0.5 *
                        log(1 - (corr**2)), which is primarily useful for
                        computing mutual information for (preferably) normally
                        distributed continuous random variables [default:
                        normalized]
  -corr_and_mi CORRELATIONS_AND_MUTUAL_INFORMATION, --correlations_and_mutual_information CORRELATIONS_AND_MUTUAL_INFORMATION
                        Should correlation and mutual information between the
                        features and the response(s) be computed when
                        computing scores for feature selection and ranking
                        [default: True]
  -discret_num DISCRETIZE_NUMERIC_FEATURES, --discretize_numeric_features DISCRETIZE_NUMERIC_FEATURES
                        The mutual information method to be used for
                        discretizing numeric features, when computing feature
                        correlation scores with responses [default: None]
  -cont_est CONTINUOUS_CORRELATION_ESTIMATORS, --continuous_correlation_estimators CONTINUOUS_CORRELATION_ESTIMATORS
                        Correlation estimators for continuous features, to be
                        used in correlation, mutual information and MRMR
                        feature selection algorithms. The options are pearson,
                        spearman, kendall and frequency, and any subset of
                        these specified thru a comma-separated string. In
                        addition, the value "all" indicates that all the
                        options should be used and value "none" indicates that
                        no options should be used. [default: pearson,spearman]
  -psg_quality PSG_QUALITY_TARGET, --psg_quality_target PSG_QUALITY_TARGET
                        Quality function (quality target/measure) used for
                        defining the importance criterion for range selction.
                        The supported options (both for numeric as well as
                        binary responses) are {} and {} [default Lift]
  -psg_dim PSG_MAX_DIMENSION, --psg_max_dimension PSG_MAX_DIMENSION
                        Maximal dimension of selected range tuples (feature-
                        range tuples) [default 3]
  -psg_top PSG_TOP_RANKED, --psg_top_ranked PSG_TOP_RANKED
                        Required count of selected range tuples (feature-range
                        tuples) [default 15]
  -spec SPEC, --spec SPEC
                        Name of spec file including full path, must be
                        provided [default None]
  -delta_rel DELTA_RELATIVE, --delta_relative DELTA_RELATIVE
                        exclude (1+DELTA)*radius region for non-grid
                        components [default: 0.01]
  -delta_abs DELTA_ABSOLUTE, --delta_absolute DELTA_ABSOLUTE
                        exclude (1+DELTA)*radius region for non-grid
                        components [default: 0.0]
  -rad_rel RADIUS_RELATIVE, --radius_relative RADIUS_RELATIVE
                        Relative radius, in terms of percentage of the value
                        of the knob to which it applies to compute the
                        absolute radius to be used in theta (stability)
                        constraint. Overrides relative radius value specified
                        in the spec file [default: None]
  -rad_abs RADIUS_ABSOLUTE, --radius_absolute RADIUS_ABSOLUTE
                        Absolute value of radius to be used in theta
                        (stability) constraint. Override relative radius value
                        specified in the spec file [default: None]
  -alpha ALPHA, --alpha ALPHA
                        constraints on model inputs (free inputs or
                        configuration knobs) [default: None]
  -beta BETA, --beta BETA
                        constraints on model outputs, relevant for "optimize"
                        mode only (when selecting model configuration that are
                        safe and near-optimal) [default: None]
  -eta ETA, --eta ETA   global constraints on/accross knobs that define legal
                        configurations of knobs during search for optimal
                        configurations in "optimize" and "optsyn" modes
                        [default: None]
  -compress_rules COMPRESS_RULES, --compress_rules COMPRESS_RULES
                        Should rules that represent tree branches be
                        compressed to eliminate redundant repeated splitting
                        of ranges of model features after training tree based
                        models, in order to build smaller model terms?
                        [default True]
  -simplify_terms SIMPLIFY_TERMS, --simplify_terms SIMPLIFY_TERMS
                        Should terms be simplified using before building
                        solver instance in model exploration modes? [default
                        False]
  -tree_encoding TREE_ENCODING, --tree_encoding TREE_ENCODING
                        Method to encode tree model to solvers. Can be "flat",
                        "nested", or "branched". The flat encoding creates a
                        formula from each branch of a tree, while the nested
                        encoding builds formula from branches using nested if-
                        then-else (ite) expressions. The branched encoding
                        also uses ite expressions and in addition the branch
                        conditions in ite expressions are shared across all
                        responses [default nested]
  -nnet_encoding NNET_ENCODING, --nnet_encoding NNET_ENCODING
                        Method to encode Keras Neural Nets model to solvers.
                        Can be "layered" or "nested". The layered encoding
                        creates a formula from each internal node of the NN
                        with nodes in the previous layer as inputs, while
                        nested encoding builds a monolithic term for each
                        response representing the function for that response
                        [default nested]
  -trace_runtime TRACE_RUNTIME, --trace_runtime TRACE_RUNTIME
                        Should trace include solver runtimes and what
                        precision to use in terms of number of decimal points
                        after 0; the option value 0 means to not include the
                        runtimes in the trace [default 0]
  -trace_prec TRACE_PRECISION, --trace_precision TRACE_PRECISION
                        Decimals after 0 to use when rounding fractions;
                        option value 0 means to use fractions (implying no
                        rounding) [default: 0]
  -trace_anonym TRACE_ANONYMIZE, --trace_anonymize TRACE_ANONYMIZE
                        Should anonymized names of system inputs, knobs and
                        outputs be uses in trace log file?[default: False]
  -quer_names QUERY_NAMES, --query_names QUERY_NAMES
                        Names of optimization objectives [default None]
  -quer_exprs QUERY_EXPRESSIONS, --query_expressions QUERY_EXPRESSIONS
                        Semicolon seperated list of expressions (functions) to
                        be applied to the responses to convert them into
                        optimization objectives [default: None]
  -lemma_prec LEMMA_PRECISION, --lemma_precision LEMMA_PRECISION
                        Number of decimals after zero to use when
                        approximating lemmas in model exploration modes. The
                        default value 0 means that lemmas should not be
                        approximated (full precision should be used [default:
                        0]
  -asrt_names ASSERTIONS_NAMES, --assertions_names ASSERTIONS_NAMES
                        Names of optimization objectives [default None]
  -asrt_exprs ASSERTIONS_EXPRESSIONS, --assertions_expressions ASSERTIONS_EXPRESSIONS
                        Semicolon seperated list of expressions (functions) to
                        be applied to the responses to convert them into
                        optimization objectives [default: None]
  -epsilon EPSILON, --epsilon EPSILON
                        ratio of the length of an estimated range of an
                        objective, computed per objective based on its
                        estimated min and max bounds [default: 0.05]
  -center_offset CENTER_OFFSET, --center_offset CENTER_OFFSET
                        Center threshold offset of threshold [default: 0]
  -objv_names OBJECTIVES_NAMES, --objectives_names OBJECTIVES_NAMES
                        Names of optimization objectives [default None]
  -objv_exprs OBJECTIVES_EXPRESSIONS, --objectives_expressions OBJECTIVES_EXPRESSIONS
                        Semicolon seperated list of expressions (functions) to
                        be applied to the responses to convert them into
                        optimization objectives [default: None]
  -scale_objv SCALE_OBJECTIVES, --scale_objectives SCALE_OBJECTIVES
                        Should optimization objectives be scaled using scaler
                        specified through option "data_scaler"? [default:
                        True]
  -pareto OPTIMIZE_PARETO, --optimize_pareto OPTIMIZE_PARETO
                        Should optimization be per objective (even if there
                        are multiple objectives) or pareto optimization must
                        be performed? [default: True]
  -frac_aprox APPROXIMATE_FRACTIONS, --approximate_fractions APPROXIMATE_FRACTIONS
                        Should fraction values form satisfying assignments be
                        converted to approximate reals? [default: True]
  -frac_prec FRACTION_PRECISION, --fraction_precision FRACTION_PRECISION
                        Decimal precision when approximating fractions by
                        reals [default 64]
  -vacuity VACUITY_CHECK, --vacuity_check VACUITY_CHECK
                        Should solver problem instance vacuity check be
                        performed? Vacuity checks whether the constraints are
                        consistent and therefore at least one satisfiable
                        assignment exist to solver constraints. Relevant in
                        "verify", "query", "optimize" and "optsyn" modes
                        [default: True]
  -opt_strategy OPTIMIZATION_STRATEGY, --optimization_strategy OPTIMIZATION_STRATEGY
                        Strategy (algorithm) to use for single objective
                        optimization in the "optimize" and "optsyn" modes.
                        Supported options are "lazy" and "eager" [default
                        eager]
  -solver SOLVER, --solver SOLVER
                        Solver to use in model exploration modes "verify,"
                        "query", "optimize" and "optsyn". [default: z3]
  -solver_path SOLVER_PATH, --solver_path SOLVER_PATH
                        Path to solver to use in model exploration modes
                        "verify," "query", "optimize" and "optsyn". [default:
                        None]
  -solver_logic SOLVER_LOGIC, --solver_logic SOLVER_LOGIC
                        SMT2-lib theory with respect to which to solve model
                        exploration task at hand, in modes "verify," "query",
                        "optimize" and "optsyn". [default: ALL]
