Run directory: /tmp/smlp_tutorial_mdmitry_61610
Cloning into 'smlp'...
Switched to a new branch 'SMLP_TUTORIAL'
branch 'SMLP_TUTORIAL' set up to track 'origin/SMLP_TUTORIAL'.
env PYTHONDONTWRITEBYTECODE=1 /usr/local/lib/python3.14/dist-packages/bin/pytest -v --forked -m forked /tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples
============================= test session starts ==============================
platform linux -- Python 3.14.2, pytest-9.0.2, pluggy-1.6.0 -- /usr/bin/python3.14
cachedir: .pytest_cache
rootdir: /tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples
plugins: forked-1.6.0, typeguard-4.4.4, mock-3.15.1
collecting ... collected 30 items / 22 deselected / 8 selected

bnh/py/test_z3.py::test_z3 
-------------------------------- live log call ---------------------------------
============================================================
BNH Multi-Objective Optimization Problem with Z3
============================================================

1. Lexicographic Optimization (f1 primary, f2 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

2. Lexicographic Optimization (f2 primary, f1 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

3. Weighted Sum Method (various weights):
------------------------------------------------------------

Weights (w1=0.2, w2=0.8):
  x1 = 2.750000, x2 = 2.625000
  f1 = 57.812500, f2 = 10.703125

Weights (w1=0.4, w2=0.6):
  x1 = 1.000000, x2 = 1.000000
  f1 = 8.000000, f2 = 32.000000

Weights (w1=0.5, w2=0.5):
  x1 = 1.000000, x2 = 2.000000
  f1 = 20.000000, f2 = 25.000000

Weights (w1=0.6, w2=0.4):
  x1 = 1.000000, x2 = 1.000000
  f1 = 8.000000, f2 = 32.000000

Weights (w1=0.8, w2=0.2):
  x1 = 0.250000, x2 = 0.250000
  f1 = 0.500000, f2 = 45.125000

4. Constraint Verification:
------------------------------------------------------------
Solution 1:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 2:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 3:
  C1 = 11.953125 (should be ≤ 25): ✓
  C2 = 59.203125 (should be ≥ 7.7): ✓
Solution 4:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 5:
  C1 = 20.000000 (should be ≤ 25): ✓
  C2 = 74.000000 (should be ≥ 7.7): ✓
Solution 6:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 7:
  C1 = 22.625000 (should be ≤ 25): ✓
  C2 = 70.625000 (should be ≥ 7.7): ✓

5. Pareto Front Plot Generated
------------------------------------------------------------

============================================================
Found 7 Pareto optimal solutions
============================================================
(0.0, 0.0, 0.0, 50.0) (0.0, 0.0, 0.0, 50.0) (2.75, 2.625, 57.8125, 10.703125) (1.0, 1.0, 8.0, 32.0) (1.0, 2.0, 20.0, 25.0) (1.0, 1.0, 8.0, 32.0) (0.25, 0.25, 0.5, 45.125)
PASSED                                        [ 12%]
bnh/py/test_z3_gradient.py::test_z3_gradient 
-------------------------------- live log call ---------------------------------
============================================================
BNH Multi-Objective Optimization Problem with Z3
============================================================

1. Lexicographic Optimization (f1 primary, f2 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

2. Lexicographic Optimization (f2 primary, f1 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

3. Weighted Sum Method (various weights):
------------------------------------------------------------

Trying weights (w1=0.1, w2=0.9)...
  ✓ x1 = 2.5020, x2 = 1.0000
    f1 = 29.0391, f2 = 22.2402

Trying weights (w1=0.2, w2=0.8)...
  ✓ x1 = 1.0000, x2 = 2.8301
    f1 = 36.0374, f2 = 20.7086

Trying weights (w1=0.4, w2=0.6)...
  ✓ x1 = 1.0000, x2 = 1.0000
    f1 = 8.0000, f2 = 32.0000

Trying weights (w1=0.5, w2=0.5)...
  ✓ x1 = 1.0000, x2 = 2.0000
    f1 = 20.0000, f2 = 25.0000

Trying weights (w1=0.6, w2=0.4)...
  ✓ x1 = 1.0000, x2 = 1.0000
    f1 = 8.0000, f2 = 32.0000

Trying weights (w1=0.8, w2=0.2)...
  ✓ x1 = 0.2500, x2 = 0.2500
    f1 = 0.5000, f2 = 45.1250

Trying weights (w1=0.9, w2=0.1)...
  ✓ x1 = 0.0010, x2 = 0.0010
    f1 = 0.0000, f2 = 49.9805

4. Constraint Verification:
------------------------------------------------------------
Solution 1:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 2:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 3:
  C1 = 7.240238 (should be ≤ 25): ✓
  C2 = 46.228519 (should be ≥ 7.7): ✓
Solution 4:
  C1 = 24.009342 (should be ≤ 25): ✓
  C2 = 82.989811 (should be ≥ 7.7): ✓
Solution 5:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 6:
  C1 = 20.000000 (should be ≤ 25): ✓
  C2 = 74.000000 (should be ≥ 7.7): ✓
Solution 7:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 8:
  C1 = 22.625000 (should be ≤ 25): ✓
  C2 = 70.625000 (should be ≥ 7.7): ✓
Solution 9:
  C1 = 24.990236 (should be ≤ 25): ✓
  C2 = 72.990236 (should be ≥ 7.7): ✓

5. Stability Analysis:
============================================================

Solution 1: x = (0.000000, 0.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  0.0000,   0.0000], ||∇f1|| = 0.0001
  ∇f2 = [-10.0000, -10.0000], ||∇f2|| = 14.1421

Constraint Status:
  C1 active: True (value: 0.000000)
  C2 active: False (value: -65.300000)
    ∇C1 = [-10.0000,   0.0000]

Sensitivity Analysis:
  Avg sensitivity (f1): 0.0005
  Avg sensitivity (f2): 13.5288
  Max sensitivity (f1): 0.0010
  Max sensitivity (f2): 14.1353
  Feasible perturbations tested: 6/20

Stability Assessment:
  At boundary: True
  Stability score: 13.8357
  Classification: MODERATELY STABLE

Solution 2: x = (0.000000, 0.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  0.0000,   0.0000], ||∇f1|| = 0.0001
  ∇f2 = [-10.0000, -10.0000], ||∇f2|| = 14.1421

Constraint Status:
  C1 active: True (value: 0.000000)
  C2 active: False (value: -65.300000)
    ∇C1 = [-10.0000,   0.0000]

Sensitivity Analysis:
  Avg sensitivity (f1): 0.0007
  Avg sensitivity (f2): 12.4137
  Max sensitivity (f1): 0.0009
  Max sensitivity (f2): 13.8773
  Feasible perturbations tested: 4/20

Stability Assessment:
  At boundary: True
  Stability score: 13.2783
  Classification: MODERATELY STABLE

Solution 3: x = (2.501953, 1.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [ 20.0157,   8.0000], ||∇f1|| = 21.5552
  ∇f2 = [ -4.9961,  -8.0000], ||∇f2|| = 9.4319

Constraint Status:
  C1 active: False (value: -17.759762)
  C2 active: False (value: -38.528519)

Sensitivity Analysis:
  Avg sensitivity (f1): 15.1275
  Avg sensitivity (f2): 6.8825
  Max sensitivity (f1): 21.5511
  Max sensitivity (f2): 9.3918
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 26.4986
  Classification: WEAKLY STABLE

Solution 4: x = (1.000000, 2.830078)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,  22.6407], ||∇f1|| = 24.0125
  ∇f2 = [ -8.0000,  -4.3398], ||∇f2|| = 9.1013

Constraint Status:
  C1 active: False (value: -0.990658)
  C2 active: False (value: -75.289811)

Sensitivity Analysis:
  Avg sensitivity (f1): 13.5034
  Avg sensitivity (f2): 4.0553
  Max sensitivity (f1): 23.8884
  Max sensitivity (f2): 8.0682
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 25.3363
  Classification: WEAKLY STABLE

Solution 5: x = (1.000000, 1.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,   8.0000], ||∇f1|| = 11.3138
  ∇f2 = [ -8.0000,  -8.0000], ||∇f2|| = 11.3137

Constraint Status:
  C1 active: False (value: -8.000000)
  C2 active: False (value: -57.300000)

Sensitivity Analysis:
  Avg sensitivity (f1): 7.3790
  Avg sensitivity (f2): 7.3790
  Max sensitivity (f1): 11.2307
  Max sensitivity (f2): 11.2300
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 18.6927
  Classification: WEAKLY STABLE

Solution 6: x = (1.000000, 2.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,  16.0000], ||∇f1|| = 17.8886
  ∇f2 = [ -8.0000,  -6.0000], ||∇f2|| = 10.0000

Constraint Status:
  C1 active: False (value: -5.000000)
  C2 active: False (value: -66.300000)

Sensitivity Analysis:
  Avg sensitivity (f1): 14.1398
  Avg sensitivity (f2): 6.1548
  Max sensitivity (f1): 17.8855
  Max sensitivity (f2): 9.8949
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 24.0916
  Classification: WEAKLY STABLE

Solution 7: x = (1.000000, 1.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,   8.0000], ||∇f1|| = 11.3138
  ∇f2 = [ -8.0000,  -8.0000], ||∇f2|| = 11.3137

Constraint Status:
  C1 active: False (value: -8.000000)
  C2 active: False (value: -57.300000)

Sensitivity Analysis:
  Avg sensitivity (f1): 7.6598
  Avg sensitivity (f2): 7.6599
  Max sensitivity (f1): 11.1929
  Max sensitivity (f2): 11.1933
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 18.9736
  Classification: WEAKLY STABLE

Solution 8: x = (0.250000, 0.250000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  2.0000,   2.0000], ||∇f1|| = 2.8285
  ∇f2 = [ -9.5000,  -9.5000], ||∇f2|| = 13.4350

Constraint Status:
  C1 active: False (value: -2.375000)
  C2 active: False (value: -62.925000)

Sensitivity Analysis:
  Avg sensitivity (f1): 1.9779
  Avg sensitivity (f2): 9.3959
  Max sensitivity (f1): 2.8250
  Max sensitivity (f2): 13.4167
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 13.8187
  Classification: MODERATELY STABLE

Solution 9: x = (0.000977, 0.000977)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  0.0079,   0.0079], ||∇f1|| = 0.0111
  ∇f2 = [ -9.9980,  -9.9980], ||∇f2|| = 14.1394

Constraint Status:
  C1 active: False (value: -0.009764)
  C2 active: False (value: -65.290236)

Sensitivity Analysis:
  Avg sensitivity (f1): 0.0062
  Avg sensitivity (f2): 7.8469
  Max sensitivity (f1): 0.0114
  Max sensitivity (f2): 14.0266
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: True
  Stability score: 11.0018
  Classification: MODERATELY STABLE

============================================================
STABILITY SUMMARY TABLE
============================================================
Sol   x1         x2         ||∇f1||    ||∇f2||    Score      Class               
------------------------------------------------------------
1     0.0000     0.0000     0.0001     14.1421    13.8357    MODERATELY STABLE   
2     0.0000     0.0000     0.0001     14.1421    13.2783    MODERATELY STABLE   
3     2.5020     1.0000     21.5552    9.4319     26.4986    WEAKLY STABLE       
4     1.0000     2.8301     24.0125    9.1013     25.3363    WEAKLY STABLE       
5     1.0000     1.0000     11.3138    11.3137    18.6927    WEAKLY STABLE       
6     1.0000     2.0000     17.8886    10.0000    24.0916    WEAKLY STABLE       
7     1.0000     1.0000     11.3138    11.3137    18.9736    WEAKLY STABLE       
8     0.2500     0.2500     2.8285     13.4350    13.8187    MODERATELY STABLE   
9     0.0010     0.0010     0.0111     14.1394    11.0018    MODERATELY STABLE   
============================================================

6. Decision Space, Pareto Front, and Stability Plots Generated
------------------------------------------------------------

============================================================
Found 9 Pareto optimal solutions

Most stable solution: #9
  x = (0.000977, 0.000977)
  Stability class: MODERATELY STABLE
  Stability score: 11.0018
============================================================
PASSED                      [ 25%]
bnh/py/test_z3_minimax.py::test_z3_minimax 
-------------------------------- live log call ---------------------------------
======================================================================
BNH Multi-Objective Optimization with Minimax Stability Analysis
======================================================================

Minimax Philosophy:
  - Optimizes for WORST-CASE scenario
  - Ensures robustness under uncertainty
  - Provides strong guarantees against adversarial perturbations
======================================================================

1. Lexicographic Optimization (f1 primary):
----------------------------------------------------------------------
  Solution: x=(0.000000, 0.000000)
  Objectives: f1=0.0000, f2=50.0000

2. Lexicographic Optimization (f2 primary):
----------------------------------------------------------------------
  Solution: x=(0.000000, 0.000000)
  Objectives: f1=0.0000, f2=50.0000

3. Weighted Sum Method:
----------------------------------------------------------------------
  w=(0.1,0.9): x=(3.0000,1.0000), f1=40.0000, f2=20.0000
  w=(0.2,0.8): x=(1.0000,2.8301), f1=36.0374, f2=20.7086
  w=(0.4,0.6): x=(1.0000,1.0000), f1=8.0000, f2=32.0000
  w=(0.5,0.5): x=(1.0000,1.0000), f1=8.0000, f2=32.0000
  w=(0.6,0.4): x=(1.0000,1.0000), f1=8.0000, f2=32.0000
  w=(0.8,0.2): x=(0.0000,0.0000), f1=0.0000, f2=50.0000
  w=(0.9,0.1): x=(0.1250,0.1250), f1=0.1250, f2=47.5312

======================================================================
MINIMAX STABILITY ANALYSIS
======================================================================

Solution 1: x = (0.000000, 0.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.000000
    f2 = 50.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.000004 (399.98%)
    Δf2 = 0.000000 (0.00%)

  Robustness Assessment:
    Score: 3.999756 (lower is better)
    Class: NOT ROBUST

Solution 2: x = (0.000000, 0.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.000000
    f2 = 50.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.000004 (399.98%)
    Δf2 = 0.000000 (0.00%)

  Robustness Assessment:
    Score: 3.999756 (lower is better)
    Class: NOT ROBUST

Solution 3: x = (3.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 40.000000
    f2 = 20.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.025301 (0.06%)
    Δf2 = 0.008942 (0.04%)

  Robustness Assessment:
    Score: 0.001080 (lower is better)
    Class: HIGHLY ROBUST

Solution 4: x = (1.000000, 2.830078)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 36.037369
    f2 = 20.708561

  Worst-case increases (within ε=0.001):
    Δf1 = 0.024008 (0.07%)
    Δf2 = 0.009102 (0.04%)

  Robustness Assessment:
    Score: 0.001106 (lower is better)
    Class: HIGHLY ROBUST

Solution 5: x = (1.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 8.000000
    f2 = 32.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.011312 (0.14%)
    Δf2 = 0.011309 (0.04%)

  Robustness Assessment:
    Score: 0.001767 (lower is better)
    Class: HIGHLY ROBUST

Solution 6: x = (1.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 8.000000
    f2 = 32.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.011312 (0.14%)
    Δf2 = 0.011309 (0.04%)

  Robustness Assessment:
    Score: 0.001767 (lower is better)
    Class: HIGHLY ROBUST

Solution 7: x = (1.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 8.000000
    f2 = 32.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.011312 (0.14%)
    Δf2 = 0.011309 (0.04%)

  Robustness Assessment:
    Score: 0.001767 (lower is better)
    Class: HIGHLY ROBUST

Solution 8: x = (0.000000, 0.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.000000
    f2 = 50.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.000004 (399.98%)
    Δf2 = 0.000000 (0.00%)

  Robustness Assessment:
    Score: 3.999756 (lower is better)
    Class: NOT ROBUST

Solution 9: x = (0.125000, 0.125000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.125000
    f2 = 47.531250

  Worst-case increases (within ε=0.001):
    Δf1 = 0.001417 (1.13%)
    Δf2 = 0.013782 (0.03%)

  Robustness Assessment:
    Score: 0.011630 (lower is better)
    Class: HIGHLY ROBUST

======================================================================
MINIMAX ROBUSTNESS SUMMARY
======================================================================
Sol   x1         x2         Δf1(%)     Δf2(%)     Score      Robustness        
----------------------------------------------------------------------
1     0.0000     0.0000     399.98     0.00       3.9998     NOT ROBUST        
2     0.0000     0.0000     399.98     0.00       3.9998     NOT ROBUST        
3     3.0000     1.0000     0.06       0.04       0.0011     HIGHLY ROBUST     
4     1.0000     2.8301     0.07       0.04       0.0011     HIGHLY ROBUST     
5     1.0000     1.0000     0.14       0.04       0.0018     HIGHLY ROBUST     
6     1.0000     1.0000     0.14       0.04       0.0018     HIGHLY ROBUST     
7     1.0000     1.0000     0.14       0.04       0.0018     HIGHLY ROBUST     
8     0.0000     0.0000     399.98     0.00       3.9998     NOT ROBUST        
9     0.1250     0.1250     1.13       0.03       0.0116     HIGHLY ROBUST     
======================================================================

Most Robust Solution: #3
  x = (3.000000, 1.000000)
  Worst-case score: 0.001080
  Class: HIGHLY ROBUST
======================================================================

✓ Visualization complete
PASSED                        [ 37%]
constraint_dora/py/test_z3_nonlinear.py::test_constraint_dora 
-------------------------------- live log call ---------------------------------
============================================================
METHOD 1: Z3 Solver
============================================================
Z3 Solution:
  x1 = 0.894400
  x2 = 0.447266
  f(x1, x2) = 1.527867
  Constraint check: x1^2 + x2^2 = 0.999998

============================================================
METHOD 2: Scipy (Recommended for this problem)
============================================================
Scipy Solution:
  x1 = 0.894429
  x2 = 0.447211
  f(x1, x2) = 1.527864
  Constraint check: x1^2 + x2^2 = 1.000000
  Success: True

============================================================
ANALYTICAL SOLUTION
============================================================
Analytical Solution:
  x1 = 0.894427
  x2 = 0.447214
  f(x1, x2) = 1.527864
  Constraint check: x1^2 + x2^2 = 1.000000

============================================================
EXPLANATION
============================================================
The minimum occurs at the point on the unit circle
closest to (2, 1). This is found by moving from the
origin toward (2, 1) until hitting the circle boundary.
============================================================
{'z3': (0.8944, 0.4473), 'slsqp': (0.8944, 0.4472), 'linalg': (0.8944, 0.4472)}
PASSED     [ 50%]
shekel/py/test_keras.py::test_keras SKIPPED (Skipping Keras tests fo...) [ 62%]
shekel/py/test_keras2onnx.py::test_keras2onnx SKIPPED (Skipping Kera...) [ 75%]
shekel/py/test_shgo_keras.py::test_shgo_nn SKIPPED (Skipping Keras t...) [ 87%]
shekel/py/test_shgo_shekel_keras2onnx.py::test_shgo_shekel_onnx SKIPPED  [100%]

=============================== warnings summary ===============================
../../../../../usr/local/lib/python3.14/dist-packages/z3/z3core.py:5
  /usr/local/lib/python3.14/dist-packages/z3/z3core.py:5: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
    import pkg_resources

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
====== 4 passed, 4 skipped, 22 deselected, 1 warning in 67.80s (0:01:07) =======
env PYTHONDONTWRITEBYTECODE=1 /usr/local/lib/python3.14/dist-packages/bin/pytest -v -s -m "not forked" /tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples
============================= test session starts ==============================
platform linux -- Python 3.14.2, pytest-9.0.2, pluggy-1.6.0 -- /usr/bin/python3.14
cachedir: .pytest_cache
rootdir: /tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples
plugins: forked-1.6.0, typeguard-4.4.4, mock-3.15.1
collecting ... autograd only supports numpy < 2.0.0 versions.
======================================================================
EXAMPLE 1: Simple Quadratic Optimization
Minimize (x-1)² + (y-1)² subject to x + y >= 0.5
======================================================================
collected 30 items / 8 deselected / 22 selected

bnh/py/test_nsga2.py::test_nsga2 
## Problem definition
```
"Boundary definition"
    def __init__(self):
        super().__init__(n_var=2, n_obj=2, n_ieq_constr=2, vtype=float)
        self.xl = np.zeros(self.n_var)
        self.xu = np.array([5.0, 3.0])

"Functions (F) and inequality constraints (G) definitions"
    def _evaluate(self, x, out, *args, **kwargs):
        f1 = 4 * x[:, 0] ** 2 + 4 * x[:, 1] ** 2
        f2 = (x[:, 0] - 5) ** 2 + (x[:, 1] - 5) ** 2
        g1 = (1 / 25) * ((x[:, 0] - 5) ** 2 + x[:, 1] ** 2 - 25)
        g2 = -1 / 7.7 * ((x[:, 0] - 8) ** 2 + (x[:, 1] + 3) ** 2 - 7.7)

        out["F"] = anp.column_stack([f1, f2])
        out["G"] = anp.column_stack([g1, g2])

"Expected Pareto front"
    def _calc_pareto_front(self, n_points=100):
        x1 = np.linspace(0, 5, n_points)
        x2 = np.linspace(0, 5, n_points)
        x2[x1 >= 3] = 3

        X = np.column_stack([x1, x2])
        return self.evaluate(X, return_values_of=["F"])

```
PASSED
c3dtlz4/py/test_gpsampler.py::test_gpsampler [I 2026-01-23 19:16:29,178] A new study created in RDB with name: my_study
[I 2026-01-23 19:16:29,272] Trial 0 finished with values: [1.2569645750397653, 4.4185249455598885e-43] and parameters: {'x0': 0.3745401188473625, 'x1': 0.9507143064099162, 'x2': 0.7319939418114051}.
[I 2026-01-23 19:16:29,346] Trial 1 finished with values: [1.236662945761789, 1.0145778959154135e-22] and parameters: {'x0': 0.5986584841970366, 'x1': 0.15601864044243652, 'x2': 0.15599452033620265}.
[I 2026-01-23 19:16:29,416] Trial 2 finished with values: [1.1443092153344159, 4.571144650098741e-124] and parameters: {'x0': 0.05808361216819946, 'x1': 0.8661761457749352, 'x2': 0.6011150117432088}.
[I 2026-01-23 19:16:29,487] Trial 3 finished with values: [1.4506544962685188, 2.3198580035410804e-15] and parameters: {'x0': 0.7080725777960455, 'x1': 0.020584494295802447, 'x2': 0.9699098521619943}.
[I 2026-01-23 19:16:29,560] Trial 4 finished with values: [1.1839841387381274, 2.017890472548048e-08] and parameters: {'x0': 0.8324426408004217, 'x1': 0.21233911067827616, 'x2': 0.18182496720710062}.
[I 2026-01-23 19:16:29,641] Trial 5 finished with values: [1.0389339803486743, 3.578564831812836e-74] and parameters: {'x0': 0.18340450985343382, 'x1': 0.3042422429595377, 'x2': 0.5247564316322378}.
[I 2026-01-23 19:16:29,711] Trial 6 finished with values: [1.0560963419602245, 5.789891256550866e-37] and parameters: {'x0': 0.43194501864211576, 'x1': 0.2912291401980419, 'x2': 0.6118528947223795}.
[I 2026-01-23 19:16:29,781] Trial 7 finished with values: [1.0610630040604079, 4.7574085951972936e-86] and parameters: {'x0': 0.13949386065204183, 'x1': 0.29214464853521815, 'x2': 0.3663618432936917}.
[I 2026-01-23 19:16:29,851] Trial 8 finished with values: [1.171521166079494, 1.4723771522333772e-34] and parameters: {'x0': 0.45606998421703593, 'x1': 0.7851759613930136, 'x2': 0.19967378215835974}.
[I 2026-01-23 19:16:29,920] Trial 9 finished with values: [1.2142476806598177, 2.491970914729005e-29] and parameters: {'x0': 0.5142344384136116, 'x1': 0.5924145688620425, 'x2': 0.046450412719997725}.
[I 2026-01-23 19:16:32,477] Trial 10 finished with values: [1.5, 3.249132054563685e-24] and parameters: {'x0': 0.5772920306987445, 'x1': 1.0, 'x2': 1.0}.
[I 2026-01-23 19:16:32,978] Trial 11 finished with values: [1.4810590572975793, 2.2471687400140787e-21] and parameters: {'x0': 0.616381271404474, 'x1': 0.0, 'x2': 0.9806860277744498}.
[I 2026-01-23 19:16:33,328] Trial 12 finished with values: [1.3962865006473393, 0.0] and parameters: {'x0': 0.0, 'x1': 0.0, 'x2': 0.8824741829814651}.
[I 2026-01-23 19:16:33,726] Trial 13 finished with values: [1.4319600251053506, 4.613008316428876e-65] and parameters: {'x0': 0.22548587130879116, 'x1': 0.0, 'x2': 0.9265677262819476}.
[I 2026-01-23 19:16:34,119] Trial 14 finished with values: [1.4657790846195586, 3.1399921722830447e-18] and parameters: {'x0': 0.6627464371294791, 'x1': 1.0, 'x2': 0.9645202736367473}.
[I 2026-01-23 19:16:34,493] Trial 15 finished with values: [1.445114846117425, 2.5479755591876564e-80] and parameters: {'x0': 0.1586725240275914, 'x1': 1.0, 'x2': 0.9417180617966907}.
[I 2026-01-23 19:16:34,823] Trial 16 finished with values: [1.4458575475275077, 6.445293888426763e-57] and parameters: {'x0': 0.2719756187229194, 'x1': 1.0, 'x2': 0.9425579595120934}.
[I 2026-01-23 19:16:35,358] Trial 17 finished with values: [1.255194971651199, 6.912014307771002e-107] and parameters: {'x0': 0.08618818508327279, 'x1': 0.3403540168938965, 'x2': 0.9792787620261143}.
[I 2026-01-23 19:16:36,019] Trial 18 finished with values: [1.4448556966461257, 0.0] and parameters: {'x0': 0.0, 'x1': 0.9998164559591163, 'x2': 0.9416324342692624}.
[I 2026-01-23 19:16:36,591] Trial 19 finished with values: [1.4445869046211222, 4.585494496743527e-44] and parameters: {'x0': 0.36564128475868873, 'x1': 0.0030928341581690846, 'x2': 0.9446011394004309}.
[I 2026-01-23 19:16:37,223] Trial 20 finished with values: [1.4443633775253015, 3.095705331360937e-36] and parameters: {'x0': 0.43787447810613506, 'x1': 0.9996722436046344, 'x2': 0.941238061024219}.
[I 2026-01-23 19:16:37,925] Trial 21 finished with values: [1.443408516454606, 4.3094579972876635e-45] and parameters: {'x0': 0.35709941202732837, 'x1': 0.9997098583444072, 'x2': 0.9401120015723482}.
[I 2026-01-23 19:16:38,522] Trial 22 finished with values: [1.463130590964571, 2.888001049597235e-17] and parameters: {'x0': 0.6776289374052729, 'x1': 0.03833931187010174, 'x2': 1.0}.
[I 2026-01-23 19:16:39,054] Trial 23 finished with values: [1.443973196351162, 1.7300030254632066e-25] and parameters: {'x0': 0.5608202394978651, 'x1': 0.9994244463105117, 'x2': 0.9410764319010944}.
[I 2026-01-23 19:16:39,602] Trial 24 finished with values: [1.3849349118817273, 8.362307825117555e-13] and parameters: {'x0': 0.7513594778738962, 'x1': 0.8673348770287508, 'x2': 1.0}.
[I 2026-01-23 19:16:40,110] Trial 25 finished with values: [1.5, 0.0] and parameters: {'x0': 0.0, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:16:40,805] Trial 26 finished with values: [1.4381781661444666, 5.444554551490792e-68] and parameters: {'x0': 0.21077558233539917, 'x1': 0.9674237804847422, 'x2': 0.03128561406564639}.
[I 2026-01-23 19:16:41,399] Trial 27 finished with values: [1.4450624040166224, 8.973847889825106e-96] and parameters: {'x0': 0.1111654223264895, 'x1': 0.9718963397733132, 'x2': 0.028432137942827843}.
[I 2026-01-23 19:16:42,062] Trial 28 finished with values: [1.4545308740819558, 0.0] and parameters: {'x0': 0.0, 'x1': 0.04774910272951829, 'x2': 1.0}.
[I 2026-01-23 19:16:42,665] Trial 29 finished with values: [1.443456628945614, 1.3257021548609495e-118] and parameters: {'x0': 0.06571571287991912, 'x1': 0.9997015324402695, 'x2': 0.9401761095544151}.
[I 2026-01-23 19:16:43,350] Trial 30 finished with values: [1.4443394761144561, 1.1982523729722923e-29] and parameters: {'x0': 0.5095978551114642, 'x1': 0.009069976469102108, 'x2': 0.9509181612115531}.
[I 2026-01-23 19:16:43,985] Trial 31 finished with values: [1.4466735412988014, 6.783807214776015e-54] and parameters: {'x0': 0.2915746955448456, 'x1': 0.05652109261115761, 'x2': 1.0}.
[I 2026-01-23 19:16:44,656] Trial 32 finished with values: [1.4442558247808832, 1.4721363098379749e-49] and parameters: {'x0': 0.3221972580606223, 'x1': 0.9695064368543467, 'x2': 0.026904311440889787}.
[I 2026-01-23 19:16:45,256] Trial 33 finished with values: [1.4462906705701832, 4.271496767328079e-15] and parameters: {'x0': 0.7124297492536091, 'x1': 0.9430470297498714, 'x2': 0.9999999999999999}.
[I 2026-01-23 19:16:45,870] Trial 34 finished with values: [8.874509324427002e-17, 1.449317359193815] and parameters: {'x0': 1.0, 'x1': 0.05355027248993058, 'x2': 0.9999999999999999}.
[I 2026-01-23 19:16:48,301] Trial 35 finished with values: [1.4421472900270447, 1.8370418696915614e-05] and parameters: {'x0': 0.8893851946560436, 'x1': 0.061653915103546945, 'x2': 0.9999999999999999}.
[I 2026-01-23 19:16:48,772] Trial 36 finished with values: [1.2860347520190483, 0.00732682699752418] and parameters: {'x0': 0.9453560035560985, 'x1': 0.2333758939789026, 'x2': 0.963645564221677}.
[I 2026-01-23 19:16:49,342] Trial 37 finished with values: [1.3979215950049932, 0.013402146762977827] and parameters: {'x0': 0.950288740884737, 'x1': 0.0, 'x2': 0.8846892746428605}.
[I 2026-01-23 19:16:49,918] Trial 38 finished with values: [1.499773926794213, 0.026041668692059296] and parameters: {'x0': 0.9559491578123709, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-23 19:16:50,520] Trial 39 finished with values: [1.2507159928328349, 0.10107387451756286] and parameters: {'x0': 0.970742794965871, 'x1': 0.12552098979677392, 'x2': 0.8384654331851219}.
[I 2026-01-23 19:16:51,222] Trial 40 finished with values: [1.3328017906425575, 0.24152549034686965] and parameters: {'x0': 0.9785294451073929, 'x1': 0.1767211714132525, 'x2': 1.0}.
[I 2026-01-23 19:16:52,114] Trial 41 finished with values: [1.4328849389260425, 0.14666799744849854] and parameters: {'x0': 0.9730271148458519, 'x1': 0.06368389887687556, 'x2': 1.0}.
[I 2026-01-23 19:16:52,720] Trial 42 finished with values: [1.333992159994511, 0.2629425225833838] and parameters: {'x0': 0.9793333800058809, 'x1': 0.0, 'x2': 0.8311487010167008}.
[I 2026-01-23 19:16:53,158] Trial 43 finished with values: [1.412236542216437, 0.5055570678257422] and parameters: {'x0': 0.9849211636388685, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-23 19:16:53,901] Trial 44 finished with values: [1.1974161959488412, 0.5105739589446882] and parameters: {'x0': 0.9864893950301072, 'x1': 0.0756990415680327, 'x2': 0.8488480742575549}.
[I 2026-01-23 19:16:54,725] Trial 45 finished with values: [1.318172221905542, 0.4928733534366517] and parameters: {'x0': 0.9853155741934119, 'x1': 0.040451134193926895, 'x2': 0.942852105669598}.
[I 2026-01-23 19:16:55,229] Trial 46 finished with values: [1.2613904163758827, 0.6637045325742482] and parameters: {'x0': 0.9883039032603425, 'x1': 0.08125736674161979, 'x2': 1.0}.
[I 2026-01-23 19:16:55,914] Trial 47 finished with values: [1.354312703621413, 0.539278551869976] and parameters: {'x0': 0.9858812308189343, 'x1': 0.04422311000879493, 'x2': 1.0}.
[I 2026-01-23 19:16:56,531] Trial 48 finished with values: [1.1762519994276803, 0.7403325974990232] and parameters: {'x0': 0.9897699973990214, 'x1': 0.05972497173503773, 'x2': 0.9427188954676303}.
[I 2026-01-23 19:16:57,143] Trial 49 finished with values: [1.4086850958794073, 0.014756195786579529] and parameters: {'x0': 0.9511308040150737, 'x1': 0.10155002762709123, 'x2': 1.0}.
[I 2026-01-23 19:16:57,812] Trial 50 finished with values: [1.5, 5.448261629847529e-09] and parameters: {'x0': 0.8196726952048367, 'x1': 0.0, 'x2': 0.9999999999999999}.
[I 2026-01-23 19:16:58,381] Trial 51 finished with values: [1.2502094463894986, 3.2056528914080544e-22] and parameters: {'x0': 0.6055195066980048, 'x1': 0.5144722627636019, 'x2': 1.0}.
[I 2026-01-23 19:16:59,161] Trial 52 finished with values: [1.2397226711499003, 0.160165830969946] and parameters: {'x0': 0.9752753876226731, 'x1': 0.49488554639808036, 'x2': 1.0}.
[I 2026-01-23 19:16:59,750] Trial 53 finished with values: [0.8925557742864271, 1.172973175213934] and parameters: {'x0': 0.994668248743633, 'x1': 0.026768851874987096, 'x2': 1.0}.
[I 2026-01-23 19:17:00,552] Trial 54 finished with values: [1.4381504402274703, 0.017328076151746984] and parameters: {'x0': 0.9524628631173203, 'x1': 0.0542626208461052, 'x2': 0.9894619669217432}.
[I 2026-01-23 19:17:01,758] Trial 55 finished with values: [7.806015666202219e-17, 1.2748191023954125] and parameters: {'x0': 1.0, 'x1': 1.0, 'x2': 0.6575407959717496}.
[I 2026-01-23 19:17:03,811] Trial 56 finished with values: [7.115937412943625e-17, 1.1621207711314017] and parameters: {'x0': 1.0, 'x1': 0.45467107825829617, 'x2': 0.9000825664598947}.
[I 2026-01-23 19:17:04,486] Trial 57 finished with values: [8.996755200178027e-17, 1.4692816257621248] and parameters: {'x0': 1.0, 'x1': 0.03172483969131484, 'x2': 0.0}.
[I 2026-01-23 19:17:05,379] Trial 58 finished with values: [9.184850993605148e-17, 1.5] and parameters: {'x0': 1.0, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:06,213] Trial 59 finished with values: [1.4999999999999603, 3.4534186058961785e-07] and parameters: {'x0': 0.8543983104594208, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-23 19:17:06,763] Trial 60 finished with values: [1.4999738186944656, 0.008862461912037569] and parameters: {'x0': 0.9457001495716729, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:07,432] Trial 61 finished with values: [1.49597040326911, 3.344106942159469e-11] and parameters: {'x0': 0.7789908792292021, 'x1': 0.0, 'x2': 0.9959540334235724}.
[I 2026-01-23 19:17:08,154] Trial 62 finished with values: [1.4999999999892142, 5.688385951871503e-06] and parameters: {'x0': 0.8786740068618826, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:08,880] Trial 63 finished with values: [1.5, 3.1557294531968846e-58] and parameters: {'x0': 0.2637964132348673, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-23 19:17:09,646] Trial 64 finished with values: [1.5, 7.480230743629189e-32] and parameters: {'x0': 0.48419136339291113, 'x1': 1.0, 'x2': 0.9999999999999999}.
[I 2026-01-23 19:17:10,286] Trial 65 finished with values: [1.5, 4.0047195573294674e-10] and parameters: {'x0': 0.7985527502075314, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:10,766] Trial 66 finished with values: [1.5, 2.5241077372173905e-13] and parameters: {'x0': 0.7418207335939818, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-23 19:17:11,273] Trial 67 finished with values: [1.5, 1.3226336189894698e-145] and parameters: {'x0': 0.03527705191527296, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-23 19:17:11,963] Trial 68 finished with values: [1.4999999999999991, 5.551585722856168e-08] and parameters: {'x0': 0.838922918712233, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:12,577] Trial 69 finished with values: [1.5, 1.3724523623822954e-16] and parameters: {'x0': 0.688102065412457, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:13,137] Trial 70 finished with values: [1.5, 8.983496600274516e-21] and parameters: {'x0': 0.624902615970522, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:14,002] Trial 71 finished with values: [1.3360437809792098, 0.12439732102478535] and parameters: {'x0': 0.9721117701172168, 'x1': 0.9610055863497126, 'x2': 0.1404219549675764}.
[I 2026-01-23 19:17:14,710] Trial 72 finished with values: [1.4999999788104759, 0.0002521280861340979] and parameters: {'x0': 0.9126286553499576, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:15,448] Trial 73 finished with values: [1.0720099406560417, 0.7904386915070076] and parameters: {'x0': 0.9909892400741751, 'x1': 0.9738849481650971, 'x2': 0.1723592549270844}.
[I 2026-01-23 19:17:17,941] Trial 74 finished with values: [1.5, 1.569773932032027e-33] and parameters: {'x0': 0.46583944509070774, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-23 19:17:18,538] Trial 75 finished with values: [1.5, 5.92727394205842e-54] and parameters: {'x0': 0.29107602712373115, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:19,306] Trial 76 finished with values: [1.5, 1.2627223706127123e-26] and parameters: {'x0': 0.5461236289090728, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-23 19:17:19,989] Trial 77 finished with values: [1.5, 7.198941849896501e-120] and parameters: {'x0': 0.06380439210513922, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-23 19:17:20,997] Trial 78 finished with values: [1.5, 1.664529328223761e-138] and parameters: {'x0': 0.04154232539309865, 'x1': 0.9999999999999999, 'x2': 0.0}.
[I 2026-01-23 19:17:21,669] Trial 79 finished with values: [1.5, 1.5607630534113503e-39] and parameters: {'x0': 0.40570583918184533, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-23 19:17:22,176] Trial 80 finished with values: [1.5, 3.290701106035644e-36] and parameters: {'x0': 0.4379764640173388, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-23 19:17:22,963] Trial 81 finished with values: [1.5, 7.585881684617074e-38] and parameters: {'x0': 0.4217721983622859, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-23 19:17:23,609] Trial 82 finished with values: [1.5, 1.3668866691685482e-77] and parameters: {'x0': 0.16890216394787, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-23 19:17:24,256] Trial 83 finished with values: [1.5, 3.908852672198784e-72] and parameters: {'x0': 0.19151305389665163, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-23 19:17:24,825] Trial 84 finished with values: [1.368738774109001, 6.413949213370657e-74] and parameters: {'x0': 0.183969933214782, 'x1': 0.8445849301826778, 'x2': 0.0}.
[I 2026-01-23 19:17:25,477] Trial 85 finished with values: [1.5, 7.821154111569983e-94] and parameters: {'x0': 0.11620119573885204, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-23 19:17:26,046] Trial 86 finished with values: [1.3798773185964155, 1.093481288079773e-84] and parameters: {'x0': 0.14355836716770828, 'x1': 0.13961504110685263, 'x2': 1.0}.
[I 2026-01-23 19:17:26,739] Trial 87 finished with values: [1.2554287177209893, 0.03870803989011958] and parameters: {'x0': 0.9614517999200447, 'x1': 0.5776228659053626, 'x2': 0.0}.
[I 2026-01-23 19:17:27,592] Trial 88 finished with values: [1.4321138234773971, 2.9377677407990403e-73] and parameters: {'x0': 0.18670639798193514, 'x1': 0.926747962475976, 'x2': 1.0}.
[I 2026-01-23 19:17:28,308] Trial 89 finished with values: [1.5, 4.826372123051888e-45] and parameters: {'x0': 0.3573667122411882, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-23 19:17:28,902] Trial 90 finished with values: [1.5, 4.604223869502085e-71] and parameters: {'x0': 0.19629509740723164, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-23 19:17:29,474] Trial 91 finished with values: [1.5, 6.601550344386256e-62] and parameters: {'x0': 0.24236748039595604, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:30,170] Trial 92 finished with values: [1.5, 1.9509996739517576e-38] and parameters: {'x0': 0.4160834680374379, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:30,853] Trial 93 finished with values: [0.20528076155011513, 1.197154164721039] and parameters: {'x0': 0.9988565040667228, 'x1': 0.6922106575700966, 'x2': 0.9215232759039778}.
[I 2026-01-23 19:17:31,532] Trial 94 finished with values: [1.4599266499683603, 5.603920856727767e-36] and parameters: {'x0': 0.4404335671196255, 'x1': 0.04182246893986581, 'x2': 1.0}.
[I 2026-01-23 19:17:32,267] Trial 95 finished with values: [0.31920819711066434, 1.2632690901735897] and parameters: {'x0': 0.998286874650178, 'x1': 0.21186748549868614, 'x2': 0.9689927504745048}.
[I 2026-01-23 19:17:32,992] Trial 96 finished with values: [1.5, 6.40152603111393e-32] and parameters: {'x0': 0.4834379323701231, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-23 19:17:33,947] Trial 97 finished with values: [1.5, 1.1719057694741297e-91] and parameters: {'x0': 0.12217063060310909, 'x1': 1.0, 'x2': 1.0}.
[I 2026-01-23 19:17:34,664] Trial 98 finished with values: [1.5, 5.705079381198967e-12] and parameters: {'x0': 0.7653154935392955, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-23 19:17:35,477] Trial 99 finished with values: [1.5, 2.2344074342642595e-25] and parameters: {'x0': 0.5620429599857228, 'x1': 1.0, 'x2': 0.0}.

my_study /tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples/c3dtlz4/py/example.db 100
Number of trials = 100
Trial: 10 [Elapsed time: 13.1] Elapsed CPU time: 8.3 seconds
Trial: 20 [Elapsed time: 19.8] Elapsed CPU time: 28.1 seconds
Trial: 30 [Elapsed time: 25.9] Elapsed CPU time: 51.4 seconds
Trial: 40 [Elapsed time: 33.7] Elapsed CPU time: 75.4 seconds
Trial: 50 [Elapsed time: 40.4] Elapsed CPU time: 100.3 seconds
Trial: 60 [Elapsed time: 49.4] Elapsed CPU time: 129.5 seconds
Trial: 70 [Elapsed time: 55.8] Elapsed CPU time: 152.5 seconds
Trial: 80 [Elapsed time: 64.9] Elapsed CPU time: 179.8 seconds
Trial: 90 [Elapsed time: 71.5] Elapsed CPU time: 203.7 seconds
Trial: 100 [Elapsed time: 78.7] Elapsed CPU time: 229.7 seconds
================================================================================
DATAFRAME COMPARISON REPORT
================================================================================

================================================================================
SUMMARY:
  Total mismatches found: 0
  Float threshold: 0.01%
  ✓ DataFrames match!
================================================================================
PASSED
c3dtlz4/py/test_variables_transformation.py::test_variables_transformation 
================================================================================
DATAFRAME COMPARISON REPORT
================================================================================

────────────────────────────────────────────────────────────────────────────────
Column: 'F1' - 4 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: 1.4445869046211222 != 1.447670173156161 (rel_diff=2.134e-03)
  Line 78: 1.495003685794626 != 1.497532147728052 (rel_diff=1.691e-03)
  Line 117: 1.47682371956128 != 1.478969064161009 (rel_diff=1.453e-03)
  Line 327: 1.4454059271875428 != 1.447548460110874 (rel_diff=1.482e-03)

────────────────────────────────────────────────────────────────────────────────
Column: 'F2' - 4 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: 4.585494496743527e-44 != 4.595281592870508e-44 (rel_diff=2.134e-03)
  Line 78: 1.8417450089905839e-121 != 1.844859905756681e-121 (rel_diff=1.691e-03)
  Line 117: 1.1232288126754917e-25 != 1.124860498864991e-25 (rel_diff=1.453e-03)
  Line 327: 1.171471360456363e-11 != 1.173207838708808e-11 (rel_diff=1.482e-03)

────────────────────────────────────────────────────────────────────────────────
Column: 'C1' - 11 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: -0.0033911465369933 != -0.009996300378869 (rel_diff=1.948e+00)
  Line 78: -0.1137832561935472 != -0.119453422682953 (rel_diff=4.983e-02)
  Line 82: -0.1203673055076137 != -0.1211223916803213 (rel_diff=6.273e-03)
  Line 98: -0.1221311482578988 != -0.125 (rel_diff=2.349e-02)
  Line 105: -0.124484267356677 != -0.125 (rel_diff=4.143e-03)
  Line 106: -0.1126779959521606 != -0.112797464282956 (rel_diff=1.060e-03)
  Line 107: -0.1119495107572825 != -0.1134573372909615 (rel_diff=1.347e-02)
  Line 117: -0.0733905089878543 != -0.078122694624534 (rel_diff=6.448e-02)
  Line 279: -0.4356605212037681 != -0.4370790202635428 (rel_diff=3.256e-03)
  Line 311: -0.5384930707665228 != -0.5406742204840589 (rel_diff=4.050e-03)
  Line 327: -0.0051438489582229 != -0.0097351992862068 (rel_diff=8.926e-01)

────────────────────────────────────────────────────────────────────────────────
Column: 'C2' - 8 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: -1.086831325002835 != -1.09574893024599 (rel_diff=8.205e-03)
  Line 78: -1.2350360205395168 != -1.242602533478992 (rel_diff=6.127e-03)
  Line 98: -1.246174321814996 != -1.249999999999629 (rel_diff=3.070e-03)
  Line 107: -1.2325880753094638 != -1.234600970274428 (rel_diff=1.633e-03)
  Line 117: -1.1810082986588144 != -1.187349492745291 (rel_diff=5.369e-03)
  Line 279: -0.0043119169509244 != -0.0055482887859028 (rel_diff=2.867e-01)
  Line 311: -0.0012322408512295 != -0.0030787929590615 (rel_diff=1.499e+00)
  Line 327: -1.0891982943400942 != -1.095396544360563 (rel_diff=5.691e-03)

================================================================================
SUMMARY:
  Total mismatches found: 27
  Float threshold: 0.1%
================================================================================
PASSED
cattlefeed/py/test_shgo_with_constraints.py::test_optimization_ex 
x1=0.636,x2=0.000,x3=0.313,x4=0.052,f=29.894
g1=-0.000,g2=-0.000,g3=0.000
PASSED
constraint_dora/py/test_slsqp.py::test_constraint_dora 
============================================================
Brute force result: x1 = 0.89039 x2 = 0.45495 f(x*) = 1.52831
============================================================
Brute force result polar: r = 0.99989 theta = 0.47238 f(x*) = 1.52831
Brute force result polar Cartesian coordinate system: x1 = 0.89039 x2 = 0.45496 f(x*) = 1.52831
============================================================
============================================================
CONSTRAINED OPTIMIZATION RESULTS
============================================================
Success: True
Message: Optimization terminated successfully

Constraint value: -0.000000 (should be >= 0)
Constraint satisfied: True

Optimal solution: x1 = 0.894429 x2 = 0.447211 f(x*) = 1.527864

Expected result (analytical solution):
  2/√5 = 0.894427, 1/√5 = 0.447214
  (√5 - 1)² = 1.527864
x1 error    = 0.000154%
x2 error    = -0.000614%
f(x*) error = -0.000001%
============================================================
PASSED
deap/py/test_easimple.py::test_easimple 
======================================================================
GENETIC ALGORITHM OPTIMIZATION: Production Planning with DEAP
======================================================================

Genetic Algorithm Parameters:
  Population Size: 100
  Generations: 50
  Crossover Probability: 0.7
  Mutation Probability: 0.2

Running optimization...

gen	nevals	avg      	std     	min      	max      
0  	100   	-17,882.9	5,345.78	-34,403.7	-9,793.94
1  	78    	-13,706.9	3,803.25	-25,887.8	-9,229.37
2  	79    	-10,920.4	1,631.75	-17,608.7	-9,166.56
3  	69    	-10,021.5	918.106 	-15,691.2	-9,163.4 
4  	77    	-9,608.92	439.015 	-11,981  	-9,136.48
5  	79    	-9,110.25	1,758.07	-10,351.2	849.448  
6  	71    	-8,623.82	2,588.86	-10,257.4	849.448  
7  	75    	-7,798.73	3,611.4 	-10,205.1	849.448  
8  	71    	-7,482.8 	3,884.68	-9,826.9 	849.448  
9  	71    	-6,980.22	4,251.6 	-10,087.7	849.448  
10 	78    	-5,799.1 	4,829.73	-10,281.5	857.208  
11 	73    	-4,452.96	5,031.82	-9,646.03	860.065  
12 	72    	-3,548.58	4,991.52	-9,617.57	861.499  
13 	75    	-3,224.47	4,930.94	-9,711.18	861.499  
14 	64    	-2,110.89	4,570.22	-9,597.36	854.187  
15 	81    	-2,821.48	4,859.18	-9,744.03	854.289  
16 	68    	-1,702.78	4,393.7 	-9,930.19	858.274  
17 	82    	-1,377.2 	4,176.56	-9,543.52	859.407  
18 	72    	-1,071.61	3,961.87	-9,713.61	859.442  
19 	69    	-1,167.8 	4,032.96	-9,805.4 	859.442  
20 	73    	-1,365.25	4,174.43	-9,455.29	860.465  
21 	76    	-1,060.88	3,950.93	-9,571   	860.465  
22 	71    	-1,965.19	4,521.65	-9,557.11	860.505  
23 	72    	-1,060.48	3,946.77	-9,677.02	860.505  
24 	74    	-949.666 	3,854.22	-9,370.66	860.541  
25 	70    	-2,261   	4,648.15	-9,492.27	860.537  
26 	73    	-1,554.44	4,291.31	-9,331.48	860.547  
27 	82    	-1,970.13	4,537.99	-10,059  	860.571  
28 	83    	-1,550   	4,282.05	-9,505.49	860.59   
29 	77    	-1,467.99	4,258.49	-9,643.01	860.595  
30 	71    	-1,160.52	4,037.29	-9,548.6 	860.595  
31 	71    	-1,661.77	4,366.03	-9,516.29	860.595  
32 	84    	-2,257.76	4,651.06	-9,417.8 	860.595  
33 	73    	-1,258.37	4,102.59	-9,564.84	860.595  
34 	82    	-1,467.6 	4,256.54	-9,583.28	860.595  
35 	79    	-1,565.68	4,311.56	-9,890.08	860.585  
36 	73    	-1,051.86	3,941.49	-9,405.88	860.597  
37 	74    	-1,452.29	4,225.93	-9,493.4 	860.597  
38 	71    	-1,848.21	4,454.11	-9,502.58	860.597  
39 	80    	-2,861.49	4,854.62	-9,843.34	860.597  
40 	87    	-1,750.6 	4,401.63	-9,378.1 	860.585  
41 	82    	-2,758.03	4,823.83	-9,617.04	860.587  
42 	81    	-2,161.99	4,614.21	-9,791.68	860.589  
43 	66    	-1,048.29	3,937.38	-9,417.34	860.586  
44 	75    	-1,046.67	3,935.72	-9,568.58	860.592  
45 	87    	-2,555.23	4,758.67	-9,560.92	860.592  
46 	78    	-1,556.01	4,292.61	-9,575.45	860.592  
47 	74    	-1,453.36	4,229.98	-9,672.97	860.593  
48 	73    	-1,551.09	4,291.74	-9,380.59	860.594  
49 	78    	-1,553.38	4,292.55	-9,404.5 	860.594  
50 	67    	-1,751.95	4,405.98	-9,525.98	860.594  

======================================================================
OPTIMIZATION RESULTS
======================================================================

Best Profit: $861.50
Constraints Satisfied: True

----------------------------------------------------------------------
Optimal Production Quantities:
----------------------------------------------------------------------
Product  Quantity  Profit_per_Unit  Total_Profit
      A      7.79               30        233.75
      B      6.55               45        294.77
      C      2.19               50        109.30
      D      6.39               35        223.68

----------------------------------------------------------------------
Resource Utilization:
----------------------------------------------------------------------
     Resource  Used  Capacity  Utilization_%
  labor_hours 59.96       100          59.96
machine_hours 35.94        80          44.93
storage_space 15.16        50          30.31

----------------------------------------------------------------------
Constraint Verification:
----------------------------------------------------------------------
Total Weight: 59.96 (limit: 60)
Product B / Product A ratio: 84.07% (min: 30%)
All resource constraints: ✓ Satisfied
Minimum quantities: ✓ Satisfied

----------------------------------------------------------------------
Evolution Statistics:
----------------------------------------------------------------------
Generation 0 - Max Fitness: $-9793.94, Avg: $-17882.88
Generation 25 - Max Fitness: $860.54, Avg: $-2261.00
Generation 50 - Max Fitness: $860.59, Avg: $-1751.95

======================================================================
To use with your own CSV files:
  products_data = pd.read_csv('your_products.csv')
  resources_data = pd.read_csv('your_resources.csv')
  requirements_data = pd.read_csv('your_requirements.csv')
======================================================================
PASSED
eggholder/py/test_shgo.py::test_optimization_ex 
The first element of the sorted dataset:     512.0 404.00 -959.5797
Analytical solution [1]:                     512.0 404.23 -959.6407
Simplicial homology global optimization [2]: 512.0 404.23 -959.6407
Dual annealing [3]:                          512.0 404.23 -959.6407
Difference between SHGO and DA methods: -2.37e-14 %
[1] EGGHOLDER: https://www.sfu.ca/~ssurjano/egg.html
[2] SHGO: https://link.springer.com/article/10.1007/s10898-018-0645-y
[3] DA: https://www.jstatsoft.org/article/view/v060i06
[4] SCIPY: https://docs.scipy.org/doc/scipy/tutorial/optimize.html#global-optimization
PASSED
pyomo/py/test_bnh_csv.py::test_bnh_csv 
================================================================================
PARETO FRONT FROM CSV DATA (NO ANALYTICAL FUNCTIONS)
================================================================================

IMPORTANT: F1 and F2 are read ONLY from the DataFrame/CSV
No analytical functions are used for objectives!

================================================================================
RUNNING NSGA-II OPTIMIZATION
================================================================================

Loaded 10201 data points from CSV:
    X1    X2      F1       F2
0  0.0  0.00  0.0000  50.0000
1  0.0  0.03  0.0036  49.7009
2  0.0  0.06  0.0144  49.4036
3  0.0  0.09  0.0324  49.1081
4  0.0  0.12  0.0576  48.8144
5  0.0  0.15  0.0900  48.5225
6  0.0  0.18  0.1296  48.2324
7  0.0  0.21  0.1764  47.9441
8  0.0  0.24  0.2304  47.6576
9  0.0  0.27  0.2916  47.3729

================================================================================
CREATING INTERPOLATORS FROM CSV DATA
================================================================================

Optimization Settings:
  Objectives: F1 and F2 from CSV data (interpolated)
  Variables: X1 ∈ [0, 5], X2 ∈ [0, 3]
  Constraints:
    C1: (x1-5)² + x2² ≤ 25
    C2: (x1-8)² + (x2+3)² ≥ 7.7
  Population: 100
  Generations: 200

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |     33 |  0.000000E+00 |  0.1286804155 |             - |             -
     2 |      200 |     55 |  0.000000E+00 |  0.000000E+00 |  0.0094193678 |         ideal
     3 |      300 |     82 |  0.000000E+00 |  0.000000E+00 |  0.0081704835 |         ideal
     4 |      400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0148040405 |         ideal
     5 |      500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0409207317 |         nadir
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0090732873 |         nadir
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0247718724 |         nadir
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0039927802 |         nadir
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0123172364 |         nadir
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0087280146 |         nadir
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015404535 |             f
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027246646 |             f
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011898586 |             f
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021149169 |             f
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0040842312 |         nadir
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015131140 |             f
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029331235 |             f
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0040889592 |         nadir
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017141994 |             f
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025055737 |             f
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015819926 |             f
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023995146 |             f
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032100807 |             f
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017333335 |             f
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028775823 |             f
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014343407 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024376487 |             f
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033168415 |             f
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010491476 |             f
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022651451 |             f
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031420779 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019945271 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031276456 |             f
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014154567 |             f
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022822208 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0034535511 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017830744 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025954251 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012745217 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019825524 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029187695 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017792946 |             f
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027285824 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012613101 |             f
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021025317 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030368777 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013100790 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023631582 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032133697 |             f
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011613555 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023170295 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0036453952 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018217201 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0034710769 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016591639 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033867141 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015431982 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028345488 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012450094 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024782038 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029351656 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013100860 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022280746 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027305119 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011811868 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019631993 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026261401 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015536843 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021669166 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028738742 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018868580 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024163081 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032159372 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019503050 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029208530 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010380393 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020349897 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0034852879 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018549965 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029985504 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007943142 |             f
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025864449 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016728062 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026176494 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015810296 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027234012 |             f
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014386024 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022021592 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033363915 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013561657 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024149096 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029913147 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017210416 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021700794 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033655252 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015355994 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024110574 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029708387 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018232538 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021787919 |             f
   101 |    10100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027711367 |             f
   102 |    10200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013276776 |             f
   103 |    10300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023581629 |             f
   104 |    10400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029317999 |             f
   105 |    10500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013695985 |             f
   106 |    10600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022131613 |             f
   107 |    10700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030340903 |             f
   108 |    10800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013582799 |             f
   109 |    10900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027426172 |             f
   110 |    11000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016461019 |             f
   111 |    11100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028334225 |             f
   112 |    11200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010757055 |             f
   113 |    11300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022792719 |             f
   114 |    11400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030587664 |             f
   115 |    11500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018517200 |             f
   116 |    11600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028269295 |             f
   117 |    11700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015249150 |             f
   118 |    11800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026199165 |             f
   119 |    11900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011350025 |             f
   120 |    12000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026849434 |             f
   121 |    12100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015559189 |             f
   122 |    12200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028787007 |             f
   123 |    12300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015404793 |             f
   124 |    12400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022887452 |             f
   125 |    12500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030552727 |             f
   126 |    12600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011260346 |             f
   127 |    12700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024988950 |             f
   128 |    12800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031291898 |             f
   129 |    12900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019267389 |             f
   130 |    13000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0036381085 |             f
   131 |    13100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012729080 |             f
   132 |    13200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021590547 |             f
   133 |    13300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029168637 |             f
   134 |    13400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018458315 |             f
   135 |    13500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028932298 |             f
   136 |    13600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016202989 |             f
   137 |    13700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026217047 |             f
   138 |    13800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018603077 |             f
   139 |    13900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025261865 |             f
   140 |    14000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021210282 |             f
   141 |    14100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028016503 |             f
   142 |    14200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014852279 |             f
   143 |    14300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025240839 |             f
   144 |    14400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017744369 |             f
   145 |    14500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028791104 |             f
   146 |    14600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011386885 |             f
   147 |    14700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022859780 |             f
   148 |    14800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029237233 |             f
   149 |    14900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015804591 |             f
   150 |    15000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028089836 |             f
   151 |    15100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015069467 |             f
   152 |    15200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026839779 |             f
   153 |    15300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015917569 |             f
   154 |    15400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022100662 |             f
   155 |    15500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026067160 |             f
   156 |    15600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011846064 |             f
   157 |    15700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022375113 |             f
   158 |    15800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030611884 |             f
   159 |    15900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015714565 |             f
   160 |    16000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028869397 |             f
   161 |    16100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016600187 |             f
   162 |    16200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027756486 |             f
   163 |    16300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009559711 |             f
   164 |    16400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017771082 |             f
   165 |    16500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027695506 |             f
   166 |    16600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014434461 |             f
   167 |    16700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022989265 |             f
   168 |    16800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028688076 |             f
   169 |    16900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015850603 |             f
   170 |    17000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027117981 |             f
   171 |    17100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011316322 |             f
   172 |    17200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022850334 |             f
   173 |    17300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026731713 |             f
   174 |    17400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015765675 |             f
   175 |    17500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022673390 |             f
   176 |    17600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031724165 |             f
   177 |    17700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016576392 |             f
   178 |    17800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027101116 |             f
   179 |    17900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011947550 |             f
   180 |    18000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028500324 |             f
   181 |    18100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015874368 |             f
   182 |    18200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027081587 |             f
   183 |    18300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019184854 |             f
   184 |    18400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030591815 |             f
   185 |    18500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018512709 |             f
   186 |    18600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029904611 |             f
   187 |    18700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013446287 |             f
   188 |    18800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021259771 |             f
   189 |    18900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029181996 |             f
   190 |    19000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017339777 |             f
   191 |    19100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026828237 |             f
   192 |    19200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015872028 |             f
   193 |    19300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023895509 |             f
   194 |    19400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033009870 |             f
   195 |    19500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009863318 |             f
   196 |    19600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019934390 |             f
   197 |    19700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032117982 |             f
   198 |    19800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019862665 |             f
   199 |    19900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033235850 |             f
   200 |    20000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019648091 |             f

================================================================================
PARETO FRONT RESULTS
================================================================================

Number of Pareto optimal solutions: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (First 15):
--------------------------------------------------------------------------------
 Solution           X1           X2           F1        F2  All_constraints_OK
        1 3.837353e-11 2.773282e-10 4.095408e-11 50.000000                True
        2 5.000000e+00 3.000000e+00 1.360000e+02  4.000000                True
        3 1.783206e+00 2.086525e+00 3.013676e+01 18.836882                True
        4 1.743059e+00 1.789321e+00 2.496172e+01 20.916628                True
        5 2.513953e-01 8.272367e-02 2.811039e-01 46.729086                True
        6 3.450829e-01 3.426404e-01 9.477030e-01 43.359693                True
        7 2.371700e+00 1.784297e+00 3.523806e+01 17.249543                True
        8 3.331934e+00 2.990644e+00 8.018604e+01  6.820723                True
        9 2.038825e+00 1.949843e+00 3.183653e+01 18.072454                True
       10 3.902808e+00 2.988529e+00 9.665423e+01  5.250193                True
       11 2.686916e+00 2.273886e+00 4.956282e+01 12.782685                True
       12 2.879320e+00 2.736689e+00 6.312286e+01  9.620620                True
       13 3.187418e+00 2.848048e+00 7.308616e+01  7.916874                True
       14 2.371700e+00 2.776703e+00 5.334351e+01 11.851846                True
       15 3.280547e+00 2.880366e+00 7.623643e+01  7.449969                True

--------------------------------------------------------------------------------
Summary Statistics:
--------------------------------------------------------------------------------
X1 range: [0.000, 5.000]
X2 range: [0.000, 3.000]
F1 range: [0.000, 136.000]
F2 range: [4.000, 50.000]
Feasible solutions: 100 / 100

✓ Pareto front saved to 'pareto_front_results_tab.csv'

================================================================================
ANALYZING ORIGINAL CSV DATA
================================================================================

Original CSV data points:
  Total points: 10201
  Feasible points: 9500
  Infeasible points: 701

================================================================================
GENERATING VISUALIZATIONS
================================================================================
✓ Visualization saved as 'pareto_front_two_plots.png'

================================================================================
OPTIMIZATION COMPLETE
================================================================================

Generated files:
  1. pareto_front_results.csv - Pareto optimal solutions
  2. pareto_front_two_plots.png - Objective and variable space plots

✓ F1 and F2 were obtained ONLY from CSV data (no analytical functions)
✓ Interpolation used to estimate objectives for new X1, X2 combinations
✓ The Pareto front shows optimal trade-offs based purely on your data
================================================================================
PASSED
pyomo/py/test_bnh_csv_decision_tree.py::test_bnh_csv_decision_tree 
================================================================================
PARETO FRONT FROM CSV DATA (NO ANALYTICAL FUNCTIONS)
================================================================================

IMPORTANT: F1 and F2 are read ONLY from the DataFrame/CSV
No analytical functions are used for objectives!

================================================================================
RUNNING NSGA-II OPTIMIZATION
================================================================================

Loaded 10201 data points from CSV:
    X1    X2      F1       F2
0  0.0  0.00  0.0000  50.0000
1  0.0  0.03  0.0036  49.7009
2  0.0  0.06  0.0144  49.4036
3  0.0  0.09  0.0324  49.1081
4  0.0  0.12  0.0576  48.8144
5  0.0  0.15  0.0900  48.5225
6  0.0  0.18  0.1296  48.2324
7  0.0  0.21  0.1764  47.9441
8  0.0  0.24  0.2304  47.6576
9  0.0  0.27  0.2916  47.3729

================================================================================
CREATING INTERPOLATORS FROM CSV DATA
================================================================================
✓ Decision Tree Regressors trained from CSV data
  - Using 10201 data points for training
  - F1 model R² score: 0.9999
  - F2 model R² score: 0.9998
  - Max depth: 15, min samples split: 5, min samples leaf 2

Optimization Settings:
  Objectives: F1 and F2 from CSV data (interpolated)
  Variables: X1 ∈ [0, 5], X2 ∈ [0, 3]
  Constraints:
    C1: (x1-5)² + x2² ≤ 25
    C2: (x1-8)² + (x2+3)² ≥ 7.7
  Population: 100
  Generations: 200

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |     30 |  0.000000E+00 |  0.1286804155 |             - |             -
     2 |      200 |     44 |  0.000000E+00 |  0.000000E+00 |  0.0094302091 |         ideal
     3 |      300 |     62 |  0.000000E+00 |  0.000000E+00 |  0.0115410372 |         ideal
     4 |      400 |     89 |  0.000000E+00 |  0.000000E+00 |  0.0061365191 |         ideal
     5 |      500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0300713395 |         nadir
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025561371 |             f
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0040839251 |         ideal
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012867109 |             f
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015692218 |             f
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019014868 |             f
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031481227 |             f
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007090949 |             f
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019060478 |             f
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027030157 |             f
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016708808 |             f
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025603877 |             f
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004354600 |             f
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013471692 |             f
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020942324 |             f
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028326305 |             f
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014291355 |             f
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026869144 |             f
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0005769092 |             f
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111668636 |         nadir
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009742629 |             f
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017255111 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021523833 |             f
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023599923 |             f
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024958437 |             f
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024026168 |             f
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023928542 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025795377 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009288330 |             f
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015219977 |             f
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020766467 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023804864 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025811476 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007267533 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011318848 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013186173 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015786909 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018739091 |             f
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018910346 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022826248 |             f
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024911365 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027546668 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009397918 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016704656 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020408042 |             f
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024335345 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025096607 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010510946 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014211499 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019081774 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021913816 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025196008 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006963894 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010077489 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013353124 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017420283 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016487246 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022158815 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021515816 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023678609 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026814146 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011998368 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014285426 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015608483 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018139799 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018702205 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021383526 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026874135 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009024738 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014231276 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015817500 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019742896 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023032428 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023927426 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024185284 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024749320 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028017452 |             f
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008968872 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014139955 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020471884 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021102208 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021090799 |             f
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020835534 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023442935 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024649430 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028183454 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0005477068 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013292190 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020583052 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023731545 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023984578 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022637622 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023641002 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027467531 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010811797 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016410104 |             f
   101 |    10100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020928041 |             f
   102 |    10200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023604151 |             f
   103 |    10300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022191684 |             f
   104 |    10400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023725381 |             f
   105 |    10500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022039221 |             f
   106 |    10600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024021392 |             f
   107 |    10700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026456217 |             f
   108 |    10800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007410709 |             f
   109 |    10900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009930977 |             f
   110 |    11000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013569448 |             f
   111 |    11100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014516287 |             f
   112 |    11200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018512369 |             f
   113 |    11300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018751660 |             f
   114 |    11400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023285958 |             f
   115 |    11500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025499144 |             f
   116 |    11600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009487206 |             f
   117 |    11700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015235255 |             f
   118 |    11800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018748144 |             f
   119 |    11900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023419670 |             f
   120 |    12000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028628649 |             f
   121 |    12100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006392903 |             f
   122 |    12200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009883826 |             f
   123 |    12300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013895981 |             f
   124 |    12400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018399688 |             f
   125 |    12500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021556451 |             f
   126 |    12600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024286520 |             f
   127 |    12700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026133341 |             f
   128 |    12800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010284491 |             f
   129 |    12900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017307901 |             f
   130 |    13000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019461611 |             f
   131 |    13100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023644548 |             f
   132 |    13200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026506380 |             f
   133 |    13300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011095317 |             f
   134 |    13400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019404517 |             f
   135 |    13500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021527853 |             f
   136 |    13600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024877835 |             f
   137 |    13700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028318665 |             f
   138 |    13800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010357167 |             f
   139 |    13900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016131633 |             f
   140 |    14000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019581487 |             f
   141 |    14100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026200964 |             f
   142 |    14200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007890022 |             f
   143 |    14300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014770793 |             f
   144 |    14400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016795981 |             f
   145 |    14500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020082712 |             f
   146 |    14600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022630893 |             f
   147 |    14700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022214199 |             f
   148 |    14800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022495865 |             f
   149 |    14900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024048373 |             f
   150 |    15000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025972941 |             f
   151 |    15100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006996171 |             f
   152 |    15200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012562225 |             f
   153 |    15300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017646442 |             f
   154 |    15400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022374361 |             f
   155 |    15500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021986566 |             f
   156 |    15600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021330201 |             f
   157 |    15700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021302569 |             f
   158 |    15800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022418936 |             f
   159 |    15900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026441403 |             f
   160 |    16000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006697163 |             f
   161 |    16100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012870144 |             f
   162 |    16200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017945717 |             f
   163 |    16300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021781799 |             f
   164 |    16400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025676585 |             f
   165 |    16500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006468599 |             f
   166 |    16600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009246857 |             f
   167 |    16700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014375262 |             f
   168 |    16800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020054469 |             f
   169 |    16900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022912921 |             f
   170 |    17000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020488525 |             f
   171 |    17100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026434282 |             f
   172 |    17200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007798044 |             f
   173 |    17300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016977903 |             f
   174 |    17400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017864554 |             f
   175 |    17500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023748086 |             f
   176 |    17600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022195944 |             f
   177 |    17700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021756936 |             f
   178 |    17800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022628786 |             f
   179 |    17900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024208893 |             f
   180 |    18000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028067153 |             f
   181 |    18100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007965127 |             f
   182 |    18200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010667288 |             f
   183 |    18300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017429960 |             f
   184 |    18400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023795566 |             f
   185 |    18500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024554663 |             f
   186 |    18600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021491463 |             f
   187 |    18700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023660342 |             f
   188 |    18800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023463169 |             f
   189 |    18900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025279838 |             f
   190 |    19000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004067531 |             f
   191 |    19100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011345890 |             f
   192 |    19200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017148499 |             f
   193 |    19300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017137386 |             f
   194 |    19400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020707721 |             f
   195 |    19500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028139363 |             f
   196 |    19600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011564032 |             f
   197 |    19700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021058298 |             f
   198 |    19800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022637902 |             f
   199 |    19900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025269764 |             f
   200 |    20000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008696117 |             f

================================================================================
PARETO FRONT RESULTS
================================================================================

Number of Pareto optimal solutions: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (First 15):
--------------------------------------------------------------------------------
 Solution       X1       X2         F1       F2  All_constraints_OK
        1 0.122334 0.089403   0.038267 48.11870                True
        2 4.816043 2.994885 127.092600  4.01500                True
        3 4.789098 2.998894 127.092600  4.01500                True
        4 0.118744 0.057356   0.038267 48.11870                True
        5 2.068819 1.999872  32.494000 17.73365                True
        6 4.489770 2.999828 115.932600  4.18375                True
        7 2.410214 2.167012  41.190000 14.61210                True
        8 4.710989 2.986896 123.292600  4.07625                True
        9 0.287783 0.368509   0.782000 43.34270                True
       10 2.746567 2.758161  59.846000 10.12710                True
       11 1.647322 1.734997  23.002800 21.55985                True
       12 0.613304 0.641731   2.838800 38.36970                True
       13 4.091660 2.991030 102.172600  4.82670                True
       14 1.310784 1.633931  17.198800 25.31870                True
       15 3.569852 2.969510  85.346800  6.23670                True

--------------------------------------------------------------------------------
Summary Statistics:
--------------------------------------------------------------------------------
X1 range: [0.098, 4.816]
X2 range: [0.057, 3.000]
F1 range: [0.038, 127.093]
F2 range: [4.015, 48.119]
Feasible solutions: 100 / 100

✓ Pareto front saved to 'pareto_front_results_dt.csv'

================================================================================
ANALYZING ORIGINAL CSV DATA
================================================================================

Original CSV data points:
  Total points: 10201
  Feasible points: 9500
  Infeasible points: 701

================================================================================
GENERATING VISUALIZATIONS
================================================================================
✓ Visualization saved as 'pareto_front_two_plots_dt.png'

================================================================================
OPTIMIZATION COMPLETE
================================================================================

Generated files:
  1. pareto_front_results_dt.csv - Pareto optimal solutions
  2. pareto_front_two_plots_dt.png - Objective and variable space plots

✓ F1 and F2 were obtained ONLY from CSV data (no analytical functions)
✓ Interpolation used to estimate objectives for new X1, X2 combinations
✓ The Pareto front shows optimal trade-offs based purely on your data
================================================================================
PASSED
pyomo/py/test_bnh_csv_decision_tree_generated_constraints.py::test_bnh_csv_decision_tree_generated_constraints 
================================================================================
PARETO FRONT FROM CSV DATA (NO ANALYTICAL FUNCTIONS)
================================================================================

IMPORTANT: F1 and F2 are read ONLY from the DataFrame/CSV
No analytical functions are used for objectives!
================================================================================
GENERIC CONSTRAINT FUNCTION GENERATOR FROM JSON
================================================================================

# ============================================================================
# GENERATED CONSTRAINT FUNCTIONS FROM JSON ALPHA ATTRIBUTE
# ============================================================================
# Source: /tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples/pyomo/py/bnh.json
# Variables: X1, X2
# Alpha: (X1-5)*(X1-5)+X2*X2-25 and -(X1-8)*(X1-8)-(X2+3)*(X2+3)+7.7
# Number of constraints: 2
# ============================================================================

def constraint_C1(X1, X2):
    """
    C1: (X1-5)*(X1-5)+X2*X2-25
    Returns: True if constraint is satisfied, False otherwise
    """
    return (X1-5)*(X1-5)+X2*X2-25

def constraint_C2(X1, X2):
    """
    C2: -(X1-8)*(X1-8)-(X2+3)*(X2+3)+7.7
    Returns: True if constraint is satisfied, False otherwise
    """
    return -(X1-8)*(X1-8)-(X2+3)*(X2+3)+7.7


================================================================================
✓ Constraints successfully generated and saved to '/tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples/pyomo/py/generated_constraints_claude.py'
================================================================================

================================================================================
RUNNING NSGA-II OPTIMIZATION
================================================================================

Loaded 10201 data points from CSV:
    X1    X2      F1       F2
0  0.0  0.00  0.0000  50.0000
1  0.0  0.03  0.0036  49.7009
2  0.0  0.06  0.0144  49.4036
3  0.0  0.09  0.0324  49.1081
4  0.0  0.12  0.0576  48.8144
5  0.0  0.15  0.0900  48.5225
6  0.0  0.18  0.1296  48.2324
7  0.0  0.21  0.1764  47.9441
8  0.0  0.24  0.2304  47.6576
9  0.0  0.27  0.2916  47.3729

================================================================================
CREATING MODEL FROM CSV DATA
================================================================================
✓ Decision Tree Regressors trained from CSV data
  - Using 10201 data points for training
  - F1 model R² score: 0.9999
  - F2 model R² score: 0.9998
  - Max depth: 15, min samples split: 5, min samples leaf 2

Optimization Settings:
  Objectives: F1 and F2 from CSV data (interpolated)
  Variables: X1 ∈ [0, 5], X2 ∈ [0, 3]
  Constraints:
    C1: (x1-5)² + x2² ≤ 25
    C2: (x1-8)² + (x2+3)² ≥ 7.7
  Population: 100
  Generations: 200

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |     30 |  0.000000E+00 |  0.1286804155 |             - |             -
     2 |      200 |     44 |  0.000000E+00 |  0.000000E+00 |  0.0094302091 |         ideal
     3 |      300 |     62 |  0.000000E+00 |  0.000000E+00 |  0.0115410372 |         ideal
     4 |      400 |     89 |  0.000000E+00 |  0.000000E+00 |  0.0061365191 |         ideal
     5 |      500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0300713395 |         nadir
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025561371 |             f
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0040839251 |         ideal
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012867109 |             f
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015692218 |             f
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019014868 |             f
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031481227 |             f
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007090949 |             f
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019060478 |             f
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027030157 |             f
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016708808 |             f
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025603877 |             f
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004354600 |             f
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013471692 |             f
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020942324 |             f
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028326305 |             f
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014291355 |             f
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026869144 |             f
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0005769092 |             f
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111668636 |         nadir
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009742629 |             f
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017255111 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021523833 |             f
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023599923 |             f
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024958437 |             f
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024026168 |             f
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023928542 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025795377 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009288330 |             f
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015219977 |             f
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020766467 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023804864 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025811476 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007267533 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011318848 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013186173 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015786909 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018739091 |             f
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018910346 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022826248 |             f
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024911365 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027546668 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009397918 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016704656 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020408042 |             f
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024335345 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025096607 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010510946 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014211499 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019081774 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021913816 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025196008 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006963894 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010077489 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013353124 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017420283 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016487246 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022158815 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021515816 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023678609 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026814146 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011998368 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014285426 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015608483 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018139799 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018702205 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021383526 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026874135 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009024738 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014231276 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015817500 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019742896 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023032428 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023927426 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024185284 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024749320 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028017452 |             f
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008968872 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014139955 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020471884 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021102208 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021090799 |             f
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020835534 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023442935 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024649430 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028183454 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0005477068 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013292190 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020583052 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023731545 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023984578 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022637622 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023641002 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027467531 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010811797 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016410104 |             f
   101 |    10100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020928041 |             f
   102 |    10200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023604151 |             f
   103 |    10300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022191684 |             f
   104 |    10400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023725381 |             f
   105 |    10500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022039221 |             f
   106 |    10600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024021392 |             f
   107 |    10700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026456217 |             f
   108 |    10800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007410709 |             f
   109 |    10900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009930977 |             f
   110 |    11000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013569448 |             f
   111 |    11100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014516287 |             f
   112 |    11200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018512369 |             f
   113 |    11300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018751660 |             f
   114 |    11400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023285958 |             f
   115 |    11500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025499144 |             f
   116 |    11600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009487206 |             f
   117 |    11700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015235255 |             f
   118 |    11800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018748144 |             f
   119 |    11900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023419670 |             f
   120 |    12000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028628649 |             f
   121 |    12100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006392903 |             f
   122 |    12200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009883826 |             f
   123 |    12300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013895981 |             f
   124 |    12400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018399688 |             f
   125 |    12500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021556451 |             f
   126 |    12600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024286520 |             f
   127 |    12700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026133341 |             f
   128 |    12800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010284491 |             f
   129 |    12900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017307901 |             f
   130 |    13000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019461611 |             f
   131 |    13100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023644548 |             f
   132 |    13200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026506380 |             f
   133 |    13300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011095317 |             f
   134 |    13400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019404517 |             f
   135 |    13500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021527853 |             f
   136 |    13600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024877835 |             f
   137 |    13700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028318665 |             f
   138 |    13800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010357167 |             f
   139 |    13900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016131633 |             f
   140 |    14000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019581487 |             f
   141 |    14100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026200964 |             f
   142 |    14200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007890022 |             f
   143 |    14300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014770793 |             f
   144 |    14400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016795981 |             f
   145 |    14500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020082712 |             f
   146 |    14600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022630893 |             f
   147 |    14700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022214199 |             f
   148 |    14800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022495865 |             f
   149 |    14900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024048373 |             f
   150 |    15000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025972941 |             f
   151 |    15100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006996171 |             f
   152 |    15200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012562225 |             f
   153 |    15300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017646442 |             f
   154 |    15400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022374361 |             f
   155 |    15500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021986566 |             f
   156 |    15600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021330201 |             f
   157 |    15700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021302569 |             f
   158 |    15800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022418936 |             f
   159 |    15900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026441403 |             f
   160 |    16000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006697163 |             f
   161 |    16100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012870144 |             f
   162 |    16200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017945717 |             f
   163 |    16300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021781799 |             f
   164 |    16400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025676585 |             f
   165 |    16500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006468599 |             f
   166 |    16600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009246857 |             f
   167 |    16700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014375262 |             f
   168 |    16800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020054469 |             f
   169 |    16900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022912921 |             f
   170 |    17000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020488525 |             f
   171 |    17100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026434282 |             f
   172 |    17200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007798044 |             f
   173 |    17300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016977903 |             f
   174 |    17400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017864554 |             f
   175 |    17500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023748086 |             f
   176 |    17600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022195944 |             f
   177 |    17700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021756936 |             f
   178 |    17800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022628786 |             f
   179 |    17900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024208893 |             f
   180 |    18000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028067153 |             f
   181 |    18100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007965127 |             f
   182 |    18200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010667288 |             f
   183 |    18300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017429960 |             f
   184 |    18400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023795566 |             f
   185 |    18500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024554663 |             f
   186 |    18600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021491463 |             f
   187 |    18700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023660342 |             f
   188 |    18800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023463169 |             f
   189 |    18900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025279838 |             f
   190 |    19000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004067531 |             f
   191 |    19100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011345890 |             f
   192 |    19200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017148499 |             f
   193 |    19300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017137386 |             f
   194 |    19400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020707721 |             f
   195 |    19500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028139363 |             f
   196 |    19600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011564032 |             f
   197 |    19700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021058298 |             f
   198 |    19800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022637902 |             f
   199 |    19900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025269764 |             f
   200 |    20000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008696117 |             f

================================================================================
PARETO FRONT RESULTS
================================================================================

Number of Pareto optimal solutions: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (First 15):
--------------------------------------------------------------------------------
 Solution       X1       X2         F1       F2  All_constraints_OK
        1 0.122334 0.089403   0.038267 48.11870                True
        2 4.816043 2.994885 127.092600  4.01500                True
        3 4.789098 2.998894 127.092600  4.01500                True
        4 0.118744 0.057356   0.038267 48.11870                True
        5 2.068819 1.999872  32.494000 17.73365                True
        6 4.489770 2.999828 115.932600  4.18375                True
        7 2.410214 2.167012  41.190000 14.61210                True
        8 4.710989 2.986896 123.292600  4.07625                True
        9 0.287783 0.368509   0.782000 43.34270                True
       10 2.746567 2.758161  59.846000 10.12710                True
       11 1.647322 1.734997  23.002800 21.55985                True
       12 0.613304 0.641731   2.838800 38.36970                True
       13 4.091660 2.991030 102.172600  4.82670                True
       14 1.310784 1.633931  17.198800 25.31870                True
       15 3.569852 2.969510  85.346800  6.23670                True

--------------------------------------------------------------------------------
Summary Statistics:
--------------------------------------------------------------------------------
X1 range: [0.098, 4.816]
X2 range: [0.057, 3.000]
F1 range: [0.038, 127.093]
F2 range: [4.015, 48.119]
Feasible solutions: 100 / 100

✓ Pareto front saved to 'pareto_front_results_dt_gc.csv'

================================================================================
ANALYZING ORIGINAL CSV DATA
================================================================================

Original CSV data points:
  Total points: 10201
  Feasible points: 9500
  Infeasible points: 701

================================================================================
GENERATING VISUALIZATIONS
================================================================================
✓ Visualization saved as 'pareto_front_two_plots_dt_gc.png'

================================================================================
OPTIMIZATION COMPLETE
================================================================================

Generated files:
  1. pareto_front_results_dt_gc.csv - Pareto optimal solutions
  2. pareto_front_two_plots_dt_gc.png - Objective and variable space plots

✓ F1 and F2 were obtained ONLY from CSV data (no analytical functions)
✓ Interpolation used to estimate objectives for new X1, X2 combinations
✓ The Pareto front shows optimal trade-offs based purely on your data
================================================================================
PASSED
pyomo/py/test_bnh_models_comparison.py::test_bnh_models_comparison 
PASSED
pyomo/py/test_glpk.py::test_glpk 
============================================================
OPTIMIZATION PROBLEM: Production Planning
============================================================

Solving optimization problem...

============================================================
OPTIMIZATION RESULTS
============================================================

Solver Status: ok
Termination Condition: optimal

Optimal Objective Value (Max Profit): $870.00

------------------------------------------------------------
Production Quantities:
------------------------------------------------------------
Product  Quantity  Profit  Total_Profit
      A  5.000000      30         150.0
      B 10.666667      45         480.0
      C  2.000000      50         100.0
      D  4.000000      35         140.0

------------------------------------------------------------
Resource Utilization:
------------------------------------------------------------
     Resource      Used  Capacity  Utilization_%
  labor_hours 60.000000       100      60.000000
machine_hours 37.333333        80      46.666667
storage_space 15.433333        50      30.866667

------------------------------------------------------------
Constraint Verification:
------------------------------------------------------------
Total Weight: 60.00 (limit: 60)
Product B / Product A ratio: 213.33% (min: 30%)

============================================================
To use with your own CSV files:
  products_data = pd.read_csv('your_products.csv')
  resources_data = pd.read_csv('your_resources.csv')
  requirements_data = pd.read_csv('your_requirements.csv')
============================================================
PASSED
pyomo/py/test_nsga2_df.py::test_nsga2 
================================================================================
NSGA-II MULTI-OBJECTIVE OPTIMIZATION with Pymoo
================================================================================

Setting up NSGA-II algorithm...
Running NSGA-II optimization...
  Population Size: 100
  Generations: 100
  Objectives: 3 (Profit, Emissions, Quality)
  Constraints: 9

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |      1 |  0.000000E+00 |  1.405063E+02 |             - |             -
     2 |      200 |      1 |  0.000000E+00 |  5.871091E+01 |  0.000000E+00 |             f
     3 |      300 |      3 |  0.000000E+00 |  2.968131E+01 |  1.0000000000 |         ideal
     4 |      400 |     15 |  0.000000E+00 |  1.438383E+01 |  0.8663739049 |         ideal
     5 |      500 |     46 |  0.000000E+00 |  3.1895751178 |  0.0213632413 |         ideal
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0633385179 |         ideal
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.1379275819 |         ideal
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0587146390 |         ideal
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.1034248913 |         ideal
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0145914143 |             f
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033445838 |         ideal
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0070014735 |         ideal
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0069128297 |         nadir
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0214295466 |         ideal
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0127526534 |             f
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030580761 |         ideal
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0105003288 |         ideal
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0459650521 |         ideal
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0211176508 |         nadir
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0117448700 |             f
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0504404175 |         ideal
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0124043725 |             f
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0183822658 |         nadir
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0094899610 |             f
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0112494538 |             f
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0053635544 |         ideal
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0145799542 |             f
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0042996363 |         nadir
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0112031810 |             f
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0126911752 |             f
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027014720 |         ideal
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0036987940 |         ideal
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0123874559 |             f
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0128530951 |         ideal
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0138647018 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0068890097 |         nadir
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0125178667 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0161412858 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0057555721 |         nadir
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0123778992 |         nadir
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0077221449 |         ideal
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0092310827 |         nadir
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0095112772 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0106517222 |             f
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0112468332 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0052128569 |         ideal
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0110094192 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0121251559 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0117391282 |             f
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0037401785 |         nadir
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025974447 |         nadir
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0092658233 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0115266512 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0124859858 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0093281969 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033777877 |         ideal
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0097838835 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0106373129 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0115405528 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0126582552 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0100998225 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032240910 |         nadir
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0150674945 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0113296737 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0112131387 |         nadir
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0100456833 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0154433447 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0101459451 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0109066390 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0118998106 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0118491703 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0125630606 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0084190405 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0098184866 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0123572484 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0139963973 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0105281336 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0129288504 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0098041944 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0121070874 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0100837925 |             f
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0103879392 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111285075 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0129714258 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0091512952 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0107039613 |             f
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0151564876 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0119392516 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0116976283 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0128062370 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0095612850 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0100893059 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0086996978 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0118496721 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0105023166 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0103166073 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0117238456 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0137732822 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0123817499 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0098649195 |             f

================================================================================
OPTIMIZATION RESULTS - PARETO FRONT
================================================================================

Number of Pareto optimal solutions found: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (Sample - First 10):
--------------------------------------------------------------------------------
 Solution     Profit  Carbon_Emissions  Avg_Quality
        1 868.773804        154.117269    87.417850
        2 811.036385        150.132101    88.644024
        3 525.072902         93.012950    86.071586
        4 868.820408        153.530263    87.018572
        5 845.008426        147.893750    83.713628
        6 827.129984        151.352179    88.306459
        7 805.441280        141.080016    83.875838
        8 575.426078        101.421042    85.963413
        9 716.477033        126.070662    85.674924
       10 724.436315        132.583030    87.892604

================================================================================
DETAILED ANALYSIS OF KEY SOLUTIONS
================================================================================

--------------------------------------------------------------------------------
Maximum Profit Solution:
--------------------------------------------------------------------------------
Profit: $868.82
Carbon Emissions: 153.53 units
Average Quality: 87.02

Production Quantities:
Product  Quantity  Profit  Emissions
      A      6.62  198.71      33.12
      B      9.28  417.49      74.22
      C      2.02  100.98      20.20
      D      4.33  151.64      26.00

Resource Utilization:
  labor_hours: 59.99 / 100 (60.0%)
  machine_hours: 36.73 / 80 (45.9%)
  storage_space: 15.35 / 50 (30.7%)

--------------------------------------------------------------------------------
Minimum Emissions Solution:
--------------------------------------------------------------------------------
Profit: $525.07
Carbon Emissions: 93.01 units
Average Quality: 86.07

Production Quantities:
Product  Quantity  Profit  Emissions
      A       5.0  150.05      25.01
      B       3.0  135.00      24.00
      C       2.0  100.02      20.00
      D       4.0  140.00      24.00

Resource Utilization:
  labor_hours: 37.01 / 100 (37.0%)
  machine_hours: 22.00 / 80 (27.5%)
  storage_space: 9.30 / 50 (18.6%)

--------------------------------------------------------------------------------
Maximum Quality Solution:
--------------------------------------------------------------------------------
Profit: $811.04
Carbon Emissions: 150.13 units
Average Quality: 88.64

Production Quantities:
Product  Quantity  Profit  Emissions
      A      5.00  150.11      25.02
      B      3.07  138.19      24.57
      C      7.65  382.71      76.54
      D      4.00  140.02      24.00

Resource Utilization:
  labor_hours: 59.84 / 100 (59.8%)
  machine_hours: 36.28 / 80 (45.4%)
  storage_space: 15.01 / 50 (30.0%)

================================================================================
TRADE-OFF ANALYSIS
================================================================================

Profit Range: $525.07 - $868.82
Emissions Range: 93.01 - 154.12 units
Quality Range: 83.71 - 88.64

Key Insights:
  • Maximizing profit results in 153.53 emissions
  • Minimizing emissions reduces profit to $525.07
  • High quality solution achieves 88.64 quality score

================================================================================
RECOMMENDATION
================================================================================
Use the Pareto front to select a solution based on your priorities:
  • High profit with acceptable environmental impact
  • Balance between all three objectives
  • Environmentally friendly with reasonable profit

Export pareto_df to CSV for further analysis or visualization.
================================================================================

================================================================================
GENERATING PARETO FRONT VISUALIZATIONS
================================================================================

✓ Pareto front visualizations saved as 'pareto_front_analysis.png'
✓ Optimal solutions comparison saved as 'optimal_solutions_comparison.png'

Visualization complete! Two PNG files have been generated:
  1. pareto_front_analysis.png - Multi-view Pareto front analysis
  2. optimal_solutions_comparison.png - Detailed comparison of key solutions
================================================================================
PASSED
pyomo/py/test_nsga2_mixed.py::test_nsga2_mixed 
==========================================================
n_gen  |  n_eval  | n_nds  |      eps      |   indicator  
==========================================================
     1 |      200 |     60 |             - |             -
     2 |      400 |    108 |  0.0042049841 |             f
     3 |      600 |    171 |  0.0029542472 |         nadir
     4 |      800 |    200 |  0.0116726050 |         nadir
     5 |     1000 |    200 |  0.0007278468 |             f
     6 |     1200 |    200 |  0.0013499157 |             f
     7 |     1400 |    200 |  0.0050418912 |         nadir
     8 |     1600 |    200 |  0.0004178698 |             f
     9 |     1800 |    200 |  0.0006504522 |             f
    10 |     2000 |    200 |  0.0009077589 |             f
    11 |     2200 |    200 |  0.0010911026 |             f
    12 |     2400 |    200 |  0.0012640075 |             f
    13 |     2600 |    200 |  0.0013163172 |             f
    14 |     2800 |    200 |  0.0014934606 |             f
    15 |     3000 |    200 |  0.0015460610 |             f
    16 |     3200 |    200 |  0.0015211573 |             f
    17 |     3400 |    200 |  0.0014847338 |             f
    18 |     3600 |    200 |  0.0015064046 |             f
    19 |     3800 |    200 |  0.0014897020 |             f
    20 |     4000 |    200 |  0.0014771119 |             f
    21 |     4200 |    200 |  0.0014996444 |             f
    22 |     4400 |    200 |  0.0015524164 |             f
    23 |     4600 |    200 |  0.0016425066 |             f
    24 |     4800 |    200 |  0.0016892185 |             f
    25 |     5000 |    200 |  0.0017363323 |             f
    26 |     5200 |    200 |  0.0017331472 |             f
    27 |     5400 |    200 |  0.0016994042 |             f
    28 |     5600 |    200 |  0.0017183797 |             f
    29 |     5800 |    200 |  0.0017203269 |             f
    30 |     6000 |    200 |  0.0017167934 |             f
    31 |     6200 |    200 |  0.0017346077 |             f
    32 |     6400 |    200 |  0.0017558965 |             f
    33 |     6600 |    200 |  0.0017068484 |             f
    34 |     6800 |    200 |  0.0017334254 |             f
    35 |     7000 |    200 |  0.0016895749 |             f
    36 |     7200 |    200 |  0.0016585545 |             f
    37 |     7400 |    200 |  0.0016777714 |             f
    38 |     7600 |    200 |  0.0016627387 |             f
    39 |     7800 |    200 |  0.0017123500 |             f
    40 |     8000 |    200 |  0.0016846829 |             f
    41 |     8200 |    200 |  0.0017241007 |             f
    42 |     8400 |    200 |  0.0016878487 |             f
    43 |     8600 |    200 |  0.0017175254 |             f
    44 |     8800 |    200 |  0.0016712230 |             f
    45 |     9000 |    200 |  0.0016678258 |             f
    46 |     9200 |    200 |  0.0016713156 |             f
    47 |     9400 |    200 |  0.0017214703 |             f
    48 |     9600 |    200 |  0.0017781077 |             f
    49 |     9800 |    200 |  0.0017362456 |             f
    50 |    10000 |    200 |  0.0017322336 |             f
    51 |    10200 |    200 |  0.0017086995 |             f
    52 |    10400 |    200 |  0.0017522075 |             f
    53 |    10600 |    200 |  0.0017467410 |             f
    54 |    10800 |    200 |  0.0017238424 |             f
    55 |    11000 |    200 |  0.0017216532 |             f
    56 |    11200 |    200 |  0.0016951815 |             f
    57 |    11400 |    200 |  0.0016960997 |             f
    58 |    11600 |    200 |  0.0017138830 |             f
    59 |    11800 |    200 |  0.0017554881 |             f
    60 |    12000 |    200 |  0.0018176273 |             f
    61 |    12200 |    200 |  0.0016910726 |             f
    62 |    12400 |    200 |  0.0017506589 |             f
    63 |    12600 |    200 |  0.0017918645 |             f
    64 |    12800 |    200 |  0.0017249525 |             f
    65 |    13000 |    200 |  0.0017871019 |             f
    66 |    13200 |    200 |  0.0018201541 |             f
    67 |    13400 |    200 |  0.0017743414 |             f
    68 |    13600 |    200 |  0.0017945823 |             f
    69 |    13800 |    200 |  0.0018049956 |             f
    70 |    14000 |    200 |  0.0016853740 |             f
    71 |    14200 |    200 |  0.0016856542 |             f
    72 |    14400 |    200 |  0.0017144380 |             f
    73 |    14600 |    200 |  0.0017545123 |             f
    74 |    14800 |    200 |  0.0017636751 |             f
    75 |    15000 |    200 |  0.0017846841 |             f
    76 |    15200 |    200 |  0.0017935991 |             f
    77 |    15400 |    200 |  0.0017230834 |             f
    78 |    15600 |    200 |  0.0016545634 |             f
    79 |    15800 |    200 |  0.0016734744 |             f
    80 |    16000 |    200 |  0.0017097434 |             f
    81 |    16200 |    200 |  0.0017017311 |             f
    82 |    16400 |    200 |  0.0017142866 |             f
    83 |    16600 |    200 |  0.0017815982 |             f
    84 |    16800 |    200 |  0.0017993270 |             f
    85 |    17000 |    200 |  0.0018145526 |             f
    86 |    17200 |    200 |  0.0017530283 |             f
    87 |    17400 |    200 |  0.0017072190 |             f
    88 |    17600 |    200 |  0.0016864610 |             f
    89 |    17800 |    200 |  0.0016702328 |             f
    90 |    18000 |    200 |  0.0016920506 |             f
    91 |    18200 |    200 |  0.0016787758 |             f
    92 |    18400 |    200 |  0.0016866061 |             f
    93 |    18600 |    200 |  0.0017256228 |             f
    94 |    18800 |    200 |  0.0017176099 |             f
    95 |    19000 |    200 |  0.0017620210 |             f
    96 |    19200 |    200 |  0.0017806446 |             f
    97 |    19400 |    200 |  0.0018591808 |             f
    98 |    19600 |    200 |  0.0017887304 |             f
    99 |    19800 |    200 |  0.0017391946 |             f
   100 |    20000 |    200 |  0.0017350571 |             f
   101 |    20200 |    200 |  0.0017250343 |             f
   102 |    20400 |    200 |  0.0017614743 |             f
   103 |    20600 |    200 |  0.0017258944 |             f
   104 |    20800 |    200 |  0.0017111050 |             f
   105 |    21000 |    200 |  0.0017216176 |             f
   106 |    21200 |    200 |  0.0017568564 |             f
   107 |    21400 |    200 |  0.0017871414 |             f
   108 |    21600 |    200 |  0.0018092364 |             f
   109 |    21800 |    200 |  0.0017943371 |             f
   110 |    22000 |    200 |  0.0018666155 |             f
   111 |    22200 |    200 |  0.0017382712 |             f
   112 |    22400 |    200 |  0.0017590904 |             f
   113 |    22600 |    200 |  0.0017232285 |             f
   114 |    22800 |    200 |  0.0017171587 |             f
   115 |    23000 |    200 |  0.0016996098 |             f
   116 |    23200 |    200 |  0.0016570203 |             f
   117 |    23400 |    200 |  0.0017128312 |             f
   118 |    23600 |    200 |  0.0017237269 |             f
   119 |    23800 |    200 |  0.0017105870 |             f
   120 |    24000 |    200 |  0.0017571804 |             f
   121 |    24200 |    200 |  0.0018031142 |             f
   122 |    24400 |    200 |  0.0017571939 |             f
   123 |    24600 |    200 |  0.0017626364 |             f
   124 |    24800 |    200 |  0.0017802442 |             f
   125 |    25000 |    200 |  0.0017943738 |             f
   126 |    25200 |    200 |  0.0017326787 |             f
   127 |    25400 |    200 |  0.0017599553 |             f
   128 |    25600 |    200 |  0.0018056518 |             f
   129 |    25800 |    200 |  0.0017409513 |             f
   130 |    26000 |    200 |  0.0017549246 |             f
   131 |    26200 |    200 |  0.0017438332 |             f
   132 |    26400 |    200 |  0.0017264253 |             f
   133 |    26600 |    200 |  0.0018152831 |             f
   134 |    26800 |    200 |  0.0017743369 |             f
   135 |    27000 |    200 |  0.0017952761 |             f
   136 |    27200 |    200 |  0.0017843448 |             f
   137 |    27400 |    200 |  0.0016733179 |             f
   138 |    27600 |    200 |  0.0016973047 |             f
   139 |    27800 |    200 |  0.0017581412 |             f
   140 |    28000 |    200 |  0.0017855258 |             f
   141 |    28200 |    200 |  0.0018537935 |             f
   142 |    28400 |    200 |  0.0017745749 |             f
   143 |    28600 |    200 |  0.0017896556 |             f
   144 |    28800 |    200 |  0.0018349193 |             f
   145 |    29000 |    200 |  0.0017665180 |             f
   146 |    29200 |    200 |  0.0017885263 |             f
   147 |    29400 |    200 |  0.0017802553 |             f
   148 |    29600 |    200 |  0.0018149172 |             f
   149 |    29800 |    200 |  0.0017880375 |             f
   150 |    30000 |    200 |  0.0018661841 |             f
   151 |    30200 |    200 |  0.0018689163 |             f
   152 |    30400 |    200 |  0.0019224762 |             f
   153 |    30600 |    200 |  0.0018932491 |             f
   154 |    30800 |    200 |  0.0019169741 |             f
   155 |    31000 |    200 |  0.0018405355 |             f
   156 |    31200 |    200 |  0.0018363832 |             f
   157 |    31400 |    200 |  0.0017086324 |             f
   158 |    31600 |    200 |  0.0016934837 |             f
   159 |    31800 |    200 |  0.0017004583 |             f
   160 |    32000 |    200 |  0.0017001298 |             f
   161 |    32200 |    200 |  0.0017508916 |             f
   162 |    32400 |    200 |  0.0018319926 |             f
   163 |    32600 |    200 |  0.0017600752 |             f
   164 |    32800 |    200 |  0.0017222700 |             f
   165 |    33000 |    200 |  0.0017240978 |             f
   166 |    33200 |    200 |  0.0017291304 |             f
   167 |    33400 |    200 |  0.0018350432 |             f
   168 |    33600 |    200 |  0.0017645432 |             f
   169 |    33800 |    200 |  0.0017706860 |             f
   170 |    34000 |    200 |  0.0017374954 |             f
   171 |    34200 |    200 |  0.0017311902 |             f
   172 |    34400 |    200 |  0.0017050296 |             f
   173 |    34600 |    200 |  0.0017856370 |             f
   174 |    34800 |    200 |  0.0018329709 |             f
   175 |    35000 |    200 |  0.0018007375 |             f
   176 |    35200 |    200 |  0.0017743768 |             f
   177 |    35400 |    200 |  0.0017687570 |             f
   178 |    35600 |    200 |  0.0017927940 |             f
   179 |    35800 |    200 |  0.0018277829 |             f
   180 |    36000 |    200 |  0.0017394154 |             f
   181 |    36200 |    200 |  0.0017379503 |             f
   182 |    36400 |    200 |  0.0017539806 |             f
   183 |    36600 |    200 |  0.0017084043 |             f
   184 |    36800 |    200 |  0.0016540176 |             f
   185 |    37000 |    200 |  0.0017012115 |             f
   186 |    37200 |    200 |  0.0016943486 |             f
   187 |    37400 |    200 |  0.0017636187 |             f
   188 |    37600 |    200 |  0.0017790755 |             f
   189 |    37800 |    200 |  0.0016775210 |             f
   190 |    38000 |    200 |  0.0016963381 |             f
   191 |    38200 |    200 |  0.0016730713 |             f
   192 |    38400 |    200 |  0.0017438642 |             f
   193 |    38600 |    200 |  0.0018284554 |             f
   194 |    38800 |    200 |  0.0017911735 |             f
   195 |    39000 |    200 |  0.0018558712 |             f
   196 |    39200 |    200 |  0.0018501381 |             f
   197 |    39400 |    200 |  0.0018471204 |             f
   198 |    39600 |    200 |  0.0018581293 |             f
   199 |    39800 |    200 |  0.0018323952 |             f
   200 |    40000 |    200 |  0.0018486819 |             f
   201 |    40200 |    200 |  0.0018364859 |             f
   202 |    40400 |    200 |  0.0017920696 |             f
   203 |    40600 |    200 |  0.0017768868 |             f
   204 |    40800 |    200 |  0.0017649678 |             f
   205 |    41000 |    200 |  0.0017333073 |             f
   206 |    41200 |    200 |  0.0016459446 |             f
   207 |    41400 |    200 |  0.0017261589 |             f
   208 |    41600 |    200 |  0.0017764654 |             f
   209 |    41800 |    200 |  0.0017324986 |             f
   210 |    42000 |    200 |  0.0018020968 |             f
   211 |    42200 |    200 |  0.0017754053 |             f
   212 |    42400 |    200 |  0.0017336874 |             f
   213 |    42600 |    200 |  0.0017050015 |             f
   214 |    42800 |    200 |  0.0017071454 |             f
   215 |    43000 |    200 |  0.0017184364 |             f
   216 |    43200 |    200 |  0.0017437272 |             f
   217 |    43400 |    200 |  0.0016937010 |             f
   218 |    43600 |    200 |  0.0017295731 |             f
   219 |    43800 |    200 |  0.0018025849 |             f
   220 |    44000 |    200 |  0.0017422153 |             f
   221 |    44200 |    200 |  0.0017372161 |             f
   222 |    44400 |    200 |  0.0017536952 |             f
   223 |    44600 |    200 |  0.0018117864 |             f
   224 |    44800 |    200 |  0.0018527080 |             f
   225 |    45000 |    200 |  0.0017541096 |             f
   226 |    45200 |    200 |  0.0017112569 |             f
   227 |    45400 |    200 |  0.0017037573 |             f
   228 |    45600 |    200 |  0.0017096721 |             f
   229 |    45800 |    200 |  0.0017518541 |             f
   230 |    46000 |    200 |  0.0017761105 |             f
   231 |    46200 |    200 |  0.0016307932 |             f
   232 |    46400 |    200 |  0.0017031764 |             f
   233 |    46600 |    200 |  0.0017486203 |             f
   234 |    46800 |    200 |  0.0017700738 |             f
   235 |    47000 |    200 |  0.0016286237 |             f
   236 |    47200 |    200 |  0.0016665871 |             f
   237 |    47400 |    200 |  0.0016625180 |             f
   238 |    47600 |    200 |  0.0016954198 |             f
   239 |    47800 |    200 |  0.0017101121 |             f
   240 |    48000 |    200 |  0.0016590709 |             f
   241 |    48200 |    200 |  0.0016655618 |             f
   242 |    48400 |    200 |  0.0016388347 |             f
   243 |    48600 |    200 |  0.0016890705 |             f
   244 |    48800 |    200 |  0.0016649029 |             f
   245 |    49000 |    200 |  0.0017272018 |             f
   246 |    49200 |    200 |  0.0017452109 |             f
   247 |    49400 |    200 |  0.0017753036 |             f
   248 |    49600 |    200 |  0.0017765626 |             f
   249 |    49800 |    200 |  0.0018247683 |             f
   250 |    50000 |    200 |  0.0017925410 |             f

======================================================================
OPTIMIZATION RESULTS
======================================================================

Total Pareto front solutions: 200

======================================================================
SOLUTIONS BY CATEGORY
======================================================================

Category A: 76 solutions
  X1= 0.000  →  F1=  2.000, F2= 40.000
  X1= 0.132  →  F1=  2.005, F2= 39.670
  X1= 0.218  →  F1=  2.014, F2= 39.455
  X1= 0.373  →  F1=  2.042, F2= 39.068
  X1= 0.431  →  F1=  2.056, F2= 38.921
  X1= 0.488  →  F1=  2.072, F2= 38.779
  X1= 0.539  →  F1=  2.087, F2= 38.652
  X1= 0.610  →  F1=  2.111, F2= 38.476
  ... and 68 more solutions

Category B: 57 solutions
  X1= 3.000  →  F1=  5.000, F2= 29.000
  X1= 3.014  →  F1=  5.000, F2= 28.971
  X1= 3.137  →  F1=  5.003, F2= 28.726
  X1= 3.183  →  F1=  5.005, F2= 28.633
  X1= 3.409  →  F1=  5.025, F2= 28.182
  X1= 3.427  →  F1=  5.027, F2= 28.146
  X1= 3.557  →  F1=  5.046, F2= 27.887
  X1= 3.769  →  F1=  5.089, F2= 27.463
  ... and 49 more solutions

Category C: 35 solutions
  X1= 7.002  →  F1=  7.000, F2= 19.497
  X1= 7.204  →  F1=  7.006, F2= 19.194
  X1= 7.353  →  F1=  7.019, F2= 18.970
  X1= 7.488  →  F1=  7.036, F2= 18.767
  X1= 7.566  →  F1=  7.048, F2= 18.651
  X1= 7.659  →  F1=  7.065, F2= 18.511
  X1= 7.873  →  F1=  7.114, F2= 18.190
  X1= 8.097  →  F1=  7.181, F2= 17.854
  ... and 27 more solutions

Category D: 32 solutions
  X1=10.000  →  F1= 10.000, F2=  3.200
  X1= 9.934  →  F1= 10.066, F2=  3.122
  X1= 9.870  →  F1= 10.130, F2=  3.049
  X1= 9.778  →  F1= 10.222, F2=  2.948
  X1= 9.702  →  F1= 10.298, F2=  2.869
  X1= 9.659  →  F1= 10.341, F2=  2.826
  X1= 9.607  →  F1= 10.393, F2=  2.774
  X1= 9.567  →  F1= 10.433, F2=  2.737
  ... and 24 more solutions
✓ Optimal solutions comparison saved as /tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples/pyomo/py/nsga2_mixed_models_comparison.png

======================================================================
STATISTICAL SUMMARY
======================================================================

Category A: 76/200 solutions (38.0%)
  X1  → min:  0.00, max:  3.16, mean:  1.87, std:  0.86
  F1  → min:   2.00, max:   5.00, mean:   3.28
  F2  → min:  32.10, max:  40.00, mean:  35.31

Category B: 57/200 solutions (28.5%)
  X1  → min:  3.00, max:  6.65, mean:  5.05, std:  1.06
  F1  → min:   5.00, max:   7.00, mean:   5.80
  F2  → min:  21.70, max:  29.00, mean:  24.90

Category C: 35/200 solutions (17.5%)
  X1  → min:  7.00, max: 10.00, mean:  8.88, std:  0.87
  F1  → min:   7.00, max:   8.35, mean:   7.64
  F2  → min:  15.00, max:  19.50, mean:  16.68

Category D: 32/200 solutions (16.0%)
  X1  → min:  8.00, max: 10.00, mean:  9.02, std:  0.60
  F1  → min:  10.00, max:  12.00, mean:  10.98
  F2  → min:   2.00, max:   3.20, mean:   2.42

======================================================================
PROBLEM DESIGN - HOW CATEGORIES COMPETE
======================================================================
Category A: Dominates LEFT side (low F1, high F2)
Category B: Competitive in MID-LEFT region
Category C: Competitive in MID-RIGHT region
Category D: Dominates RIGHT side (high F1, low F2)

Each category is non-dominated in its specialized region!
PASSED
pyomo/py/test_nsga3_mixed.py::test_nsga3_mixed 
Number of reference directions: 496
WARNING: pop_size=300 is less than the number of reference directions ref_dirs=496.
This might cause unwanted behavior of the algorithm. 
Please make sure pop_size is equal or larger than the number of reference directions. 
==========================================================
n_gen  |  n_eval  | n_nds  |      eps      |   indicator  
==========================================================
     1 |      300 |     39 |             - |             -
     2 |      600 |     44 |  0.0367877232 |         nadir
     3 |      900 |     44 |  0.0023070974 |             f
     4 |     1200 |     45 |  0.0035211125 |             f
     5 |     1500 |     45 |  0.0002588012 |             f
     6 |     1800 |     45 |  0.0045527970 |         nadir
     7 |     2100 |     45 |  0.0010665891 |             f
     8 |     2400 |     45 |  0.0004567215 |             f
     9 |     2700 |     45 |  0.0006302142 |             f
    10 |     3000 |     45 |  0.0008078675 |             f
    11 |     3300 |     45 |  0.0009062124 |             f
    12 |     3600 |     45 |  0.0010105192 |             f
    13 |     3900 |     45 |  0.0011465905 |             f
    14 |     4200 |     46 |  0.0014756638 |             f
    15 |     4500 |     46 |  0.0015875214 |             f
    16 |     4800 |     46 |  0.0015736692 |             f
    17 |     5100 |     46 |  0.0014561735 |             f
    18 |     5400 |     46 |  0.0014205093 |             f
    19 |     5700 |     46 |  0.0013584048 |             f
    20 |     6000 |     46 |  0.0016013582 |             f
    21 |     6300 |     46 |  0.0016759459 |             f
    22 |     6600 |     46 |  0.0016560576 |             f
    23 |     6900 |     46 |  0.0015481683 |             f
    24 |     7200 |     46 |  0.0016176573 |             f
    25 |     7500 |     46 |  0.0019166319 |             f
    26 |     7800 |     46 |  0.0018274352 |             f
    27 |     8100 |     46 |  0.0015496038 |             f
    28 |     8400 |     46 |  0.0015530363 |             f
    29 |     8700 |     46 |  0.0015629162 |             f
    30 |     9000 |     46 |  0.0015252842 |             f
    31 |     9300 |     46 |  0.0014853959 |             f
    32 |     9600 |     46 |  0.0014853959 |             f
    33 |     9900 |     46 |  0.0015135522 |             f
    34 |    10200 |     46 |  0.0016201857 |             f
    35 |    10500 |     46 |  0.0016700199 |             f
    36 |    10800 |     46 |  0.0016692338 |             f
    37 |    11100 |     46 |  0.0016343289 |             f
    38 |    11400 |     46 |  0.0016316568 |             f
    39 |    11700 |     46 |  0.0016500208 |             f
    40 |    12000 |     46 |  0.0015665913 |             f
    41 |    12300 |     46 |  0.0022594546 |             f
    42 |    12600 |     46 |  0.0023660138 |             f
    43 |    12900 |     46 |  0.0016301250 |             f
    44 |    13200 |     46 |  0.0017648354 |             f
    45 |    13500 |     46 |  0.0021761102 |             f
    46 |    13800 |     46 |  0.0017023525 |             f
    47 |    14100 |     46 |  0.0016610817 |             f
    48 |    14400 |     46 |  0.0015453881 |             f
    49 |    14700 |     46 |  0.0014737412 |             f
    50 |    15000 |     46 |  0.0015200052 |             f
    51 |    15300 |     46 |  0.0015156335 |             f
    52 |    15600 |     46 |  0.0015324574 |             f
    53 |    15900 |     46 |  0.0015246837 |             f
    54 |    16200 |     46 |  0.0016105324 |             f
    55 |    16500 |     46 |  0.0015776779 |             f
    56 |    16800 |     46 |  0.0015809102 |             f
    57 |    17100 |     46 |  0.0015809913 |             f
    58 |    17400 |     46 |  0.0015605588 |             f
    59 |    17700 |     46 |  0.0017063410 |             f
    60 |    18000 |     46 |  0.0017272036 |             f
    61 |    18300 |     46 |  0.0016520806 |             f
    62 |    18600 |     46 |  0.0016548055 |             f
    63 |    18900 |     46 |  0.0016773627 |             f
    64 |    19200 |     46 |  0.0016092955 |             f
    65 |    19500 |     46 |  0.0016168405 |             f
    66 |    19800 |     46 |  0.0015565709 |             f
    67 |    20100 |     46 |  0.0016979611 |             f
    68 |    20400 |     46 |  0.0016399444 |             f
    69 |    20700 |     46 |  0.0018661643 |             f
    70 |    21000 |     46 |  0.0016330749 |             f
    71 |    21300 |     46 |  0.0016494453 |             f
    72 |    21600 |     46 |  0.0015073257 |             f
    73 |    21900 |     46 |  0.0014400363 |             f
    74 |    22200 |     46 |  0.0014930726 |             f
    75 |    22500 |     46 |  0.0014494946 |             f
    76 |    22800 |     46 |  0.0014338615 |             f
    77 |    23100 |     46 |  0.0014808133 |             f
    78 |    23400 |     46 |  0.0014629162 |             f
    79 |    23700 |     46 |  0.0014947193 |             f
    80 |    24000 |     46 |  0.0014340734 |             f
    81 |    24300 |     46 |  0.0014340734 |             f
    82 |    24600 |     46 |  0.0025364323 |             f
    83 |    24900 |     46 |  0.0012447383 |             f
    84 |    25200 |     46 |  0.0012328370 |             f
    85 |    25500 |     46 |  0.0010175412 |             f
    86 |    25800 |     46 |  0.0010701498 |             f
    87 |    26100 |     46 |  0.0012902771 |             f
    88 |    26400 |     46 |  0.0012859992 |             f
    89 |    26700 |     46 |  0.0008316357 |             f
    90 |    27000 |     46 |  0.0012158959 |             f
    91 |    27300 |     46 |  0.0012753951 |             f
    92 |    27600 |     46 |  0.0012968897 |             f
    93 |    27900 |     46 |  0.0010608956 |             f
    94 |    28200 |     46 |  0.0012640060 |             f
    95 |    28500 |     46 |  0.0012852629 |             f
    96 |    28800 |     46 |  0.0013147510 |             f
    97 |    29100 |     46 |  0.0010473550 |             f
    98 |    29400 |     46 |  0.0005656195 |             f
    99 |    29700 |     46 |  0.0010579516 |             f
   100 |    30000 |     46 |  0.0011782627 |             f
   101 |    30300 |     46 |  0.0011687199 |             f
   102 |    30600 |     46 |  0.0011164488 |             f
   103 |    30900 |     46 |  0.0011210855 |             f
   104 |    31200 |     46 |  0.0007926707 |             f
   105 |    31500 |     46 |  0.0010683340 |             f
   106 |    31800 |     46 |  0.0012095387 |             f
   107 |    32100 |     46 |  0.0011368988 |             f
   108 |    32400 |     46 |  0.0011422944 |             f
   109 |    32700 |     46 |  0.0011434130 |             f
   110 |    33000 |     46 |  0.0011434130 |             f
   111 |    33300 |     46 |  0.0011128038 |             f
   112 |    33600 |     46 |  0.0011128038 |             f
   113 |    33900 |     46 |  0.0011226669 |             f
   114 |    34200 |     46 |  0.0010345609 |             f
   115 |    34500 |     46 |  0.0006264288 |             f
   116 |    34800 |     46 |  0.0006725700 |             f
   117 |    35100 |     46 |  0.0009520528 |             f
   118 |    35400 |     46 |  0.0006200654 |             f
   119 |    35700 |     46 |  0.0005550373 |             f
   120 |    36000 |     46 |  0.0008920785 |             f
   121 |    36300 |     46 |  0.0009064358 |             f
   122 |    36600 |     46 |  0.0008956362 |             f
   123 |    36900 |     46 |  0.0008973969 |             f
   124 |    37200 |     46 |  0.0008825462 |             f
   125 |    37500 |     46 |  0.0008902132 |             f
   126 |    37800 |     46 |  0.0009480907 |             f
   127 |    38100 |     46 |  0.0013382278 |             f
   128 |    38400 |     46 |  0.0012578132 |             f
   129 |    38700 |     46 |  0.0014125891 |             f
   130 |    39000 |     46 |  0.0013939285 |             f
   131 |    39300 |     46 |  0.0014015469 |             f
   132 |    39600 |     46 |  0.0013654022 |             f
   133 |    39900 |     46 |  0.0013781044 |             f
   134 |    40200 |     46 |  0.0013969025 |             f
   135 |    40500 |     46 |  0.0013646166 |             f
   136 |    40800 |     46 |  0.0012570711 |             f
   137 |    41100 |     46 |  0.0012901289 |             f
   138 |    41400 |     46 |  0.0012519292 |             f
   139 |    41700 |     46 |  0.0012834063 |             f
   140 |    42000 |     46 |  0.0006647590 |             f
   141 |    42300 |     46 |  0.0011822516 |             f
   142 |    42600 |     46 |  0.0005390969 |             f
   143 |    42900 |     46 |  0.0011319068 |             f
   144 |    43200 |     46 |  0.0011636616 |             f
   145 |    43500 |     46 |  0.0012064205 |             f
   146 |    43800 |     46 |  0.0011716834 |             f
   147 |    44100 |     46 |  0.0010626471 |             f
   148 |    44400 |     46 |  0.0014415253 |             f
   149 |    44700 |     46 |  0.0014420687 |             f
   150 |    45000 |     46 |  0.0013730785 |             f
   151 |    45300 |     46 |  0.0012711345 |             f
   152 |    45600 |     46 |  0.0012397543 |             f
   153 |    45900 |     46 |  0.0007766753 |             f
   154 |    46200 |     46 |  0.0009570425 |             f
   155 |    46500 |     46 |  0.0006843118 |             f
   156 |    46800 |     46 |  0.0006843118 |             f
   157 |    47100 |     46 |  0.0011511462 |             f
   158 |    47400 |     46 |  0.0011511462 |             f
   159 |    47700 |     46 |  0.0011324964 |             f
   160 |    48000 |     46 |  0.0010871108 |             f
   161 |    48300 |     46 |  0.0010489748 |             f
   162 |    48600 |     46 |  0.0010057935 |             f
   163 |    48900 |     46 |  0.0012065121 |             f
   164 |    49200 |     46 |  0.0012280625 |             f
   165 |    49500 |     46 |  0.0012026463 |             f
   166 |    49800 |     46 |  0.0012114676 |             f
   167 |    50100 |     46 |  0.0013449695 |             f
   168 |    50400 |     46 |  0.0010337396 |             f
   169 |    50700 |     46 |  0.0009926450 |             f
   170 |    51000 |     46 |  0.0009870506 |             f
   171 |    51300 |     46 |  0.0008035187 |             f
   172 |    51600 |     46 |  0.0010128048 |             f
   173 |    51900 |     46 |  0.0010143565 |             f
   174 |    52200 |     46 |  0.0010047561 |             f
   175 |    52500 |     46 |  0.0012381813 |             f
   176 |    52800 |     46 |  0.0012408373 |             f
   177 |    53100 |     46 |  0.0011785313 |             f
   178 |    53400 |     46 |  0.0007695675 |             f
   179 |    53700 |     46 |  0.0012478106 |             f
   180 |    54000 |     46 |  0.0011783255 |             f
   181 |    54300 |     46 |  0.0014339288 |             f
   182 |    54600 |     46 |  0.0010375418 |             f
   183 |    54900 |     46 |  0.0010397665 |             f
   184 |    55200 |     46 |  0.0010577322 |             f
   185 |    55500 |     46 |  0.0011056501 |             f
   186 |    55800 |     46 |  0.0011015840 |             f
   187 |    56100 |     46 |  0.0011099389 |             f
   188 |    56400 |     46 |  0.0011101310 |             f
   189 |    56700 |     46 |  0.0009737514 |             f
   190 |    57000 |     46 |  0.0010159831 |             f
   191 |    57300 |     46 |  0.0008680937 |             f
   192 |    57600 |     46 |  0.0008706271 |             f
   193 |    57900 |     46 |  0.0014384755 |             f
   194 |    58200 |     46 |  0.0014309141 |             f
   195 |    58500 |     46 |  0.0013884342 |             f
   196 |    58800 |     46 |  0.0010111501 |             f
   197 |    59100 |     46 |  0.0010111501 |             f
   198 |    59400 |     46 |  0.0009869646 |             f
   199 |    59700 |     46 |  0.0014952559 |             f
   200 |    60000 |     46 |  0.0014263084 |             f
   201 |    60300 |     46 |  0.0013621052 |             f
   202 |    60600 |     46 |  0.0013651931 |             f
   203 |    60900 |     46 |  0.0013310257 |             f
   204 |    61200 |     46 |  0.0012626045 |             f
   205 |    61500 |     46 |  0.0012693543 |             f
   206 |    61800 |     46 |  0.0011441628 |             f
   207 |    62100 |     46 |  0.0010207100 |             f
   208 |    62400 |     46 |  0.0009782996 |             f
   209 |    62700 |     46 |  0.0012633389 |             f
   210 |    63000 |     46 |  0.0013235404 |             f
   211 |    63300 |     46 |  0.0012633389 |             f
   212 |    63600 |     46 |  0.0013222494 |             f
   213 |    63900 |     46 |  0.0012650444 |             f
   214 |    64200 |     46 |  0.0013376445 |             f
   215 |    64500 |     46 |  0.0012856680 |             f
   216 |    64800 |     46 |  0.0013870067 |             f
   217 |    65100 |     46 |  0.0014378990 |             f
   218 |    65400 |     46 |  0.0013867401 |             f
   219 |    65700 |     46 |  0.0013141429 |             f
   220 |    66000 |     46 |  0.0008749459 |             f
   221 |    66300 |     46 |  0.0013190917 |             f
   222 |    66600 |     46 |  0.0009502098 |             f
   223 |    66900 |     46 |  0.0009846626 |             f
   224 |    67200 |     46 |  0.0011712286 |             f
   225 |    67500 |     46 |  0.0011542957 |             f
   226 |    67800 |     46 |  0.0011693778 |             f
   227 |    68100 |     46 |  0.0011404627 |             f
   228 |    68400 |     46 |  0.0011774367 |             f
   229 |    68700 |     46 |  0.0008625515 |             f
   230 |    69000 |     46 |  0.0007798842 |             f
   231 |    69300 |     46 |  0.0013864984 |             f
   232 |    69600 |     46 |  0.0013396409 |             f
   233 |    69900 |     46 |  0.0011004505 |             f
   234 |    70200 |     46 |  0.0013905947 |             f
   235 |    70500 |     46 |  0.0013521965 |             f
   236 |    70800 |     46 |  0.0013570311 |             f
   237 |    71100 |     46 |  0.0010781287 |             f
   238 |    71400 |     46 |  0.0013187361 |             f
   239 |    71700 |     46 |  0.0013421438 |             f
   240 |    72000 |     46 |  0.0012921592 |             f
   241 |    72300 |     46 |  0.0013091531 |             f
   242 |    72600 |     46 |  0.0013381189 |             f
   243 |    72900 |     46 |  0.0013253034 |             f
   244 |    73200 |     46 |  0.0013225646 |             f
   245 |    73500 |     46 |  0.0013225646 |             f
   246 |    73800 |     46 |  0.0013458922 |             f
   247 |    74100 |     46 |  0.0013134070 |             f
   248 |    74400 |     46 |  0.0014722060 |             f
   249 |    74700 |     46 |  0.0014722060 |             f
   250 |    75000 |     46 |  0.0014385116 |             f

======================================================================
OPTIMIZATION RESULTS
======================================================================

Total Pareto front solutions: 46

======================================================================
SOLUTIONS BY CATEGORY
======================================================================

Category A: 12 solutions
  X1= 0.000  →  F1=  2.000, F2= 32.500, F3= 40.000
  X1= 1.253  →  F1=  2.471, F2= 27.022, F3= 36.869
  X1= 2.313  →  F1=  3.605, F2= 23.610, F3= 34.218
  X1= 3.146  →  F1=  4.970, F2= 21.718, F3= 32.134
  X1= 3.750  →  F1=  6.219, F2= 20.781, F3= 30.625
  X1= 4.374  →  F1=  7.740, F2= 20.196, F3= 29.064
  X1= 4.759  →  F1=  8.793, F2= 20.029, F3= 28.103
  X1= 4.959  →  F1=  9.378, F2= 20.001, F3= 27.602
  ... and 4 more solutions

Category B: 17 solutions
  X1= 3.159  →  F1= 15.005, F2=  6.993, F3= 28.681
  X1= 2.552  →  F1= 15.040, F2=  5.606, F3= 29.895
  X1= 3.476  →  F1= 15.045, F2=  7.834, F3= 28.047
  X1= 3.689  →  F1= 15.095, F2=  8.444, F3= 27.622
  X1= 3.979  →  F1= 15.192, F2=  9.333, F3= 27.042
  X1= 4.178  →  F1= 15.278, F2=  9.984, F3= 26.643
  X1= 1.776  →  F1= 15.299, F2=  4.262, F3= 31.447
  X1= 4.450  →  F1= 15.420, F2= 10.920, F3= 26.101
  ... and 9 more solutions

Category C: 8 solutions
  X1= 7.634  →  F1= 18.548, F2= 18.801, F3= 22.399
  X1= 7.338  →  F1= 18.993, F2= 18.537, F3= 20.846
  X1= 6.787  →  F1= 19.820, F2= 18.186, F3= 18.120
  X1= 4.012  →  F1= 23.982, F2= 19.186, F3=  7.634
  X1= 3.230  →  F1= 25.155, F2= 20.301, F3=  5.652
  X1= 2.550  →  F1= 26.176, F2= 21.572, F3=  4.275
  X1= 1.425  →  F1= 27.862, F2= 24.278, F3=  2.711
  X1= 0.000  →  F1= 30.000, F2= 28.800, F3=  2.000

Category D: 9 solutions
  X1= 7.373  →  F1= 20.035, F2= 18.728, F3=  8.414
  X1= 6.511  →  F1= 20.060, F2= 20.281, F3=  6.891
  X1= 7.863  →  F1= 20.186, F2= 17.847, F3=  9.476
  X1= 8.098  →  F1= 20.301, F2= 17.424, F3= 10.038
  X1= 8.525  →  F1= 20.581, F2= 16.656, F3= 11.142
  X1= 5.243  →  F1= 20.772, F2= 22.563, F3=  5.463
  X1= 8.761  →  F1= 20.775, F2= 16.231, F3= 11.799
  X1= 9.150  →  F1= 21.156, F2= 15.530, F3= 12.958
  ... and 1 more solutions
✓ Optimal solutions comparison saved as /tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples/pyomo/py/nsga3_mixed_models_comparison.png
PASSED
pyomo/py/test_scip.py::test_scip 

--- Running with SCIP (Global Optimizer) ---
SCIP Solution: x=1.000038, y=0.999962
Objective: 0.000000

--- Running with IPOPT (Local Optimizer) from different starts ---
IPOPT (start 0.5, 0.5): x=1.000000, y=1.000000
Objective: 0.000000
IPOPT (start 0.1, 1.9): x=1.000000, y=1.000000
Objective: 0.000000


======================================================================
EXAMPLE 2: Multi-Modal Function (Multiple Local Optima)
Minimize: sin(5*x)*cos(5*y)/5 + (x-1)² + (y-1)²
======================================================================

--- Running with SCIP (Global Optimizer) ---
SCIP Solution: x=0.959588, y=1.181927
Objective: -0.150796

--- Running with IPOPT from different starting points ---
IPOPT (start center): x=0.959780, y=1.181882, obj=-0.150796
IPOPT (start bottom-left): x=0.959780, y=1.181882, obj=-0.150796
IPOPT (start top-right): x=0.959780, y=1.181882, obj=-0.150796
IPOPT (start near optimum): x=0.959780, y=1.181882, obj=-0.150796

Best IPOPT solution: obj=-0.150796
SCIP solution: obj=-0.150796
Difference: 0.000000


======================================================================
EXAMPLE 3: Rosenbrock Function
Minimize: (1-x)² + 100*(y-x²)²  subject to x²+y²<=4
======================================================================

--- Running with SCIP ---
SCIP Solution: x=1.000077, y=1.000136
Objective: 0.000000

--- Running with IPOPT from different starting points ---
IPOPT (start standard): x=1.000000, y=1.000000, obj=0.000000
IPOPT (start origin): x=1.000000, y=1.000000, obj=0.000000
IPOPT (start near boundary): x=1.000000, y=1.000000, obj=0.000000


Generating comprehensive comparison visualizations...

Visualization saved as 'scip_vs_ipopt_comparison.png'

======================================================================
COMPARISON COMPLETE!
======================================================================

Key Insight: SCIP guarantees global optimum, IPOPT is faster but
may find different solutions depending on starting point!
======================================================================
PASSED
pyomo/py/test_scip_nsga2.py::test_scip_nsga2 

======================================================================
MULTI-OBJECTIVE BIOPROCESS OPTIMIZATION
======================================================================
METHOD 1: Weighted Sum (Single Run)
======================================================================
Objectives: Maximize Yield AND Minimize Cost
======================================================================

Testing different weight combinations:
Weights (yield=0.8, cost=0.2):
  Glucose: 10.00, Temp: 29.90
  Yield: 9.999457525513202, Cost: 22.989583453949123
Weights (yield=0.5, cost=0.5):
  Glucose: 10.00, Temp: 29.58
  Yield: 9.99131922859124, Cost: 22.95833290122058
Weights (yield=0.2, cost=0.8):
  Glucose: 3.04, Temp: 28.38
  Yield: 3.0000000223801684, Cost: 8.917778601509964

======================================================================
METHOD 2: Epsilon-Constraint (Pareto Front Generation)
======================================================================

Generating Pareto front (varying cost constraint):

-------------------------------- live log call ---------------------------------
Loading a SolverResults object with a warning status into model.name="Epsilon_Constraint";
  - termination condition: infeasible
  - message from solver: Ipopt 3.14.19\x3a Converged to a locally infeasible point. Problem may be infeasible.
Loading a SolverResults object with a warning status into model.name="Epsilon_Constraint";
  - termination condition: infeasible
  - message from solver: Ipopt 3.14.19\x3a Converged to a locally infeasible point. Problem may be infeasible.
Loading a SolverResults object with a warning status into model.name="Epsilon_Constraint";
  - termination condition: infeasible
  - message from solver: Ipopt 3.14.19\x3a Converged to a locally infeasible point. Problem may be infeasible.
Loading a SolverResults object with a warning status into model.name="Epsilon_Constraint";
  - termination condition: infeasible
  - message from solver: Ipopt 3.14.19\x3a Converged to a locally infeasible point. Problem may be infeasible.
Loading a SolverResults object with a warning status into model.name="Epsilon_Constraint";
  - termination condition: infeasible
  - message from solver: Ipopt 3.14.19\x3a Converged to a locally infeasible point. Problem may be infeasible.
ε=9.5: Yield=3.2745847588235004, Cost=9.473684300168728
ε=10.4: Yield=3.717538789992287, Cost=10.368421151249251
ε=11.3: Yield=4.161404668664917, Cost=11.263158002314803
ε=12.2: Yield=4.605929208078503, Cost=12.157894853375758
ε=13.1: Yield=5.05094419008997, Cost=13.052631704434383
ε=13.9: Yield=5.496333755554645, Cost=13.947368555491579
ε=14.8: Yield=5.942015638591265, Cost=14.842105406547812
ε=15.7: Yield=6.3879298839916725, Cost=15.7368422576034
ε=16.6: Yield=6.834031802135927, Cost=16.63157910865851
ε=17.5: Yield=7.280287422759853, Cost=17.52631595971331
ε=18.4: Yield=7.726670475532958, Cost=18.421052810767947
ε=19.3: Yield=8.173160333139313, Cost=19.3157896618226
ε=20.2: Yield=8.619740578149942, Cost=20.21052651287754
ε=21.1: Yield=9.066397984260815, Cost=21.10526336393353
ε=22.0: Yield=9.513121778926397, Cost=22.00000021499397

Found 15 Pareto-optimal solutions

======================================================================
METHOD 3: pymoo with NSGA-II (Evolutionary Algorithm)
======================================================================

Running NSGA-II evolutionary algorithm...
Found 100 Pareto-optimal solutions
Best yield: 10.00
Lowest cost: 8.93

======================================================================
GENERATING COMPREHENSIVE VISUALIZATION
======================================================================

Visualization saved as 'multi_objective_comparison.png'

======================================================================
MULTI-OBJECTIVE OPTIMIZATION COMPLETE!
======================================================================

All three methods successfully demonstrated:
1. Weighted Sum - Fast single solutions
2. Epsilon-Constraint - Complete Pareto front
3. NSGA-II (pymoo) - Evolutionary approach
======================================================================
PASSED
s2_rx_anonym/py/test_s2_rx_anonym.py::test_s2_rx_anonym_csv 
Data shape: (6809448, 11)
Columns: ['CH', 'RANK', 'Byte', 'p0', 'p1', 'p2', 'p3', 'p4', 'p5', 'o0', 'o1']

First few rows:
   CH  RANK  Byte  p0  p1  p2  p3  p4  p5   o0    o1
0   0     0     0  40  56  60   9   0   0 -1.0   0.0
1   0     0     0  40  56  60   9   0   0 -1.0   5.0
2   0     0     0  40  56  60   9   0   0 -1.0  10.0
3   0     0     0  40  56  60   9   0   0 -1.0  15.0
4   0     0     0  40  56  60   9   0   0 -1.0  20.0

Number of unique fixed parameter combinations: 32

Training surrogate models...
max_depth=17; n_estimators=18; min_samples_split=20
Model o0 R² score: 0.2315
Model o1 R² score: 0.1007
PASSED
s2_tx_anonym/py/test_s2_tx_anonym.py::test_s2_tx_anonym_csv 
Data shape: (2240060, 9)
Columns: ['CH', 'RANK', 'Byte', 'p0', 'p1', 'p2', 'p3', 'o0', 'o1']

First few rows:
   CH  RANK  Byte  p0  p1  p2  p3    o0    o1
0   0     0     0  15   3  60  40 -8.25   0.0
1   0     0     0  15   3  60  40 -8.25   4.0
2   0     0     0  15   3  60  40 -8.25   8.0
3   0     0     0  15   3  60  40 -8.25  12.0
4   0     0     0  15   3  60  40 -8.25  16.0

Number of unique fixed parameter combinations: 32

Training surrogate models...
max_depth=17; n_estimators=18; min_samples_split=20
Model o0 R² score: 0.3885
Model o1 R² score: 0.0994
PASSED
shekel/py/test_pytorch.py::test_pytorch SKIPPED (Skipping PyTorch te...)
shekel/py/test_shgo_shekel.py::test_optimization_ex2 
Optimizing Shekel function using SHGO algorithm...
============================================================

Optimization Results:
Success: True
Message: Optimization terminated successfully.

Optimal solution (x*):
  x = [4.00074686 3.99950947 4.00074686 3.99950947]

Optimal function value (f(x*)):
  f(x*) = -10.5364431535

Number of function evaluations: 2254
Number of iterations: 5

============================================================
Note: The known global minimum is approximately:
  x* ≈ [4, 4, 4, 4]
  f(x*) ≈ -10.5363

============================================================
Function evaluations at test points:
  f([4.0, 4.0, 4.0, 4.0]) = -10.536284
  f([1.2, 2.0, 3.2, 4.0]) = -0.323428
  f([8.0, 7.2, 6.0, 4.8]) = -0.416895
Creating CSV with 456976 points...
Grid size: 26 points per dimension
Total points: 456976

CSV file '/tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples/shekel/py/shekel_meshgrid_26.csv' created successfully!
Total rows: 456977 (including header)

Minimum value found in grid:
  X = [4. 4. 4. 4.]
  Y = -10.5362837262
PASSED
shekel/py/test_shgo_shekel_pytorch.py::test_shgo_shekel_onnx SKIPPED

=============================== warnings summary ===============================
../../../../../usr/local/lib/python3.14/dist-packages/z3/z3core.py:5
  /usr/local/lib/python3.14/dist-packages/z3/z3core.py:5: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
    import pkg_resources

c3dtlz4/py/test_gpsampler.py::test_gpsampler
  /tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples/c3dtlz4/py/c3dtlz4_ex.py:38: ExperimentalWarning: GPSampler is experimental (supported from v3.6.0). The interface can change in the future.
    sampler=optuna.samplers.GPSampler(seed=42, constraints_func=c3dtlz4.constraints_func, deterministic_objective=True),

c3dtlz4/py/test_gpsampler.py::test_gpsampler
  /tmp/smlp_tutorial_mdmitry_61610/smlp/tutorial/examples/c3dtlz4/py/c3dtlz4_ex.py:38: ExperimentalWarning: Argument ``constraints_func`` is an experimental feature. The interface can change in the future.
    sampler=optuna.samplers.GPSampler(seed=42, constraints_func=c3dtlz4.constraints_func, deterministic_objective=True),

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
===== 20 passed, 2 skipped, 8 deselected, 3 warnings in 581.93s (0:09:41) ======
