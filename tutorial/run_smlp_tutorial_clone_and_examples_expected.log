Run directory: /tmp/smlp_tutorial_mdmitry_638344
Cloning into 'smlp'...
Switched to a new branch 'SMLP_TUTORIAL'
branch 'SMLP_TUTORIAL' set up to track 'origin/SMLP_TUTORIAL'.
env PYTHONDONTWRITEBYTECODE=1 /usr/bin/pytest -v --forked -m forked /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-7.4.4, pluggy-1.4.0 -- /usr/bin/python3
cachedir: .pytest_cache
rootdir: /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples
plugins: forked-1.6.0, typeguard-4.4.4, mock-3.12.0
collecting ... collected 32 items / 22 deselected / 10 selected

bnh/py/test_z3.py::test_z3 
-------------------------------- live log call ---------------------------------
============================================================
BNH Multi-Objective Optimization Problem with Z3
============================================================

1. Lexicographic Optimization (f1 primary, f2 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

2. Lexicographic Optimization (f2 primary, f1 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

3. Weighted Sum Method (various weights):
------------------------------------------------------------

Weights (w1=0.2, w2=0.8):
  x1 = 2.750000, x2 = 2.625000
  f1 = 57.812500, f2 = 10.703125

Weights (w1=0.4, w2=0.6):
  x1 = 1.000000, x2 = 1.000000
  f1 = 8.000000, f2 = 32.000000

Weights (w1=0.5, w2=0.5):
  x1 = 1.000000, x2 = 2.000000
  f1 = 20.000000, f2 = 25.000000

Weights (w1=0.6, w2=0.4):
  x1 = 1.000000, x2 = 1.000000
  f1 = 8.000000, f2 = 32.000000

Weights (w1=0.8, w2=0.2):
  x1 = 0.250000, x2 = 0.250000
  f1 = 0.500000, f2 = 45.125000

4. Constraint Verification:
------------------------------------------------------------
Solution 1:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 2:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 3:
  C1 = 11.953125 (should be ≤ 25): ✓
  C2 = 59.203125 (should be ≥ 7.7): ✓
Solution 4:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 5:
  C1 = 20.000000 (should be ≤ 25): ✓
  C2 = 74.000000 (should be ≥ 7.7): ✓
Solution 6:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 7:
  C1 = 22.625000 (should be ≤ 25): ✓
  C2 = 70.625000 (should be ≥ 7.7): ✓

5. Pareto Front Plot Generated
------------------------------------------------------------

============================================================
Found 7 Pareto optimal solutions
============================================================
(0.0, 0.0, 0.0, 50.0) (0.0, 0.0, 0.0, 50.0) (2.75, 2.625, 57.8125, 10.703125) (1.0, 1.0, 8.0, 32.0) (1.0, 2.0, 20.0, 25.0) (1.0, 1.0, 8.0, 32.0) (0.25, 0.25, 0.5, 45.125)
PASSED                                        [ 10%]
bnh/py/test_z3_bnh_pysmt.py::test_z3_bnh_pysmt 
-------------------------------- live log call ---------------------------------
============================================================
BNH Multi-Objective Optimization Problem with PySMT
============================================================

1. Optimization: Minimize f1
------------------------------------------------------------
x1 = 0.031250
x2 = 0.015625
f1 = 0.004883
f2 = 49.532471

2. Optimization: Minimize f2
------------------------------------------------------------
x1 = 5.000000
x2 = 2.999512
f1 = 135.988282
f2 = 4.001953

3. Weighted Sum Method (various weights):
------------------------------------------------------------

Weights (w1=0.2, w2=0.8):
  x1 = 2.500000, x2 = 2.500000
  f1 = 50.000000, f2 = 12.500000

Weights (w1=0.4, w2=0.6):
  x1 = 1.363636, x2 = 1.375000
  f1 = 15.000517, f2 = 26.363765

Weights (w1=0.5, w2=0.5):
  x1 = 1.000000, x2 = 1.000000
  f1 = 8.000000, f2 = 32.000000

Weights (w1=0.6, w2=0.4):
  x1 = 0.714286, x2 = 0.718750
  f1 = 4.107223, f2 = 36.696449

Weights (w1=0.8, w2=0.2):
  x1 = 0.294118, x2 = 0.312500
  f1 = 0.736646, f2 = 44.117985

4. Constraint Verification:
------------------------------------------------------------
Solution 1:
  C1 = 24.688721 (should be ≤ 25): ✓
  C2 = 72.594971 (should be ≥ 7.7): ✓
Solution 2:
  C1 = 8.997071 (should be ≤ 25): ✓
  C2 = 44.994141 (should be ≥ 7.7): ✓
Solution 3:
  C1 = 12.500000 (should be ≤ 25): ✓
  C2 = 60.500000 (should be ≥ 7.7): ✓
Solution 4:
  C1 = 15.113765 (should be ≤ 25): ✓
  C2 = 63.181947 (should be ≥ 7.7): ✓
Solution 5:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 6:
  C1 = 18.883949 (should be ≤ 25): ✓
  C2 = 66.910734 (should be ≥ 7.7): ✓
Solution 7:
  C1 = 22.242985 (should be ≤ 25): ✓
  C2 = 70.353279 (should be ≥ 7.7): ✓

5. Pareto Front Plot Generated
------------------------------------------------------------

============================================================
Found 7 Pareto optimal solutions
============================================================
(0.03125, 0.015625, 0.0048828125, 49.532470703125) (5.0, 2.99951171875, 135.98828220367432, 4.001953363418579) (2.5, 2.5, 50.0, 12.5) (1.3636363636363635, 1.375, 15.000516528925619, 26.36376549586777) (1.0, 1.0, 8.0, 32.0) (0.7142857142857143, 0.71875, 4.107222576530612, 36.69644850127551) (0.29411764705882354, 0.3125, 0.7366457612456747, 44.11798496972318)
PASSED                    [ 20%]
bnh/py/test_z3_gradient.py::test_z3_gradient 
-------------------------------- live log call ---------------------------------
============================================================
BNH Multi-Objective Optimization Problem with Z3
============================================================

1. Lexicographic Optimization (f1 primary, f2 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

2. Lexicographic Optimization (f2 primary, f1 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

3. Weighted Sum Method (various weights):
------------------------------------------------------------

Trying weights (w1=0.1, w2=0.9)...
  ✓ x1 = 2.5020, x2 = 1.0000
    f1 = 29.0391, f2 = 22.2402

Trying weights (w1=0.2, w2=0.8)...
  ✓ x1 = 1.0000, x2 = 2.8301
    f1 = 36.0374, f2 = 20.7086

Trying weights (w1=0.4, w2=0.6)...
  ✓ x1 = 1.0000, x2 = 1.0000
    f1 = 8.0000, f2 = 32.0000

Trying weights (w1=0.5, w2=0.5)...
  ✓ x1 = 1.0000, x2 = 2.0000
    f1 = 20.0000, f2 = 25.0000

Trying weights (w1=0.6, w2=0.4)...
  ✓ x1 = 1.0000, x2 = 1.0000
    f1 = 8.0000, f2 = 32.0000

Trying weights (w1=0.8, w2=0.2)...
  ✓ x1 = 0.2500, x2 = 0.2500
    f1 = 0.5000, f2 = 45.1250

Trying weights (w1=0.9, w2=0.1)...
  ✓ x1 = 0.0010, x2 = 0.0010
    f1 = 0.0000, f2 = 49.9805

4. Constraint Verification:
------------------------------------------------------------
Solution 1:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 2:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 3:
  C1 = 7.240238 (should be ≤ 25): ✓
  C2 = 46.228519 (should be ≥ 7.7): ✓
Solution 4:
  C1 = 24.009342 (should be ≤ 25): ✓
  C2 = 82.989811 (should be ≥ 7.7): ✓
Solution 5:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 6:
  C1 = 20.000000 (should be ≤ 25): ✓
  C2 = 74.000000 (should be ≥ 7.7): ✓
Solution 7:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 8:
  C1 = 22.625000 (should be ≤ 25): ✓
  C2 = 70.625000 (should be ≥ 7.7): ✓
Solution 9:
  C1 = 24.990236 (should be ≤ 25): ✓
  C2 = 72.990236 (should be ≥ 7.7): ✓

5. Stability Analysis:
============================================================

Solution 1: x = (0.000000, 0.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  0.0000,   0.0000], ||∇f1|| = 0.0001
  ∇f2 = [-10.0000, -10.0000], ||∇f2|| = 14.1421

Constraint Status:
  C1 active: True (value: 0.000000)
  C2 active: False (value: -65.300000)
    ∇C1 = [-10.0000,   0.0000]

Sensitivity Analysis:
  Avg sensitivity (f1): 0.0004
  Avg sensitivity (f2): 12.1475
  Max sensitivity (f1): 0.0006
  Max sensitivity (f2): 13.4516
  Feasible perturbations tested: 4/20

Stability Assessment:
  At boundary: True
  Stability score: 13.1451
  Classification: MODERATELY STABLE

Solution 2: x = (0.000000, 0.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  0.0000,   0.0000], ||∇f1|| = 0.0001
  ∇f2 = [-10.0000, -10.0000], ||∇f2|| = 14.1421

Constraint Status:
  C1 active: True (value: 0.000000)
  C2 active: False (value: -65.300000)
    ∇C1 = [-10.0000,   0.0000]

Sensitivity Analysis:
  Avg sensitivity (f1): 0.0004
  Avg sensitivity (f2): 13.6543
  Max sensitivity (f1): 0.0005
  Max sensitivity (f2): 13.7401
  Feasible perturbations tested: 3/20

Stability Assessment:
  At boundary: True
  Stability score: 13.8984
  Classification: MODERATELY STABLE

Solution 3: x = (2.501953, 1.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [ 20.0157,   8.0000], ||∇f1|| = 21.5552
  ∇f2 = [ -4.9961,  -8.0000], ||∇f2|| = 9.4319

Constraint Status:
  C1 active: False (value: -17.759762)
  C2 active: False (value: -38.528519)

Sensitivity Analysis:
  Avg sensitivity (f1): 11.9396
  Avg sensitivity (f2): 4.6351
  Max sensitivity (f1): 21.5051
  Max sensitivity (f2): 9.0217
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 23.7809
  Classification: WEAKLY STABLE

Solution 4: x = (1.000000, 2.830078)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,  22.6407], ||∇f1|| = 24.0125
  ∇f2 = [ -8.0000,  -4.3398], ||∇f2|| = 9.1013

Constraint Status:
  C1 active: False (value: -0.990658)
  C2 active: False (value: -75.289811)

Sensitivity Analysis:
  Avg sensitivity (f1): 15.7144
  Avg sensitivity (f2): 5.7450
  Max sensitivity (f1): 24.0125
  Max sensitivity (f2): 9.0297
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 27.2866
  Classification: WEAKLY STABLE

Solution 5: x = (1.000000, 1.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,   8.0000], ||∇f1|| = 11.3138
  ∇f2 = [ -8.0000,  -8.0000], ||∇f2|| = 11.3137

Constraint Status:
  C1 active: False (value: -8.000000)
  C2 active: False (value: -57.300000)

Sensitivity Analysis:
  Avg sensitivity (f1): 7.6237
  Avg sensitivity (f2): 7.6235
  Max sensitivity (f1): 11.2433
  Max sensitivity (f2): 11.2430
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 18.9373
  Classification: WEAKLY STABLE

Solution 6: x = (1.000000, 2.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,  16.0000], ||∇f1|| = 17.8886
  ∇f2 = [ -8.0000,  -6.0000], ||∇f2|| = 10.0000

Constraint Status:
  C1 active: False (value: -5.000000)
  C2 active: False (value: -66.300000)

Sensitivity Analysis:
  Avg sensitivity (f1): 11.3005
  Avg sensitivity (f2): 5.8973
  Max sensitivity (f1): 17.7493
  Max sensitivity (f2): 9.9987
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 22.5432
  Classification: WEAKLY STABLE

Solution 7: x = (1.000000, 1.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,   8.0000], ||∇f1|| = 11.3138
  ∇f2 = [ -8.0000,  -8.0000], ||∇f2|| = 11.3137

Constraint Status:
  C1 active: False (value: -8.000000)
  C2 active: False (value: -57.300000)

Sensitivity Analysis:
  Avg sensitivity (f1): 6.3497
  Avg sensitivity (f2): 6.3499
  Max sensitivity (f1): 11.2651
  Max sensitivity (f2): 11.2661
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 17.6635
  Classification: WEAKLY STABLE

Solution 8: x = (0.250000, 0.250000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  2.0000,   2.0000], ||∇f1|| = 2.8285
  ∇f2 = [ -9.5000,  -9.5000], ||∇f2|| = 13.4350

Constraint Status:
  C1 active: False (value: -2.375000)
  C2 active: False (value: -62.925000)

Sensitivity Analysis:
  Avg sensitivity (f1): 1.9765
  Avg sensitivity (f2): 9.3885
  Max sensitivity (f1): 2.8286
  Max sensitivity (f2): 13.4352
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 13.8142
  Classification: MODERATELY STABLE

Solution 9: x = (0.000977, 0.000977)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  0.0079,   0.0079], ||∇f1|| = 0.0111
  ∇f2 = [ -9.9980,  -9.9980], ||∇f2|| = 14.1394

Constraint Status:
  C1 active: False (value: -0.009764)
  C2 active: False (value: -65.290236)

Sensitivity Analysis:
  Avg sensitivity (f1): 0.0069
  Avg sensitivity (f2): 8.9257
  Max sensitivity (f1): 0.0117
  Max sensitivity (f2): 14.0785
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: True
  Stability score: 11.5415
  Classification: MODERATELY STABLE

============================================================
STABILITY SUMMARY TABLE
============================================================
Sol   x1         x2         ||∇f1||    ||∇f2||    Score      Class               
------------------------------------------------------------
1     0.0000     0.0000     0.0001     14.1421    13.1451    MODERATELY STABLE   
2     0.0000     0.0000     0.0001     14.1421    13.8984    MODERATELY STABLE   
3     2.5020     1.0000     21.5552    9.4319     23.7809    WEAKLY STABLE       
4     1.0000     2.8301     24.0125    9.1013     27.2866    WEAKLY STABLE       
5     1.0000     1.0000     11.3138    11.3137    18.9373    WEAKLY STABLE       
6     1.0000     2.0000     17.8886    10.0000    22.5432    WEAKLY STABLE       
7     1.0000     1.0000     11.3138    11.3137    17.6635    WEAKLY STABLE       
8     0.2500     0.2500     2.8285     13.4350    13.8142    MODERATELY STABLE   
9     0.0010     0.0010     0.0111     14.1394    11.5415    MODERATELY STABLE   
============================================================

6. Decision Space, Pareto Front, and Stability Plots Generated
------------------------------------------------------------

============================================================
Found 9 Pareto optimal solutions

Most stable solution: #9
  x = (0.000977, 0.000977)
  Stability class: MODERATELY STABLE
  Stability score: 11.5415
============================================================
PASSED                      [ 30%]
bnh/py/test_z3_minimax.py::test_z3_minimax 
-------------------------------- live log call ---------------------------------
======================================================================
BNH Multi-Objective Optimization with Minimax Stability Analysis
======================================================================

Minimax Philosophy:
  - Optimizes for WORST-CASE scenario
  - Ensures robustness under uncertainty
  - Provides strong guarantees against adversarial perturbations
======================================================================

1. Lexicographic Optimization (f1 primary):
----------------------------------------------------------------------
  Solution: x=(0.000000, 0.000000)
  Objectives: f1=0.0000, f2=50.0000

2. Lexicographic Optimization (f2 primary):
----------------------------------------------------------------------
  Solution: x=(0.000000, 0.000000)
  Objectives: f1=0.0000, f2=50.0000

3. Weighted Sum Method:
----------------------------------------------------------------------
  w=(0.1,0.9): x=(3.0000,1.0000), f1=40.0000, f2=20.0000
  w=(0.2,0.8): x=(1.0000,2.8301), f1=36.0374, f2=20.7086
  w=(0.4,0.6): x=(1.0000,1.0000), f1=8.0000, f2=32.0000
  w=(0.5,0.5): x=(1.0000,1.0000), f1=8.0000, f2=32.0000
  w=(0.6,0.4): x=(1.0000,1.0000), f1=8.0000, f2=32.0000
  w=(0.8,0.2): x=(0.0000,0.0000), f1=0.0000, f2=50.0000
  w=(0.9,0.1): x=(0.1250,0.1250), f1=0.1250, f2=47.5312

======================================================================
MINIMAX STABILITY ANALYSIS
======================================================================

Solution 1: x = (0.000000, 0.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.000000
    f2 = 50.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.000004 (399.98%)
    Δf2 = 0.000000 (0.00%)

  Robustness Assessment:
    Score: 3.999756 (lower is better)
    Class: NOT ROBUST

Solution 2: x = (0.000000, 0.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.000000
    f2 = 50.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.000004 (399.98%)
    Δf2 = 0.000000 (0.00%)

  Robustness Assessment:
    Score: 3.999756 (lower is better)
    Class: NOT ROBUST

Solution 3: x = (3.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 40.000000
    f2 = 20.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.025301 (0.06%)
    Δf2 = 0.008942 (0.04%)

  Robustness Assessment:
    Score: 0.001080 (lower is better)
    Class: HIGHLY ROBUST

Solution 4: x = (1.000000, 2.830078)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 36.037369
    f2 = 20.708561

  Worst-case increases (within ε=0.001):
    Δf1 = 0.024008 (0.07%)
    Δf2 = 0.009102 (0.04%)

  Robustness Assessment:
    Score: 0.001106 (lower is better)
    Class: HIGHLY ROBUST

Solution 5: x = (1.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 8.000000
    f2 = 32.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.011312 (0.14%)
    Δf2 = 0.011309 (0.04%)

  Robustness Assessment:
    Score: 0.001767 (lower is better)
    Class: HIGHLY ROBUST

Solution 6: x = (1.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 8.000000
    f2 = 32.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.011312 (0.14%)
    Δf2 = 0.011309 (0.04%)

  Robustness Assessment:
    Score: 0.001767 (lower is better)
    Class: HIGHLY ROBUST

Solution 7: x = (1.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 8.000000
    f2 = 32.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.011312 (0.14%)
    Δf2 = 0.011309 (0.04%)

  Robustness Assessment:
    Score: 0.001767 (lower is better)
    Class: HIGHLY ROBUST

Solution 8: x = (0.000000, 0.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.000000
    f2 = 50.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.000004 (399.98%)
    Δf2 = 0.000000 (0.00%)

  Robustness Assessment:
    Score: 3.999756 (lower is better)
    Class: NOT ROBUST

Solution 9: x = (0.125000, 0.125000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.125000
    f2 = 47.531250

  Worst-case increases (within ε=0.001):
    Δf1 = 0.001417 (1.13%)
    Δf2 = 0.013782 (0.03%)

  Robustness Assessment:
    Score: 0.011630 (lower is better)
    Class: HIGHLY ROBUST

======================================================================
MINIMAX ROBUSTNESS SUMMARY
======================================================================
Sol   x1         x2         Δf1(%)     Δf2(%)     Score      Robustness        
----------------------------------------------------------------------
1     0.0000     0.0000     399.98     0.00       3.9998     NOT ROBUST        
2     0.0000     0.0000     399.98     0.00       3.9998     NOT ROBUST        
3     3.0000     1.0000     0.06       0.04       0.0011     HIGHLY ROBUST     
4     1.0000     2.8301     0.07       0.04       0.0011     HIGHLY ROBUST     
5     1.0000     1.0000     0.14       0.04       0.0018     HIGHLY ROBUST     
6     1.0000     1.0000     0.14       0.04       0.0018     HIGHLY ROBUST     
7     1.0000     1.0000     0.14       0.04       0.0018     HIGHLY ROBUST     
8     0.0000     0.0000     399.98     0.00       3.9998     NOT ROBUST        
9     0.1250     0.1250     1.13       0.03       0.0116     HIGHLY ROBUST     
======================================================================

Most Robust Solution: #3
  x = (3.000000, 1.000000)
  Worst-case score: 0.001080
  Class: HIGHLY ROBUST
======================================================================

✓ Visualization complete
PASSED                        [ 40%]
constraint_dora/py/test_z3_nonlinear.py::test_constraint_dora 
-------------------------------- live log call ---------------------------------
============================================================
METHOD 1: Z3 Solver
============================================================
Z3 Solution:
  x1 = 0.894400
  x2 = 0.447266
  f(x1, x2) = 1.527867
  Constraint check: x1^2 + x2^2 = 0.999998

============================================================
METHOD 2: Scipy (Recommended for this problem)
============================================================
Scipy Solution:
  x1 = 0.894429
  x2 = 0.447211
  f(x1, x2) = 1.527864
  Constraint check: x1^2 + x2^2 = 1.000000
  Success: True

============================================================
ANALYTICAL SOLUTION
============================================================
Analytical Solution:
  x1 = 0.894427
  x2 = 0.447214
  f(x1, x2) = 1.527864
  Constraint check: x1^2 + x2^2 = 1.000000

============================================================
EXPLANATION
============================================================
The minimum occurs at the point on the unit circle
closest to (2, 1). This is found by moving from the
origin toward (2, 1) until hitting the circle boundary.
============================================================
{'z3': (0.8944, 0.4473), 'slsqp': (0.8944, 0.4472), 'linalg': (0.8944, 0.4472)}
PASSED     [ 50%]
constraint_dora/py/test_z3_pysmt.py::test_constraint_dora 
-------------------------------- live log call ---------------------------------
============================================================
CONSTRAINED OPTIMIZATION WITH PYSMT AND Z3
============================================================
Objective: minimize f(x) = (x1-2)² + (x2-1)²
Subject to: x1² + x2² ≤ 1
============================================================
============================================================
SOLVER AVAILABILITY CHECK
============================================================
Available solvers: ['z3', 'msat']
Z3 available: True
============================================================

Searching for optimal solution...
Initial bounds: [0.0000, 5.0000]
Iteration 10: Found solution at f(x)=1.528008

Completed in 16 iterations

============================================================
SOLUTION VERIFICATION
============================================================
x1 = 0.894397
x2 = 0.447266

Objective function f(x) = 1.527873
Constraint value (1 - x1² - x2²) = 0.000008
Constraint satisfied: True

Analytical solution (for comparison):
  x1 ≈ 0.894427, x2 ≈ 0.447214, f(x) ≈ 1.527864
  (Point on unit circle closest to (2,1))
============================================================
PASSED         [ 60%]
shekel/py/test_keras.py::test_keras 
-------------------------------- live log call ---------------------------------
Random seeds set for reproducibility (seed=42)
============================================================
Loading data from /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/shekel/py/shekel_meshgrid_26.csv.expected.gz...
Data shape: (456976, 5)
Columns: ['X1', 'X2', 'X3', 'X4', 'Y']

First few rows:
    X1   X2   X3   X4         Y
0  0.0  0.0  0.0  0.0 -0.321729
1  0.0  0.0  0.0  0.4 -0.367250
2  0.0  0.0  0.0  0.8 -0.397599
3  0.0  0.0  0.0  1.2 -0.400056
4  0.0  0.0  0.0  1.6 -0.374583

Features shape: (456976, 4)
Target shape: (456976,)
Target range: [-10.5363, -0.0809]

Train set size: 365580
Test set size: 91396

Creating neural network model...
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ dense (Dense)                   │ (None, 32)             │           160 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization             │ (None, 32)             │           128 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 64)             │         2,112 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_1           │ (None, 64)             │           256 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_2 (Dense)                 │ (None, 32)             │         2,080 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_2           │ (None, 32)             │           128 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_3 (Dense)                 │ (None, 16)             │           528 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_4 (Dense)                 │ (None, 1)              │            17 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 5,409 (21.13 KB)
 Trainable params: 5,153 (20.13 KB)
 Non-trainable params: 256 (1.00 KB)

None

Training model...
Initial learning rate: 0.005 (using Nadam optimizer with cosine annealing)
Epoch 1/150
715/715 - 6s - 8ms/step - loss: 0.1270 - mae: 0.1686 - mse: 0.1270 - val_loss: 0.1096 - val_mae: 0.2063 - val_mse: 0.1096 - learning_rate: 0.0050 - lr: 0.0050

Epoch 2/150
715/715 - 3s - 4ms/step - loss: 0.0715 - mae: 0.1208 - mse: 0.0715 - val_loss: 0.0769 - val_mae: 0.1451 - val_mse: 0.0769 - learning_rate: 0.0050 - lr: 0.0050

Epoch 3/150
715/715 - 3s - 4ms/step - loss: 0.0642 - mae: 0.1106 - mse: 0.0642 - val_loss: 0.0481 - val_mae: 0.1090 - val_mse: 0.0481 - learning_rate: 0.0050 - lr: 0.0050

Epoch 4/150
715/715 - 3s - 4ms/step - loss: 0.0594 - mae: 0.1053 - mse: 0.0594 - val_loss: 0.0411 - val_mae: 0.1079 - val_mse: 0.0411 - learning_rate: 0.0050 - lr: 0.0050

Epoch 5/150
715/715 - 3s - 4ms/step - loss: 0.0607 - mae: 0.1077 - mse: 0.0607 - val_loss: 0.0346 - val_mae: 0.0988 - val_mse: 0.0346 - learning_rate: 0.0050 - lr: 0.0050

Epoch 6/150
715/715 - 3s - 4ms/step - loss: 0.0584 - mae: 0.1065 - mse: 0.0584 - val_loss: 0.0306 - val_mae: 0.0829 - val_mse: 0.0306 - learning_rate: 0.0050 - lr: 0.0050

Epoch 7/150
715/715 - 3s - 4ms/step - loss: 0.0559 - mae: 0.1022 - mse: 0.0559 - val_loss: 0.0362 - val_mae: 0.0828 - val_mse: 0.0362 - learning_rate: 0.0050 - lr: 0.0050

Epoch 8/150
715/715 - 3s - 4ms/step - loss: 0.0536 - mae: 0.1001 - mse: 0.0536 - val_loss: 0.0539 - val_mae: 0.0972 - val_mse: 0.0539 - learning_rate: 0.0050 - lr: 0.0050

Epoch 9/150
715/715 - 3s - 4ms/step - loss: 0.0649 - mae: 0.1109 - mse: 0.0649 - val_loss: 0.0355 - val_mae: 0.0897 - val_mse: 0.0355 - learning_rate: 0.0050 - lr: 0.0050

Epoch 10/150
715/715 - 3s - 4ms/step - loss: 0.0512 - mae: 0.1004 - mse: 0.0512 - val_loss: 0.0444 - val_mae: 0.1036 - val_mse: 0.0444 - learning_rate: 0.0050 - lr: 0.0050

Epoch 11/150
715/715 - 3s - 4ms/step - loss: 0.0504 - mae: 0.0997 - mse: 0.0504 - val_loss: 0.0332 - val_mae: 0.0880 - val_mse: 0.0332 - learning_rate: 0.0049 - lr: 0.0049

Epoch 12/150
715/715 - 3s - 4ms/step - loss: 0.0531 - mae: 0.1004 - mse: 0.0531 - val_loss: 0.0323 - val_mae: 0.0878 - val_mse: 0.0323 - learning_rate: 0.0049 - lr: 0.0049

Epoch 13/150
715/715 - 3s - 4ms/step - loss: 0.0436 - mae: 0.0930 - mse: 0.0436 - val_loss: 0.0293 - val_mae: 0.0786 - val_mse: 0.0293 - learning_rate: 0.0049 - lr: 0.0049

Epoch 14/150
715/715 - 3s - 4ms/step - loss: 0.0431 - mae: 0.0915 - mse: 0.0431 - val_loss: 0.0312 - val_mae: 0.0850 - val_mse: 0.0312 - learning_rate: 0.0049 - lr: 0.0049

Epoch 15/150
715/715 - 3s - 4ms/step - loss: 0.0440 - mae: 0.0924 - mse: 0.0440 - val_loss: 0.0430 - val_mae: 0.1077 - val_mse: 0.0430 - learning_rate: 0.0049 - lr: 0.0049

Epoch 16/150
715/715 - 3s - 4ms/step - loss: 0.0450 - mae: 0.0922 - mse: 0.0450 - val_loss: 0.0267 - val_mae: 0.0895 - val_mse: 0.0267 - learning_rate: 0.0049 - lr: 0.0049

Epoch 17/150
715/715 - 3s - 4ms/step - loss: 0.0425 - mae: 0.0903 - mse: 0.0425 - val_loss: 0.0277 - val_mae: 0.0845 - val_mse: 0.0277 - learning_rate: 0.0049 - lr: 0.0049

Epoch 18/150
715/715 - 3s - 4ms/step - loss: 0.0427 - mae: 0.0917 - mse: 0.0427 - val_loss: 0.0423 - val_mae: 0.1226 - val_mse: 0.0423 - learning_rate: 0.0048 - lr: 0.0048

Epoch 19/150
715/715 - 3s - 4ms/step - loss: 0.0421 - mae: 0.0906 - mse: 0.0421 - val_loss: 0.0275 - val_mae: 0.0912 - val_mse: 0.0275 - learning_rate: 0.0048 - lr: 0.0048

Epoch 20/150
715/715 - 3s - 4ms/step - loss: 0.0416 - mae: 0.0894 - mse: 0.0416 - val_loss: 0.0359 - val_mae: 0.0939 - val_mse: 0.0359 - learning_rate: 0.0048 - lr: 0.0048

Epoch 21/150
715/715 - 3s - 4ms/step - loss: 0.0416 - mae: 0.0885 - mse: 0.0416 - val_loss: 0.0382 - val_mae: 0.0810 - val_mse: 0.0382 - learning_rate: 0.0048 - lr: 0.0048

Epoch 22/150
715/715 - 3s - 4ms/step - loss: 0.0410 - mae: 0.0874 - mse: 0.0410 - val_loss: 0.0215 - val_mae: 0.0731 - val_mse: 0.0215 - learning_rate: 0.0048 - lr: 0.0048

Epoch 23/150
715/715 - 5s - 7ms/step - loss: 0.0359 - mae: 0.0855 - mse: 0.0359 - val_loss: 0.0351 - val_mae: 0.0955 - val_mse: 0.0351 - learning_rate: 0.0047 - lr: 0.0047

Epoch 24/150
715/715 - 5s - 8ms/step - loss: 0.0470 - mae: 0.0931 - mse: 0.0470 - val_loss: 0.0412 - val_mae: 0.1292 - val_mse: 0.0412 - learning_rate: 0.0047 - lr: 0.0047

Epoch 25/150
715/715 - 5s - 7ms/step - loss: 0.0386 - mae: 0.0860 - mse: 0.0386 - val_loss: 0.0681 - val_mae: 0.1321 - val_mse: 0.0681 - learning_rate: 0.0047 - lr: 0.0047

Epoch 26/150
715/715 - 4s - 5ms/step - loss: 0.0377 - mae: 0.0863 - mse: 0.0377 - val_loss: 0.0303 - val_mae: 0.0908 - val_mse: 0.0303 - learning_rate: 0.0047 - lr: 0.0047

Epoch 27/150
715/715 - 4s - 6ms/step - loss: 0.0431 - mae: 0.0898 - mse: 0.0431 - val_loss: 0.0395 - val_mae: 0.1097 - val_mse: 0.0395 - learning_rate: 0.0046 - lr: 0.0046

Epoch 28/150
715/715 - 3s - 5ms/step - loss: 0.0381 - mae: 0.0865 - mse: 0.0381 - val_loss: 0.0438 - val_mae: 0.0908 - val_mse: 0.0438 - learning_rate: 0.0046 - lr: 0.0046

Epoch 29/150
715/715 - 3s - 4ms/step - loss: 0.0415 - mae: 0.0891 - mse: 0.0415 - val_loss: 0.0904 - val_mae: 0.1510 - val_mse: 0.0904 - learning_rate: 0.0046 - lr: 0.0046

Epoch 30/150
715/715 - 3s - 4ms/step - loss: 0.0376 - mae: 0.0838 - mse: 0.0376 - val_loss: 0.0298 - val_mae: 0.0856 - val_mse: 0.0298 - learning_rate: 0.0046 - lr: 0.0046

Epoch 31/150
715/715 - 3s - 5ms/step - loss: 0.0375 - mae: 0.0856 - mse: 0.0375 - val_loss: 0.0254 - val_mae: 0.0926 - val_mse: 0.0254 - learning_rate: 0.0045 - lr: 0.0045

Epoch 32/150
715/715 - 5s - 7ms/step - loss: 0.0373 - mae: 0.0865 - mse: 0.0373 - val_loss: 0.0353 - val_mae: 0.0998 - val_mse: 0.0353 - learning_rate: 0.0045 - lr: 0.0045

Epoch 33/150
715/715 - 3s - 4ms/step - loss: 0.0366 - mae: 0.0855 - mse: 0.0366 - val_loss: 0.0258 - val_mae: 0.0749 - val_mse: 0.0258 - learning_rate: 0.0045 - lr: 0.0045

Epoch 34/150
715/715 - 3s - 4ms/step - loss: 0.0352 - mae: 0.0846 - mse: 0.0352 - val_loss: 0.0238 - val_mae: 0.0671 - val_mse: 0.0238 - learning_rate: 0.0044 - lr: 0.0044

Epoch 35/150
715/715 - 3s - 4ms/step - loss: 0.0357 - mae: 0.0852 - mse: 0.0357 - val_loss: 0.0341 - val_mae: 0.0851 - val_mse: 0.0341 - learning_rate: 0.0044 - lr: 0.0044

Epoch 36/150
715/715 - 3s - 4ms/step - loss: 0.0331 - mae: 0.0829 - mse: 0.0331 - val_loss: 0.0312 - val_mae: 0.0793 - val_mse: 0.0312 - learning_rate: 0.0044 - lr: 0.0044

Epoch 37/150

Epoch 37: ReduceLROnPlateau reducing learning rate to 0.002161278622224927.
715/715 - 3s - 4ms/step - loss: 0.0291 - mae: 0.0814 - mse: 0.0291 - val_loss: 0.0234 - val_mae: 0.0826 - val_mse: 0.0234 - learning_rate: 0.0043 - lr: 0.0043

Epoch 38/150
715/715 - 3s - 4ms/step - loss: 0.0430 - mae: 0.0912 - mse: 0.0430 - val_loss: 0.0435 - val_mae: 0.1235 - val_mse: 0.0435 - learning_rate: 0.0043 - lr: 0.0043

Epoch 39/150
715/715 - 3s - 4ms/step - loss: 0.0330 - mae: 0.0826 - mse: 0.0330 - val_loss: 0.0385 - val_mae: 0.1258 - val_mse: 0.0385 - learning_rate: 0.0042 - lr: 0.0042

Epoch 40/150
715/715 - 3s - 4ms/step - loss: 0.0323 - mae: 0.0829 - mse: 0.0323 - val_loss: 0.0315 - val_mae: 0.0910 - val_mse: 0.0315 - learning_rate: 0.0042 - lr: 0.0042

Epoch 41/150
715/715 - 3s - 4ms/step - loss: 0.0350 - mae: 0.0855 - mse: 0.0350 - val_loss: 0.0355 - val_mae: 0.1018 - val_mse: 0.0355 - learning_rate: 0.0042 - lr: 0.0042

Epoch 42/150
715/715 - 4s - 5ms/step - loss: 0.0307 - mae: 0.0816 - mse: 0.0307 - val_loss: 0.0218 - val_mae: 0.0788 - val_mse: 0.0218 - learning_rate: 0.0041 - lr: 0.0041

Epoch 43/150
715/715 - 3s - 4ms/step - loss: 0.0266 - mae: 0.0800 - mse: 0.0266 - val_loss: 0.0266 - val_mae: 0.0946 - val_mse: 0.0266 - learning_rate: 0.0041 - lr: 0.0041

Epoch 44/150
715/715 - 3s - 4ms/step - loss: 0.0307 - mae: 0.0824 - mse: 0.0307 - val_loss: 0.0262 - val_mae: 0.0975 - val_mse: 0.0262 - learning_rate: 0.0041 - lr: 0.0041

Epoch 45/150
715/715 - 3s - 4ms/step - loss: 0.0302 - mae: 0.0823 - mse: 0.0302 - val_loss: 0.0219 - val_mae: 0.0870 - val_mse: 0.0219 - learning_rate: 0.0040 - lr: 0.0040

Epoch 46/150
715/715 - 3s - 4ms/step - loss: 0.0285 - mae: 0.0823 - mse: 0.0285 - val_loss: 0.0313 - val_mae: 0.0957 - val_mse: 0.0313 - learning_rate: 0.0040 - lr: 0.0040

Epoch 47/150
715/715 - 3s - 4ms/step - loss: 0.0258 - mae: 0.0795 - mse: 0.0258 - val_loss: 0.0313 - val_mae: 0.0993 - val_mse: 0.0313 - learning_rate: 0.0039 - lr: 0.0039

Epoch 48/150
715/715 - 3s - 4ms/step - loss: 0.0338 - mae: 0.0840 - mse: 0.0338 - val_loss: 0.0346 - val_mae: 0.0913 - val_mse: 0.0346 - learning_rate: 0.0039 - lr: 0.0039

Epoch 49/150
715/715 - 3s - 4ms/step - loss: 0.0254 - mae: 0.0778 - mse: 0.0254 - val_loss: 0.0244 - val_mae: 0.0852 - val_mse: 0.0244 - learning_rate: 0.0038 - lr: 0.0038

Epoch 50/150
715/715 - 3s - 4ms/step - loss: 0.0218 - mae: 0.0758 - mse: 0.0218 - val_loss: 0.0242 - val_mae: 0.0764 - val_mse: 0.0242 - learning_rate: 0.0038 - lr: 0.0038

Epoch 51/150
715/715 - 3s - 4ms/step - loss: 0.0226 - mae: 0.0763 - mse: 0.0226 - val_loss: 0.0292 - val_mae: 0.0801 - val_mse: 0.0292 - learning_rate: 0.0038 - lr: 0.0038

Epoch 52/150

Epoch 52: ReduceLROnPlateau reducing learning rate to 0.001852321671321988.
715/715 - 3s - 4ms/step - loss: 0.0248 - mae: 0.0773 - mse: 0.0248 - val_loss: 0.0360 - val_mae: 0.0993 - val_mse: 0.0360 - learning_rate: 0.0037 - lr: 0.0037

Epoch 53/150
715/715 - 3s - 4ms/step - loss: 0.0236 - mae: 0.0765 - mse: 0.0236 - val_loss: 0.0193 - val_mae: 0.0752 - val_mse: 0.0193 - learning_rate: 0.0037 - lr: 0.0037

Epoch 54/150
715/715 - 3s - 4ms/step - loss: 0.0214 - mae: 0.0749 - mse: 0.0214 - val_loss: 0.0294 - val_mae: 0.0865 - val_mse: 0.0294 - learning_rate: 0.0036 - lr: 0.0036

Epoch 55/150
715/715 - 3s - 4ms/step - loss: 0.0286 - mae: 0.0791 - mse: 0.0286 - val_loss: 0.0308 - val_mae: 0.0969 - val_mse: 0.0308 - learning_rate: 0.0036 - lr: 0.0036

Epoch 56/150
715/715 - 3s - 4ms/step - loss: 0.0246 - mae: 0.0790 - mse: 0.0246 - val_loss: 0.0379 - val_mae: 0.0864 - val_mse: 0.0379 - learning_rate: 0.0035 - lr: 0.0035

Epoch 57/150
715/715 - 3s - 5ms/step - loss: 0.0267 - mae: 0.0762 - mse: 0.0267 - val_loss: 0.0276 - val_mae: 0.0735 - val_mse: 0.0276 - learning_rate: 0.0035 - lr: 0.0035

Epoch 58/150
715/715 - 3s - 5ms/step - loss: 0.0190 - mae: 0.0728 - mse: 0.0190 - val_loss: 0.0223 - val_mae: 0.0727 - val_mse: 0.0223 - learning_rate: 0.0034 - lr: 0.0034

Epoch 59/150
715/715 - 3s - 4ms/step - loss: 0.0188 - mae: 0.0726 - mse: 0.0188 - val_loss: 0.0212 - val_mae: 0.0690 - val_mse: 0.0212 - learning_rate: 0.0034 - lr: 0.0034

Epoch 60/150
715/715 - 3s - 4ms/step - loss: 0.0205 - mae: 0.0734 - mse: 0.0205 - val_loss: 0.0258 - val_mae: 0.0774 - val_mse: 0.0258 - learning_rate: 0.0033 - lr: 0.0033

Epoch 61/150
715/715 - 3s - 5ms/step - loss: 0.0231 - mae: 0.0747 - mse: 0.0231 - val_loss: 0.0277 - val_mae: 0.0815 - val_mse: 0.0277 - learning_rate: 0.0033 - lr: 0.0033

Epoch 62/150
715/715 - 3s - 4ms/step - loss: 0.0250 - mae: 0.0763 - mse: 0.0250 - val_loss: 0.0254 - val_mae: 0.0634 - val_mse: 0.0254 - learning_rate: 0.0032 - lr: 0.0032

Epoch 63/150
715/715 - 3s - 4ms/step - loss: 0.0211 - mae: 0.0732 - mse: 0.0211 - val_loss: 0.0300 - val_mae: 0.0855 - val_mse: 0.0300 - learning_rate: 0.0032 - lr: 0.0032

Epoch 64/150
715/715 - 3s - 4ms/step - loss: 0.0196 - mae: 0.0720 - mse: 0.0196 - val_loss: 0.0234 - val_mae: 0.0697 - val_mse: 0.0234 - learning_rate: 0.0031 - lr: 0.0031

Epoch 65/150
715/715 - 3s - 4ms/step - loss: 0.0179 - mae: 0.0712 - mse: 0.0179 - val_loss: 0.0319 - val_mae: 0.0894 - val_mse: 0.0319 - learning_rate: 0.0031 - lr: 0.0031

Epoch 66/150
715/715 - 3s - 4ms/step - loss: 0.0196 - mae: 0.0720 - mse: 0.0196 - val_loss: 0.0230 - val_mae: 0.0693 - val_mse: 0.0230 - learning_rate: 0.0030 - lr: 0.0030

Epoch 67/150
715/715 - 3s - 4ms/step - loss: 0.0173 - mae: 0.0708 - mse: 0.0173 - val_loss: 0.0247 - val_mae: 0.0763 - val_mse: 0.0247 - learning_rate: 0.0030 - lr: 0.0030

Epoch 68/150

Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0014586691977456212.
715/715 - 3s - 4ms/step - loss: 0.0177 - mae: 0.0708 - mse: 0.0177 - val_loss: 0.0214 - val_mae: 0.0732 - val_mse: 0.0214 - learning_rate: 0.0029 - lr: 0.0029

Epoch 69/150
715/715 - 3s - 4ms/step - loss: 0.0186 - mae: 0.0713 - mse: 0.0186 - val_loss: 0.0202 - val_mae: 0.0751 - val_mse: 0.0202 - learning_rate: 0.0029 - lr: 0.0029

Epoch 70/150
715/715 - 3s - 4ms/step - loss: 0.0250 - mae: 0.0752 - mse: 0.0250 - val_loss: 0.0186 - val_mae: 0.0765 - val_mse: 0.0186 - learning_rate: 0.0028 - lr: 0.0028

Epoch 71/150
715/715 - 3s - 4ms/step - loss: 0.0234 - mae: 0.0723 - mse: 0.0234 - val_loss: 0.0197 - val_mae: 0.0641 - val_mse: 0.0197 - learning_rate: 0.0028 - lr: 0.0028

Epoch 72/150
715/715 - 3s - 4ms/step - loss: 0.0206 - mae: 0.0711 - mse: 0.0206 - val_loss: 0.0174 - val_mae: 0.0619 - val_mse: 0.0174 - learning_rate: 0.0027 - lr: 0.0027

Epoch 73/150
715/715 - 3s - 4ms/step - loss: 0.0197 - mae: 0.0715 - mse: 0.0197 - val_loss: 0.0249 - val_mae: 0.0687 - val_mse: 0.0249 - learning_rate: 0.0027 - lr: 0.0027

Epoch 74/150
715/715 - 3s - 4ms/step - loss: 0.0161 - mae: 0.0691 - mse: 0.0161 - val_loss: 0.0204 - val_mae: 0.0715 - val_mse: 0.0204 - learning_rate: 0.0026 - lr: 0.0026

Epoch 75/150
715/715 - 3s - 4ms/step - loss: 0.0165 - mae: 0.0692 - mse: 0.0165 - val_loss: 0.0176 - val_mae: 0.0626 - val_mse: 0.0176 - learning_rate: 0.0026 - lr: 0.0026

Epoch 76/150
715/715 - 3s - 4ms/step - loss: 0.0171 - mae: 0.0693 - mse: 0.0171 - val_loss: 0.0234 - val_mae: 0.0710 - val_mse: 0.0234 - learning_rate: 0.0025 - lr: 0.0025

Epoch 77/150
715/715 - 3s - 4ms/step - loss: 0.0169 - mae: 0.0691 - mse: 0.0169 - val_loss: 0.0188 - val_mae: 0.0682 - val_mse: 0.0188 - learning_rate: 0.0024 - lr: 0.0024

Epoch 78/150
715/715 - 3s - 4ms/step - loss: 0.0175 - mae: 0.0695 - mse: 0.0175 - val_loss: 0.0207 - val_mae: 0.0680 - val_mse: 0.0207 - learning_rate: 0.0024 - lr: 0.0024

Epoch 79/150
715/715 - 3s - 4ms/step - loss: 0.0157 - mae: 0.0680 - mse: 0.0157 - val_loss: 0.0176 - val_mae: 0.0592 - val_mse: 0.0176 - learning_rate: 0.0023 - lr: 0.0023

Epoch 80/150
715/715 - 3s - 4ms/step - loss: 0.0160 - mae: 0.0680 - mse: 0.0160 - val_loss: 0.0190 - val_mae: 0.0587 - val_mse: 0.0190 - learning_rate: 0.0023 - lr: 0.0023

Epoch 81/150
715/715 - 3s - 4ms/step - loss: 0.0175 - mae: 0.0687 - mse: 0.0175 - val_loss: 0.0163 - val_mae: 0.0584 - val_mse: 0.0163 - learning_rate: 0.0022 - lr: 0.0022

Epoch 82/150
715/715 - 3s - 4ms/step - loss: 0.0147 - mae: 0.0670 - mse: 0.0147 - val_loss: 0.0145 - val_mae: 0.0607 - val_mse: 0.0145 - learning_rate: 0.0022 - lr: 0.0022

Epoch 83/150
715/715 - 3s - 4ms/step - loss: 0.0151 - mae: 0.0671 - mse: 0.0151 - val_loss: 0.0145 - val_mae: 0.0590 - val_mse: 0.0145 - learning_rate: 0.0021 - lr: 0.0021

Epoch 84/150
715/715 - 3s - 4ms/step - loss: 0.0162 - mae: 0.0675 - mse: 0.0162 - val_loss: 0.0161 - val_mae: 0.0557 - val_mse: 0.0161 - learning_rate: 0.0021 - lr: 0.0021

Epoch 85/150
715/715 - 3s - 4ms/step - loss: 0.0153 - mae: 0.0669 - mse: 0.0153 - val_loss: 0.0162 - val_mae: 0.0623 - val_mse: 0.0162 - learning_rate: 0.0020 - lr: 0.0020

Epoch 86/150
715/715 - 3s - 4ms/step - loss: 0.0153 - mae: 0.0669 - mse: 0.0153 - val_loss: 0.0159 - val_mae: 0.0603 - val_mse: 0.0159 - learning_rate: 0.0020 - lr: 0.0020

Epoch 87/150
715/715 - 3s - 4ms/step - loss: 0.0139 - mae: 0.0658 - mse: 0.0139 - val_loss: 0.0139 - val_mae: 0.0592 - val_mse: 0.0139 - learning_rate: 0.0019 - lr: 0.0019

Epoch 88/150
715/715 - 3s - 4ms/step - loss: 0.0138 - mae: 0.0656 - mse: 0.0138 - val_loss: 0.0151 - val_mae: 0.0565 - val_mse: 0.0151 - learning_rate: 0.0019 - lr: 0.0019

Epoch 89/150
715/715 - 3s - 4ms/step - loss: 0.0146 - mae: 0.0661 - mse: 0.0146 - val_loss: 0.0156 - val_mae: 0.0533 - val_mse: 0.0156 - learning_rate: 0.0018 - lr: 0.0018

Epoch 90/150
715/715 - 3s - 4ms/step - loss: 0.0144 - mae: 0.0658 - mse: 0.0144 - val_loss: 0.0146 - val_mae: 0.0578 - val_mse: 0.0146 - learning_rate: 0.0018 - lr: 0.0018

Epoch 91/150
715/715 - 3s - 4ms/step - loss: 0.0143 - mae: 0.0656 - mse: 0.0143 - val_loss: 0.0144 - val_mae: 0.0532 - val_mse: 0.0144 - learning_rate: 0.0017 - lr: 0.0017

Epoch 92/150
715/715 - 3s - 4ms/step - loss: 0.0135 - mae: 0.0649 - mse: 0.0135 - val_loss: 0.0141 - val_mae: 0.0575 - val_mse: 0.0141 - learning_rate: 0.0017 - lr: 0.0017

Epoch 93/150
715/715 - 3s - 4ms/step - loss: 0.0131 - mae: 0.0646 - mse: 0.0131 - val_loss: 0.0134 - val_mae: 0.0542 - val_mse: 0.0134 - learning_rate: 0.0016 - lr: 0.0016

Epoch 94/150
715/715 - 3s - 4ms/step - loss: 0.0130 - mae: 0.0643 - mse: 0.0130 - val_loss: 0.0145 - val_mae: 0.0549 - val_mse: 0.0145 - learning_rate: 0.0016 - lr: 0.0016

Epoch 95/150
715/715 - 3s - 4ms/step - loss: 0.0133 - mae: 0.0644 - mse: 0.0133 - val_loss: 0.0141 - val_mae: 0.0539 - val_mse: 0.0141 - learning_rate: 0.0015 - lr: 0.0015

Epoch 96/150
715/715 - 3s - 5ms/step - loss: 0.0135 - mae: 0.0644 - mse: 0.0135 - val_loss: 0.0122 - val_mae: 0.0509 - val_mse: 0.0122 - learning_rate: 0.0015 - lr: 0.0015

Epoch 97/150
715/715 - 3s - 4ms/step - loss: 0.0131 - mae: 0.0640 - mse: 0.0131 - val_loss: 0.0116 - val_mae: 0.0530 - val_mse: 0.0116 - learning_rate: 0.0014 - lr: 0.0014

Epoch 98/150
715/715 - 3s - 4ms/step - loss: 0.0127 - mae: 0.0636 - mse: 0.0127 - val_loss: 0.0117 - val_mae: 0.0516 - val_mse: 0.0117 - learning_rate: 0.0014 - lr: 0.0014

Epoch 99/150
715/715 - 3s - 4ms/step - loss: 0.0125 - mae: 0.0634 - mse: 0.0125 - val_loss: 0.0116 - val_mae: 0.0510 - val_mse: 0.0116 - learning_rate: 0.0013 - lr: 0.0013

Epoch 100/150
715/715 - 3s - 5ms/step - loss: 0.0125 - mae: 0.0633 - mse: 0.0125 - val_loss: 0.0109 - val_mae: 0.0497 - val_mse: 0.0109 - learning_rate: 0.0013 - lr: 0.0013

Epoch 101/150
715/715 - 3s - 4ms/step - loss: 0.0121 - mae: 0.0630 - mse: 0.0121 - val_loss: 0.0115 - val_mae: 0.0494 - val_mse: 0.0115 - learning_rate: 0.0013 - lr: 0.0013

Epoch 102/150
715/715 - 3s - 4ms/step - loss: 0.0123 - mae: 0.0629 - mse: 0.0123 - val_loss: 0.0111 - val_mae: 0.0496 - val_mse: 0.0111 - learning_rate: 0.0012 - lr: 0.0012

Epoch 103/150
715/715 - 3s - 4ms/step - loss: 0.0122 - mae: 0.0628 - mse: 0.0122 - val_loss: 0.0108 - val_mae: 0.0490 - val_mse: 0.0108 - learning_rate: 0.0012 - lr: 0.0012

Epoch 104/150
715/715 - 3s - 4ms/step - loss: 0.0120 - mae: 0.0625 - mse: 0.0120 - val_loss: 0.0110 - val_mae: 0.0483 - val_mse: 0.0110 - learning_rate: 0.0011 - lr: 0.0011

Epoch 105/150
715/715 - 3s - 4ms/step - loss: 0.0118 - mae: 0.0622 - mse: 0.0118 - val_loss: 0.0110 - val_mae: 0.0484 - val_mse: 0.0110 - learning_rate: 0.0011 - lr: 0.0011

Epoch 106/150
715/715 - 3s - 4ms/step - loss: 0.0119 - mae: 0.0622 - mse: 0.0119 - val_loss: 0.0104 - val_mae: 0.0474 - val_mse: 0.0104 - learning_rate: 0.0010 - lr: 0.0010

Epoch 107/150
715/715 - 3s - 4ms/step - loss: 0.0115 - mae: 0.0619 - mse: 0.0115 - val_loss: 0.0103 - val_mae: 0.0470 - val_mse: 0.0103 - learning_rate: 9.8930e-04 - lr: 9.8930e-04

Epoch 108/150
715/715 - 3s - 4ms/step - loss: 0.0115 - mae: 0.0617 - mse: 0.0115 - val_loss: 0.0103 - val_mae: 0.0465 - val_mse: 0.0103 - learning_rate: 9.4794e-04 - lr: 9.4794e-04

Epoch 109/150
715/715 - 3s - 4ms/step - loss: 0.0114 - mae: 0.0616 - mse: 0.0114 - val_loss: 0.0099 - val_mae: 0.0464 - val_mse: 0.0099 - learning_rate: 9.0726e-04 - lr: 9.0726e-04

Epoch 110/150
715/715 - 3s - 4ms/step - loss: 0.0114 - mae: 0.0614 - mse: 0.0114 - val_loss: 0.0101 - val_mae: 0.0462 - val_mse: 0.0101 - learning_rate: 8.6728e-04 - lr: 8.6728e-04

Epoch 111/150
715/715 - 3s - 4ms/step - loss: 0.0113 - mae: 0.0613 - mse: 0.0113 - val_loss: 0.0093 - val_mae: 0.0453 - val_mse: 0.0093 - learning_rate: 8.2801e-04 - lr: 8.2801e-04

Epoch 112/150
715/715 - 3s - 4ms/step - loss: 0.0113 - mae: 0.0612 - mse: 0.0113 - val_loss: 0.0091 - val_mae: 0.0450 - val_mse: 0.0091 - learning_rate: 7.8947e-04 - lr: 7.8947e-04

Epoch 113/150
715/715 - 3s - 4ms/step - loss: 0.0111 - mae: 0.0610 - mse: 0.0111 - val_loss: 0.0087 - val_mae: 0.0443 - val_mse: 0.0087 - learning_rate: 7.5169e-04 - lr: 7.5169e-04

Epoch 114/150
715/715 - 3s - 4ms/step - loss: 0.0111 - mae: 0.0609 - mse: 0.0111 - val_loss: 0.0088 - val_mae: 0.0440 - val_mse: 0.0088 - learning_rate: 7.1468e-04 - lr: 7.1468e-04

Epoch 115/150
715/715 - 3s - 4ms/step - loss: 0.0110 - mae: 0.0607 - mse: 0.0110 - val_loss: 0.0083 - val_mae: 0.0433 - val_mse: 0.0083 - learning_rate: 6.7844e-04 - lr: 6.7844e-04

Epoch 116/150
715/715 - 3s - 4ms/step - loss: 0.0110 - mae: 0.0606 - mse: 0.0110 - val_loss: 0.0082 - val_mae: 0.0426 - val_mse: 0.0082 - learning_rate: 6.4301e-04 - lr: 6.4301e-04

Epoch 117/150
715/715 - 3s - 4ms/step - loss: 0.0108 - mae: 0.0604 - mse: 0.0108 - val_loss: 0.0079 - val_mae: 0.0423 - val_mse: 0.0079 - learning_rate: 6.0839e-04 - lr: 6.0839e-04

Epoch 118/150
715/715 - 3s - 4ms/step - loss: 0.0108 - mae: 0.0603 - mse: 0.0108 - val_loss: 0.0080 - val_mae: 0.0417 - val_mse: 0.0080 - learning_rate: 5.7460e-04 - lr: 5.7460e-04

Epoch 119/150
715/715 - 3s - 4ms/step - loss: 0.0107 - mae: 0.0601 - mse: 0.0107 - val_loss: 0.0079 - val_mae: 0.0415 - val_mse: 0.0079 - learning_rate: 5.4166e-04 - lr: 5.4166e-04

Epoch 120/150
715/715 - 3s - 4ms/step - loss: 0.0106 - mae: 0.0600 - mse: 0.0106 - val_loss: 0.0076 - val_mae: 0.0409 - val_mse: 0.0076 - learning_rate: 5.0957e-04 - lr: 5.0957e-04

Epoch 121/150
715/715 - 3s - 5ms/step - loss: 0.0106 - mae: 0.0598 - mse: 0.0106 - val_loss: 0.0074 - val_mae: 0.0405 - val_mse: 0.0074 - learning_rate: 4.7836e-04 - lr: 4.7836e-04

Epoch 122/150
715/715 - 3s - 5ms/step - loss: 0.0105 - mae: 0.0597 - mse: 0.0105 - val_loss: 0.0075 - val_mae: 0.0402 - val_mse: 0.0075 - learning_rate: 4.4804e-04 - lr: 4.4804e-04

Epoch 123/150
715/715 - 3s - 4ms/step - loss: 0.0104 - mae: 0.0595 - mse: 0.0104 - val_loss: 0.0072 - val_mae: 0.0398 - val_mse: 0.0072 - learning_rate: 4.1861e-04 - lr: 4.1861e-04

Epoch 124/150
715/715 - 3s - 5ms/step - loss: 0.0104 - mae: 0.0595 - mse: 0.0104 - val_loss: 0.0070 - val_mae: 0.0393 - val_mse: 0.0070 - learning_rate: 3.9010e-04 - lr: 3.9010e-04

Epoch 125/150
715/715 - 4s - 5ms/step - loss: 0.0103 - mae: 0.0593 - mse: 0.0103 - val_loss: 0.0069 - val_mae: 0.0390 - val_mse: 0.0069 - learning_rate: 3.6252e-04 - lr: 3.6252e-04

Epoch 126/150
715/715 - 3s - 4ms/step - loss: 0.0103 - mae: 0.0592 - mse: 0.0103 - val_loss: 0.0068 - val_mae: 0.0387 - val_mse: 0.0068 - learning_rate: 3.3587e-04 - lr: 3.3587e-04

Epoch 127/150
715/715 - 3s - 4ms/step - loss: 0.0102 - mae: 0.0591 - mse: 0.0102 - val_loss: 0.0066 - val_mae: 0.0384 - val_mse: 0.0066 - learning_rate: 3.1017e-04 - lr: 3.1017e-04

Epoch 128/150
715/715 - 3s - 4ms/step - loss: 0.0102 - mae: 0.0590 - mse: 0.0102 - val_loss: 0.0065 - val_mae: 0.0382 - val_mse: 0.0065 - learning_rate: 2.8543e-04 - lr: 2.8543e-04

Epoch 129/150
715/715 - 3s - 4ms/step - loss: 0.0102 - mae: 0.0588 - mse: 0.0102 - val_loss: 0.0064 - val_mae: 0.0380 - val_mse: 0.0064 - learning_rate: 2.6167e-04 - lr: 2.6167e-04

Epoch 130/150
715/715 - 3s - 4ms/step - loss: 0.0101 - mae: 0.0587 - mse: 0.0101 - val_loss: 0.0063 - val_mae: 0.0379 - val_mse: 0.0063 - learning_rate: 2.3888e-04 - lr: 2.3888e-04

Epoch 131/150
715/715 - 3s - 4ms/step - loss: 0.0101 - mae: 0.0586 - mse: 0.0101 - val_loss: 0.0062 - val_mae: 0.0377 - val_mse: 0.0062 - learning_rate: 2.1709e-04 - lr: 2.1709e-04

Epoch 132/150
715/715 - 3s - 4ms/step - loss: 0.0100 - mae: 0.0586 - mse: 0.0100 - val_loss: 0.0060 - val_mae: 0.0375 - val_mse: 0.0060 - learning_rate: 1.9630e-04 - lr: 1.9630e-04

Epoch 133/150
715/715 - 3s - 4ms/step - loss: 0.0100 - mae: 0.0585 - mse: 0.0100 - val_loss: 0.0060 - val_mae: 0.0374 - val_mse: 0.0060 - learning_rate: 1.7652e-04 - lr: 1.7652e-04

Epoch 134/150
715/715 - 3s - 4ms/step - loss: 0.0100 - mae: 0.0584 - mse: 0.0100 - val_loss: 0.0059 - val_mae: 0.0373 - val_mse: 0.0059 - learning_rate: 1.5776e-04 - lr: 1.5776e-04

Epoch 135/150
715/715 - 3s - 4ms/step - loss: 0.0099 - mae: 0.0583 - mse: 0.0099 - val_loss: 0.0058 - val_mae: 0.0372 - val_mse: 0.0058 - learning_rate: 1.4003e-04 - lr: 1.4003e-04

Epoch 136/150
715/715 - 3s - 4ms/step - loss: 0.0099 - mae: 0.0582 - mse: 0.0099 - val_loss: 0.0057 - val_mae: 0.0370 - val_mse: 0.0057 - learning_rate: 1.2333e-04 - lr: 1.2333e-04

Epoch 137/150
715/715 - 3s - 4ms/step - loss: 0.0099 - mae: 0.0582 - mse: 0.0099 - val_loss: 0.0056 - val_mae: 0.0368 - val_mse: 0.0056 - learning_rate: 1.0768e-04 - lr: 1.0768e-04

Epoch 138/150
715/715 - 3s - 4ms/step - loss: 0.0099 - mae: 0.0581 - mse: 0.0099 - val_loss: 0.0055 - val_mae: 0.0367 - val_mse: 0.0055 - learning_rate: 9.3075e-05 - lr: 9.3075e-05

Epoch 139/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0580 - mse: 0.0098 - val_loss: 0.0054 - val_mae: 0.0366 - val_mse: 0.0054 - learning_rate: 7.9526e-05 - lr: 7.9526e-05

Epoch 140/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0580 - mse: 0.0098 - val_loss: 0.0054 - val_mae: 0.0365 - val_mse: 0.0054 - learning_rate: 6.7040e-05 - lr: 6.7040e-05

Epoch 141/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0579 - mse: 0.0098 - val_loss: 0.0053 - val_mae: 0.0365 - val_mse: 0.0053 - learning_rate: 5.5620e-05 - lr: 5.5620e-05

Epoch 142/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0579 - mse: 0.0098 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 4.5273e-05 - lr: 4.5273e-05

Epoch 143/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0578 - mse: 0.0098 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 3.6003e-05 - lr: 3.6003e-05

Epoch 144/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0578 - mse: 0.0098 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 2.7814e-05 - lr: 2.7814e-05

Epoch 145/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0577 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 2.0709e-05 - lr: 2.0709e-05

Epoch 146/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0577 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 1.4693e-05 - lr: 1.4693e-05

Epoch 147/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0577 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 9.7661e-06 - lr: 9.7661e-06

Epoch 148/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0577 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 5.9322e-06 - lr: 5.9322e-06

Epoch 149/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0576 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 3.1925e-06 - lr: 3.1925e-06

Epoch 150/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0576 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 1.5482e-06 - lr: 1.5482e-06

Restoring model weights from the end of the best epoch: 144.

============================================================
Model Evaluation:
Test Loss (MSE): 0.005302
Test MAE: 0.036418
Test MSE: 0.005302

Metrics on original scale:
MSE: 0.000160
RMSE: 0.012649
MAE: 0.006326
R² Score: 0.994517
Correlation Coefficient: 0.997259

============================================================
Testing at known minimum [4, 4, 4, 4]:
Predicted: -8.6552
Expected: ~-10.5363

Training history plot saved as 'training_history.png'
Ignoring fixed x limits to fulfill fixed data aspect with adjustable data limits.
Ignoring fixed x limits to fulfill fixed data aspect with adjustable data limits.

Correlation plot saved as 'correlation_plot.png'

Model saved to shekel_model.keras
Scalers saved to scaler_X.pkl and scaler_y.pkl

============================================================
Training complete!
PASSED                               [ 70%]
shekel/py/test_keras2onnx.py::test_keras2onnx 
-------------------------------- live log call ---------------------------------
Using tensorflow=2.20.0, onnx=1.20.1, tf2onnx=1.16.1/15c810
Using opset <onnx, 13>
Computed 0 values for constant folding
Optimizing ONNX model
After optimization: Identity -2 (2->0)
PASSED                     [ 80%]
shekel/py/test_shgo_keras.py::test_shgo_nn 
-------------------------------- live log call ---------------------------------
============================================================

============================================================
Testing trained model on new points:
============================================================

============================================================
Comparing Model Predictions vs Actual Function
============================================================
Model loaded from shekel_model_expected.keras
Scalers loaded

Point                                    Actual       Predicted    Error     
--------------------------------------------------------------------------------
['4.00', '4.00', '4.00', '4.00']         -10.536284   -8.655185    1.881099  
['1.00', '1.00', '1.00', '1.00']         -5.128471    -3.301217    1.827254  
['8.00', '8.00', '8.00', '8.00']         -5.175617    -3.962699    1.212918  
['3.50', '4.20', '3.80', '4.10']         -2.705209    -2.463345    0.241863  
['6.00', '5.50', '6.20', '5.80']         -1.774179    -1.777872    0.003693  

--------------------------------------------------------------------------------
Mean Absolute Error (MAE): 1.033366
Root Mean Squared Error (RMSE): 1.296694
Max Error: 1.881099

================================================================================
COMPARING SHGO OPTIMIZATION: ACTUAL FUNCTION vs NEURAL NETWORK MODEL
================================================================================

[1/2] Running SHGO with ACTUAL Shekel function...
--------------------------------------------------------------------------------

[2/2] Running SHGO with NEURAL NETWORK model...
--------------------------------------------------------------------------------
Model loaded from shekel_model_expected.keras
Scalers loaded

============================================================
Optimizing with Neural Network Model using SHGO
============================================================
[Elapsed time: 14.669] Elapsed CPU time: 14.416 seconds

Optimization Results (Model):
Success: True
Message: Optimization terminated successfully.

Optimal solution (x*):
  x = [3.75      3.75      3.7497237 3.75     ]

Optimal function value (f(x*)):
  f(x*) = -3.7842361927

Number of function evaluations: 1059
Number of iterations: 5

================================================================================
COMPARISON RESULTS
================================================================================

Metric                         Actual Function      NN Model             Difference     
-------------------------------------------------------------------------------------
Optimal X:                    
  x[0]                         4.000747             3.750000             0.250747       
  x[1]                         3.999509             3.750000             0.249509       
  x[2]                         4.000747             3.749724             0.251023       
  x[3]                         3.999509             3.750000             0.249509       

Optimal f(x):                 
  Function result             -10.5364431535       -3.7842361927        6.7522069608   
  Actual Shekel at x*         -10.5364431535       -3.3562372557        7.1802058978   

Performance:                  
  Function evaluations        2254                 1059                 1195           
  Iterations                  5                    5                    0              
  Success                     True                 True                

================================================================================
SUMMARY
================================================================================
Distance between optimal solutions (L2 norm): 0.500396
Difference in actual function values: 7.1802058978
✗ Neural network model solution differs significantly from actual optimum.
⚡ Speedup: 2.13x fewer function evaluations with NN model
PASSED                        [ 90%]
shekel/py/test_shgo_shekel_keras2onnx.py::test_shgo_shekel_onnx 
-------------------------------- live log call ---------------------------------
Using tensorflow=2.20.0, onnx=1.20.1, tf2onnx=1.16.1/15c810
Using opset <onnx, 13>
Computed 0 values for constant folding
Optimizing ONNX model
After optimization: Identity -2 (2->0)
============================================================

============================================================
Testing trained ONNX model on new points:
============================================================

============================================================
Comparing Model Predictions vs Actual Function
============================================================
ONNX model loaded from shekel_model.onnx
Scalers loaded

Point                                    Actual       Predicted    Error     
--------------------------------------------------------------------------------
['4.00', '4.00', '4.00', '4.00']         -10.536284   -8.655186    1.881098  
['1.00', '1.00', '1.00', '1.00']         -5.128471    -3.301217    1.827254  
['8.00', '8.00', '8.00', '8.00']         -5.175617    -3.962699    1.212919  
['3.50', '4.20', '3.80', '4.10']         -2.705209    -2.463345    0.241863  
['6.00', '5.50', '6.20', '5.80']         -1.774179    -1.777872    0.003693  

--------------------------------------------------------------------------------
Mean Absolute Error (MAE): 1.033365
Root Mean Squared Error (RMSE): 1.296693
Max Error: 1.881098

================================================================================
COMPARING SHGO OPTIMIZATION: ACTUAL FUNCTION vs ONNX NEURAL NETWORK MODEL
================================================================================

[1/2] Running SHGO with ACTUAL Shekel function...
--------------------------------------------------------------------------------

[2/2] Running SHGO with ONNX NEURAL NETWORK model...
--------------------------------------------------------------------------------
ONNX model loaded from shekel_model.onnx
Scalers loaded

============================================================
Optimizing with Keras ONNX Neural Network Model using SHGO
============================================================
[Elapsed time: 0.566] Elapsed CPU time: 0.604 seconds

Optimization Results (ONNX Model):
Success: True
Message: Optimization terminated successfully.

Optimal solution (x*):
  x = [3.75      3.75      3.7499218 3.75     ]

Optimal function value (f(x*)):
  f(x*) = -3.7852702141

Number of function evaluations: 1052
Number of iterations: 5

================================================================================
COMPARISON RESULTS
================================================================================

Metric                         Actual Function      NN Model             Difference     
-------------------------------------------------------------------------------------
Optimal X:                    
  x[0]                         4.000747             3.750000             0.250747       
  x[1]                         3.999509             3.750000             0.249509       
  x[2]                         4.000747             3.749922             0.250825       
  x[3]                         3.999509             3.750000             0.249509       

Optimal f(x):                 
  Function result             -10.5364431535       -3.7852702141        6.7511729394   
  Actual Shekel at x*         -10.5364431535       -3.3570769339        7.1793662196   

Performance:                  
  Function evaluations        2254                 1052                 1202           
  Iterations                  5                    5                    0              
  Success                     True                 True                

================================================================================
SUMMARY
================================================================================
Distance between optimal solutions (L2 norm): 0.500297
Difference in actual function values: 7.1793662196
Distance from [4,4,4,4]             0.001264             0.500039            
✗ ONNX neural network model solution differs significantly from actual optimum.
⚡ Speedup: 2.14x fewer function evaluations with ONNX NN model
PASSED   [100%]

=============================== warnings summary ===============================
../../../../../usr/lib/python3/dist-packages/z3/z3core.py:5
  /usr/lib/python3/dist-packages/z3/z3core.py:5: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
    import pkg_resources

bnh/py/test_z3.py: 1 warning
bnh/py/test_z3_bnh_pysmt.py: 1 warning
bnh/py/test_z3_gradient.py: 1 warning
bnh/py/test_z3_minimax.py: 1 warning
constraint_dora/py/test_z3_nonlinear.py: 1 warning
constraint_dora/py/test_z3_pysmt.py: 1 warning
shekel/py/test_keras.py: 1 warning
shekel/py/test_keras2onnx.py: 1 warning
shekel/py/test_shgo_keras.py: 1 warning
shekel/py/test_shgo_shekel_keras2onnx.py: 1 warning
  /usr/local/lib/python3.12/dist-packages/py/_process/forkedfunc.py:45: DeprecationWarning: This process (pid=638389) is multi-threaded, use of fork() may lead to deadlocks in the child.
    pid = os.fork()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
========== 10 passed, 22 deselected, 11 warnings in 579.12s (0:09:39) ==========
env PYTHONDONTWRITEBYTECODE=1 /usr/bin/pytest -v -s -m "not forked" /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-7.4.4, pluggy-1.4.0 -- /usr/bin/python3
cachedir: .pytest_cache
rootdir: /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples
plugins: forked-1.6.0, typeguard-4.4.4, mock-3.12.0
collecting ... autograd only supports numpy < 2.0.0 versions.
======================================================================
EXAMPLE 1: Simple Quadratic Optimization
Minimize (x-1)² + (y-1)² subject to x + y >= 0.5
======================================================================
collected 32 items / 10 deselected / 22 selected

bnh/py/test_nsga2.py::test_nsga2 
## Problem definition
```
"Boundary definition"
    def __init__(self):
        super().__init__(n_var=2, n_obj=2, n_ieq_constr=2, vtype=float)
        self.xl = np.zeros(self.n_var)
        self.xu = np.array([5.0, 3.0])

"Functions (F) and inequality constraints (G) definitions"
    def _evaluate(self, x, out, *args, **kwargs):
        f1 = 4 * x[:, 0] ** 2 + 4 * x[:, 1] ** 2
        f2 = (x[:, 0] - 5) ** 2 + (x[:, 1] - 5) ** 2
        g1 = (1 / 25) * ((x[:, 0] - 5) ** 2 + x[:, 1] ** 2 - 25)
        g2 = -1 / 7.7 * ((x[:, 0] - 8) ** 2 + (x[:, 1] + 3) ** 2 - 7.7)

        out["F"] = anp.column_stack([f1, f2])
        out["G"] = anp.column_stack([g1, g2])

"Expected Pareto front"
    def _calc_pareto_front(self, n_points=100):
        x1 = np.linspace(0, 5, n_points)
        x2 = np.linspace(0, 5, n_points)
        x2[x1 >= 3] = 3

        X = np.column_stack([x1, x2])
        return self.evaluate(X, return_values_of=["F"])

```
PASSED
c3dtlz4/py/test_gpsampler.py::test_gpsampler [I 2026-01-27 14:34:42,298] A new study created in RDB with name: my_study
[I 2026-01-27 14:34:42,389] Trial 0 finished with values: [1.2569645750397653, 4.4185249455598885e-43] and parameters: {'x0': 0.3745401188473625, 'x1': 0.9507143064099162, 'x2': 0.7319939418114051}.
[I 2026-01-27 14:34:42,458] Trial 1 finished with values: [1.236662945761789, 1.0145778959154135e-22] and parameters: {'x0': 0.5986584841970366, 'x1': 0.15601864044243652, 'x2': 0.15599452033620265}.
[I 2026-01-27 14:34:42,521] Trial 2 finished with values: [1.1443092153344159, 4.571144650098741e-124] and parameters: {'x0': 0.05808361216819946, 'x1': 0.8661761457749352, 'x2': 0.6011150117432088}.
[I 2026-01-27 14:34:42,584] Trial 3 finished with values: [1.4506544962685188, 2.3198580035410804e-15] and parameters: {'x0': 0.7080725777960455, 'x1': 0.020584494295802447, 'x2': 0.9699098521619943}.
[I 2026-01-27 14:34:42,654] Trial 4 finished with values: [1.1839841387381274, 2.017890472548048e-08] and parameters: {'x0': 0.8324426408004217, 'x1': 0.21233911067827616, 'x2': 0.18182496720710062}.
[I 2026-01-27 14:34:42,718] Trial 5 finished with values: [1.0389339803486743, 3.578564831812836e-74] and parameters: {'x0': 0.18340450985343382, 'x1': 0.3042422429595377, 'x2': 0.5247564316322378}.
[I 2026-01-27 14:34:42,784] Trial 6 finished with values: [1.0560963419602245, 5.789891256550866e-37] and parameters: {'x0': 0.43194501864211576, 'x1': 0.2912291401980419, 'x2': 0.6118528947223795}.
[I 2026-01-27 14:34:42,847] Trial 7 finished with values: [1.0610630040604079, 4.7574085951972936e-86] and parameters: {'x0': 0.13949386065204183, 'x1': 0.29214464853521815, 'x2': 0.3663618432936917}.
[I 2026-01-27 14:34:42,917] Trial 8 finished with values: [1.171521166079494, 1.4723771522333772e-34] and parameters: {'x0': 0.45606998421703593, 'x1': 0.7851759613930136, 'x2': 0.19967378215835974}.
[I 2026-01-27 14:34:42,991] Trial 9 finished with values: [1.2142476806598177, 2.491970914729005e-29] and parameters: {'x0': 0.5142344384136116, 'x1': 0.5924145688620425, 'x2': 0.046450412719997725}.
[I 2026-01-27 14:34:48,306] Trial 10 finished with values: [1.5, 3.2491320545635597e-24] and parameters: {'x0': 0.5772920306987442, 'x1': 1.0, 'x2': 1.0}.
[I 2026-01-27 14:34:48,648] Trial 11 finished with values: [1.4810590572975788, 2.247168740014078e-21] and parameters: {'x0': 0.616381271404474, 'x1': 0.0, 'x2': 0.9806860277744494}.
[I 2026-01-27 14:34:49,018] Trial 12 finished with values: [1.3962865006473386, 0.0] and parameters: {'x0': 0.0, 'x1': 0.0, 'x2': 0.8824741829814644}.
[I 2026-01-27 14:34:49,560] Trial 13 finished with values: [1.4319600251053517, 4.613008316404974e-65] and parameters: {'x0': 0.22548587130877948, 'x1': 0.0, 'x2': 0.926567726281949}.
[I 2026-01-27 14:34:50,048] Trial 14 finished with values: [1.465779084619566, 3.1399921722886888e-18] and parameters: {'x0': 0.662746437129491, 'x1': 1.0, 'x2': 0.9645202736367552}.
[I 2026-01-27 14:34:50,575] Trial 15 finished with values: [1.4451148461174426, 2.5479755591424486e-80] and parameters: {'x0': 0.15867252402756324, 'x1': 1.0, 'x2': 0.9417180617967104}.
[I 2026-01-27 14:34:51,157] Trial 16 finished with values: [1.4458575475275, 6.445293888399892e-57] and parameters: {'x0': 0.2719756187229081, 'x1': 1.0, 'x2': 0.9425579595120845}.
[I 2026-01-27 14:34:51,791] Trial 17 finished with values: [1.2551949716510336, 6.91201430745902e-107] and parameters: {'x0': 0.086188185083234, 'x1': 0.3403540168942501, 'x2': 0.9792787620260596}.
[I 2026-01-27 14:34:52,481] Trial 18 finished with values: [1.4448556966456945, 0.0] and parameters: {'x0': 0.0, 'x1': 0.9998164559581626, 'x2': 0.9416324342698537}.
[I 2026-01-27 14:34:53,164] Trial 19 finished with values: [1.4445869046210822, 4.585494497145505e-44] and parameters: {'x0': 0.36564128475900937, 'x1': 0.0030928341581776507, 'x2': 0.9446011394003956}.
[I 2026-01-27 14:34:53,860] Trial 20 finished with values: [1.4443633775767744, 3.095706585905284e-36] and parameters: {'x0': 0.4378744798804788, 'x1': 0.9996722435385519, 'x2': 0.9412380611573807}.
[I 2026-01-27 14:34:55,077] Trial 21 finished with values: [1.4434085272454735, 4.308745749927249e-45] and parameters: {'x0': 0.3570988217548881, 'x1': 0.999709898427196, 'x2': 0.940111968320966}.
[I 2026-01-27 14:34:56,611] Trial 22 finished with values: [1.4631306710395755, 2.888001914202581e-17] and parameters: {'x0': 0.6776289390630913, 'x1': 0.038339225145155344, 'x2': 0.9999999999999999}.
[I 2026-01-27 14:34:57,476] Trial 23 finished with values: [1.4439732243595276, 1.729331037500226e-25] and parameters: {'x0': 0.5608180605667094, 'x1': 0.9994245458459029, 'x2': 0.9410763509486098}.
[I 2026-01-27 14:34:58,545] Trial 24 finished with values: [1.3849396931161913, 8.360840801109502e-13] and parameters: {'x0': 0.751358133688389, 'x1': 0.8673413849761437, 'x2': 1.0}.
[I 2026-01-27 14:34:59,225] Trial 25 finished with values: [1.5, 0.0] and parameters: {'x0': 0.0, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:00,405] Trial 26 finished with values: [1.438178203632694, 5.443791033012012e-68] and parameters: {'x0': 0.21077528667826861, 'x1': 0.9674237351222392, 'x2': 0.03128552883757819}.
[I 2026-01-27 14:35:01,048] Trial 27 finished with values: [1.4450624114934714, 8.972767234624615e-96] and parameters: {'x0': 0.11116528844432848, 'x1': 0.9718962885211153, 'x2': 0.028432078727286966}.
[I 2026-01-27 14:35:01,844] Trial 28 finished with values: [1.45453088833317, 0.0] and parameters: {'x0': 0.0, 'x1': 0.04774908697364682, 'x2': 1.0}.
[I 2026-01-27 14:35:02,694] Trial 29 finished with values: [1.4434566403695939, 1.3272700039370876e-118] and parameters: {'x0': 0.06571648961065392, 'x1': 0.9997016108337469, 'x2': 0.940176033536303}.
[I 2026-01-27 14:35:03,611] Trial 30 finished with values: [1.4443394851480686, 1.198167541256189e-29] and parameters: {'x0': 0.5095974942910249, 'x1': 0.009069883410625559, 'x2': 0.9509180699124974}.
[I 2026-01-27 14:35:04,523] Trial 31 finished with values: [1.4466735607290349, 6.783392741743936e-54] and parameters: {'x0': 0.29157451735571727, 'x1': 0.05652107070455444, 'x2': 0.9999999999999999}.
[I 2026-01-27 14:35:05,266] Trial 32 finished with values: [1.4442557968207508, 1.4720918900233475e-49] and parameters: {'x0': 0.3221971609026787, 'x1': 0.9695063579934945, 'x2': 0.026904262728530467}.
[I 2026-01-27 14:35:05,950] Trial 33 finished with values: [1.4462908447552192, 4.271025515569692e-15] and parameters: {'x0': 0.7124289623664152, 'x1': 0.9430472263260647, 'x2': 1.0}.
[I 2026-01-27 14:35:06,720] Trial 34 finished with values: [8.874509332699859e-17, 1.4493173605448753] and parameters: {'x0': 1.0, 'x1': 0.053550270976815036, 'x2': 1.0}.
[I 2026-01-27 14:35:08,122] Trial 35 finished with values: [1.4421568787639951, 1.837065440088349e-05] and parameters: {'x0': 0.8893852496345934, 'x1': 0.061642977835416814, 'x2': 1.0}.
[I 2026-01-27 14:35:08,744] Trial 36 finished with values: [1.2860383181663224, 0.007326510084370064] and parameters: {'x0': 0.945355568439831, 'x1': 0.23338666264443744, 'x2': 0.9636556003781945}.
[I 2026-01-27 14:35:09,423] Trial 37 finished with values: [1.3979167019120315, 0.013403684702364877] and parameters: {'x0': 0.9502898645034935, 'x1': 0.0, 'x2': 0.8846829342483115}.
[I 2026-01-27 14:35:09,934] Trial 38 finished with values: [1.499773897996056, 0.026043327162943688] and parameters: {'x0': 0.9559497666532302, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-27 14:35:10,794] Trial 39 finished with values: [1.250712737004554, 0.1010776946800888] and parameters: {'x0': 0.9707431854307712, 'x1': 0.1255272850041475, 'x2': 0.8384680586709571}.
[I 2026-01-27 14:35:13,452] Trial 40 finished with values: [1.3328222247586254, 0.24141091158846267] and parameters: {'x0': 0.9785247538773743, 'x1': 0.17672166502758255, 'x2': 1.0}.
[I 2026-01-27 14:35:14,246] Trial 41 finished with values: [1.4328815666695063, 0.14666919156543964] and parameters: {'x0': 0.9730272162589036, 'x1': 0.06368760392959295, 'x2': 1.0}.
[I 2026-01-27 14:35:15,079] Trial 42 finished with values: [1.333977566908557, 0.2627994815408751] and parameters: {'x0': 0.9793282889081107, 'x1': 0.0, 'x2': 0.8310853198854861}.
[I 2026-01-27 14:35:15,675] Trial 43 finished with values: [1.4122457398788533, 0.5055313740946554] and parameters: {'x0': 0.984920642366945, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-27 14:35:16,523] Trial 44 finished with values: [1.1974133712736046, 0.5105779210003433] and parameters: {'x0': 0.9864894843873959, 'x1': 0.07568957199019709, 'x2': 0.8488350593516301}.
[I 2026-01-27 14:35:17,489] Trial 45 finished with values: [1.318254059817107, 0.4927308930891034] and parameters: {'x0': 0.9853124018847859, 'x1': 0.04044778491539739, 'x2': 0.9428788547815112}.
[I 2026-01-27 14:35:18,221] Trial 46 finished with values: [1.2614016531710956, 0.6636658970949502] and parameters: {'x0': 0.9883033388809468, 'x1': 0.08126697372492657, 'x2': 1.0}.
[I 2026-01-27 14:35:18,923] Trial 47 finished with values: [1.3542788183245063, 0.5393498690106704] and parameters: {'x0': 0.9858826369514627, 'x1': 0.04422870032782121, 'x2': 1.0}.
[I 2026-01-27 14:35:19,705] Trial 48 finished with values: [1.1764610493163947, 0.7404700267131807] and parameters: {'x0': 0.9897700601868784, 'x1': 0.05974041189912139, 'x2': 0.9430166409846574}.
[I 2026-01-27 14:35:20,604] Trial 49 finished with values: [1.408662439009498, 0.014765025759469901] and parameters: {'x0': 0.9511366463613904, 'x1': 0.10157834223468554, 'x2': 1.0}.
[I 2026-01-27 14:35:21,442] Trial 50 finished with values: [1.5, 5.457685652943318e-09] and parameters: {'x0': 0.8196868612059792, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-27 14:35:22,099] Trial 51 finished with values: [1.2502148317987192, 3.165222896043341e-22] and parameters: {'x0': 0.6054426311337101, 'x1': 0.5146571415603193, 'x2': 0.9999999999999999}.
[I 2026-01-27 14:35:22,910] Trial 52 finished with values: [1.2397444554013508, 0.15996434332802972] and parameters: {'x0': 0.9752630760696195, 'x1': 0.4953136217641316, 'x2': 1.0}.
[I 2026-01-27 14:35:23,753] Trial 53 finished with values: [0.8928310418974353, 1.1728143416176722] and parameters: {'x0': 0.9946659370399358, 'x1': 0.026726249990533968, 'x2': 1.0}.
[I 2026-01-27 14:35:24,804] Trial 54 finished with values: [1.43786159331247, 0.01744329138951213] and parameters: {'x0': 0.9525278922928867, 'x1': 0.05419850242794867, 'x2': 0.9891098241291347}.
[I 2026-01-27 14:35:25,688] Trial 55 finished with values: [7.810200787676129e-17, 1.2755025846005386] and parameters: {'x0': 1.0, 'x1': 1.0, 'x2': 0.659695286719861}.
[I 2026-01-27 14:35:26,716] Trial 56 finished with values: [7.107475185042783e-17, 1.1607387844383024] and parameters: {'x0': 1.0, 'x1': 0.454084643766724, 'x2': 0.8982845270661086}.
[I 2026-01-27 14:35:27,534] Trial 57 finished with values: [8.990407216803828e-17, 1.4682449214031834] and parameters: {'x0': 1.0, 'x1': 0.03283309042357941, 'x2': 0.0}.
[I 2026-01-27 14:35:28,918] Trial 58 finished with values: [9.184850993605148e-17, 1.5] and parameters: {'x0': 1.0, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:29,857] Trial 59 finished with values: [1.4999999999999605, 3.44511784004443e-07] and parameters: {'x0': 0.854377749349218, 'x1': 0.0, 'x2': 0.9999999999999999}.
[I 2026-01-27 14:35:30,605] Trial 60 finished with values: [1.4999731145795585, 0.008980842861265463] and parameters: {'x0': 0.9458256456878092, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:31,825] Trial 61 finished with values: [1.4960084260109783, 3.3197943026015283e-11] and parameters: {'x0': 0.7789338415213843, 'x1': 0.0, 'x2': 0.9959923648716562}.
[I 2026-01-27 14:35:32,787] Trial 62 finished with values: [1.4999999999883415, 5.913995337808853e-06] and parameters: {'x0': 0.8790158343733363, 'x1': 0.9999999999999999, 'x2': 0.0}.
[I 2026-01-27 14:35:33,908] Trial 63 finished with values: [1.5, 3.2037889857640175e-58] and parameters: {'x0': 0.26383628773780876, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-27 14:35:34,889] Trial 64 finished with values: [1.5, 7.525798909714829e-32] and parameters: {'x0': 0.4842207708388645, 'x1': 1.0, 'x2': 1.0}.
[I 2026-01-27 14:35:37,314] Trial 65 finished with values: [1.5, 4.090321744413122e-10] and parameters: {'x0': 0.7987216626001631, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:37,847] Trial 66 finished with values: [1.499999999999999, 5.760234325197428e-08] and parameters: {'x0': 0.8392324926738586, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:38,502] Trial 67 finished with values: [1.5, 1.3982607319873028e-13] and parameters: {'x0': 0.7374520211713042, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:39,639] Trial 68 finished with values: [1.3427104373511138, 0.12513088585333118] and parameters: {'x0': 0.972120490103324, 'x1': 0.9635302668022724, 'x2': 0.13439342155933764}.
[I 2026-01-27 14:35:40,684] Trial 69 finished with values: [1.4999999777744355, 0.00025821830498866074] and parameters: {'x0': 0.9128465087785315, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:41,507] Trial 70 finished with values: [1.5, 1.2712759903448953e-145] and parameters: {'x0': 0.03526308362768111, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-27 14:35:42,536] Trial 71 finished with values: [1.5, 1.232699815798953e-16] and parameters: {'x0': 0.6873634924095512, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:43,720] Trial 72 finished with values: [1.5, 8.890297558699147e-21] and parameters: {'x0': 0.624837450381796, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:44,664] Trial 73 finished with values: [1.067420731123239, 0.7940539241671648] and parameters: {'x0': 0.9910550685344225, 'x1': 0.973895564674685, 'x2': 0.17472760320517677}.
[I 2026-01-27 14:35:46,024] Trial 74 finished with values: [1.5, 1.3607954625359705e-33] and parameters: {'x0': 0.4651744118160152, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-27 14:35:47,059] Trial 75 finished with values: [1.5, 9.893778082996251e-120] and parameters: {'x0': 0.06400759512114065, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-27 14:35:48,020] Trial 76 finished with values: [1.5, 6.92676178606209e-139] and parameters: {'x0': 0.041179701210507944, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:49,462] Trial 77 finished with values: [1.5, 1.1809200984814624e-39] and parameters: {'x0': 0.40457597905999715, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-27 14:35:50,622] Trial 78 finished with values: [1.5, 3.050846512441688e-36] and parameters: {'x0': 0.4376451220518814, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-27 14:35:51,383] Trial 79 finished with values: [1.5, 6.736335924136762e-38] and parameters: {'x0': 0.4212715455088259, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-27 14:35:52,133] Trial 80 finished with values: [1.5, 5.912572067416848e-96] and parameters: {'x0': 0.11066127077486723, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-27 14:35:53,049] Trial 81 finished with values: [1.5, 5.739028975645033e-79] and parameters: {'x0': 0.16363125933768616, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-27 14:35:53,823] Trial 82 finished with values: [1.5, 4.73764660477088e-72] and parameters: {'x0': 0.1918816813393383, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-27 14:35:54,622] Trial 83 finished with values: [1.3592823034457704, 5.827654946793028e-74] and parameters: {'x0': 0.1838064058306985, 'x1': 0.8305787401599964, 'x2': 0.0}.
[I 2026-01-27 14:35:55,466] Trial 84 finished with values: [1.5, 1.6561638887331216e-39] and parameters: {'x0': 0.4059466125436356, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:35:56,291] Trial 85 finished with values: [1.2544910760381072, 0.04044571843886656] and parameters: {'x0': 0.961881001528379, 'x1': 0.5717140614132067, 'x2': 0.0}.
[I 2026-01-27 14:35:57,094] Trial 86 finished with values: [1.5, 1.9875897175306012e-87] and parameters: {'x0': 0.1346669909072944, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-27 14:35:59,533] Trial 87 finished with values: [1.5, 8.47049247559072e-26] and parameters: {'x0': 0.5566176523384142, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-27 14:36:00,358] Trial 88 finished with values: [1.4314910610822649, 9.982678623122517e-44] and parameters: {'x0': 0.3685304570118488, 'x1': 0.9260176769598474, 'x2': 0.0}.
[I 2026-01-27 14:36:01,004] Trial 89 finished with values: [1.378368946889322, 7.54188144322449e-72] and parameters: {'x0': 0.19293896454141676, 'x1': 0.14171387566733465, 'x2': 0.0}.
[I 2026-01-27 14:36:01,784] Trial 90 finished with values: [1.2657674926128637, 2.132497468800038e-74] and parameters: {'x0': 0.18209756861647983, 'x1': 0.6255686768778894, 'x2': 1.0}.
[I 2026-01-27 14:36:02,845] Trial 91 finished with values: [1.3678137788071427, 5.468271425810099e-87] and parameters: {'x0': 0.13616234809017383, 'x1': 0.15675988170503333, 'x2': 0.0}.
[I 2026-01-27 14:36:03,723] Trial 92 finished with values: [1.4226636495375196, 2.212303007446426e-88] and parameters: {'x0': 0.1318123870621619, 'x1': 1.0, 'x2': 0.9155281573341565}.
[I 2026-01-27 14:36:04,596] Trial 93 finished with values: [0.27659441885549313, 1.260066808394015] and parameters: {'x0': 0.9985211829335164, 'x1': 0.7001674005539376, 'x2': 1.0}.
[I 2026-01-27 14:36:05,529] Trial 94 finished with values: [1.5, 6.954658168092794e-62] and parameters: {'x0': 0.24249380409869348, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:36:06,298] Trial 95 finished with values: [0.5609604161550339, 1.1544182178050615] and parameters: {'x0': 0.9966095696471239, 'x1': 0.21051724657711737, 'x2': 0.9468715566731107}.
[I 2026-01-27 14:36:07,222] Trial 96 finished with values: [1.5, 1.2913452454800858e-25] and parameters: {'x0': 0.5589697586129256, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:36:07,746] Trial 97 finished with values: [1.5, 6.694313455144608e-54] and parameters: {'x0': 0.29143046488351065, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-27 14:36:08,415] Trial 98 finished with values: [1.5, 3.424508652094816e-28] and parameters: {'x0': 0.5267734420277151, 'x1': 1.0, 'x2': 1.0}.
[I 2026-01-27 14:36:10,056] Trial 99 finished with values: [1.5, 1.2297035024645931e-22] and parameters: {'x0': 0.5986540056289273, 'x1': 1.0, 'x2': 0.0}.

my_study /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/c3dtlz4/py/example.db 100
Number of trials = 100
Trial: 10 [Elapsed time: 16.0] Elapsed CPU time: 10.5 seconds
Trial: 20 [Elapsed time: 26.2] Elapsed CPU time: 32.3 seconds
Trial: 30 [Elapsed time: 35.7] Elapsed CPU time: 61.8 seconds
Trial: 40 [Elapsed time: 43.8] Elapsed CPU time: 89.6 seconds
Trial: 50 [Elapsed time: 53.6] Elapsed CPU time: 118.1 seconds
Trial: 60 [Elapsed time: 62.9] Elapsed CPU time: 148.6 seconds
Trial: 70 [Elapsed time: 73.7] Elapsed CPU time: 178.3 seconds
Trial: 80 [Elapsed time: 84.4] Elapsed CPU time: 210.3 seconds
Trial: 90 [Elapsed time: 94.1] Elapsed CPU time: 236.8 seconds
Trial: 100 [Elapsed time: 103.1] Elapsed CPU time: 266.6 seconds
================================================================================
DATAFRAME COMPARISON REPORT
================================================================================

================================================================================
SUMMARY:
  Total mismatches found: 0
  Float threshold: 0.01%
  ✓ DataFrames match!
================================================================================
PASSED
c3dtlz4/py/test_variables_transformation.py::test_variables_transformation 
================================================================================
DATAFRAME COMPARISON REPORT
================================================================================

────────────────────────────────────────────────────────────────────────────────
Column: 'F1' - 4 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: 1.4445869046211222 != 1.447670173156161 (rel_diff=2.134e-03)
  Line 78: 1.495003685794626 != 1.497532147728052 (rel_diff=1.691e-03)
  Line 117: 1.47682371956128 != 1.478969064161009 (rel_diff=1.453e-03)
  Line 327: 1.4454059271875428 != 1.447548460110874 (rel_diff=1.482e-03)

────────────────────────────────────────────────────────────────────────────────
Column: 'F2' - 4 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: 4.585494496743527e-44 != 4.595281592870508e-44 (rel_diff=2.134e-03)
  Line 78: 1.8417450089905839e-121 != 1.844859905756681e-121 (rel_diff=1.691e-03)
  Line 117: 1.1232288126754917e-25 != 1.124860498864991e-25 (rel_diff=1.453e-03)
  Line 327: 1.171471360456363e-11 != 1.173207838708808e-11 (rel_diff=1.482e-03)

────────────────────────────────────────────────────────────────────────────────
Column: 'C1' - 11 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: -0.0033911465369933 != -0.009996300378869 (rel_diff=1.948e+00)
  Line 78: -0.1137832561935472 != -0.119453422682953 (rel_diff=4.983e-02)
  Line 82: -0.1203673055076137 != -0.1211223916803213 (rel_diff=6.273e-03)
  Line 98: -0.1221311482578988 != -0.125 (rel_diff=2.349e-02)
  Line 105: -0.124484267356677 != -0.125 (rel_diff=4.143e-03)
  Line 106: -0.1126779959521606 != -0.112797464282956 (rel_diff=1.060e-03)
  Line 107: -0.1119495107572825 != -0.1134573372909615 (rel_diff=1.347e-02)
  Line 117: -0.0733905089878543 != -0.078122694624534 (rel_diff=6.448e-02)
  Line 279: -0.4356605212037681 != -0.4370790202635428 (rel_diff=3.256e-03)
  Line 311: -0.5384930707665228 != -0.5406742204840589 (rel_diff=4.050e-03)
  Line 327: -0.0051438489582229 != -0.0097351992862068 (rel_diff=8.926e-01)

────────────────────────────────────────────────────────────────────────────────
Column: 'C2' - 8 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: -1.086831325002835 != -1.09574893024599 (rel_diff=8.205e-03)
  Line 78: -1.2350360205395168 != -1.242602533478992 (rel_diff=6.127e-03)
  Line 98: -1.246174321814996 != -1.249999999999629 (rel_diff=3.070e-03)
  Line 107: -1.2325880753094638 != -1.234600970274428 (rel_diff=1.633e-03)
  Line 117: -1.1810082986588144 != -1.187349492745291 (rel_diff=5.369e-03)
  Line 279: -0.0043119169509244 != -0.0055482887859028 (rel_diff=2.867e-01)
  Line 311: -0.0012322408512295 != -0.0030787929590615 (rel_diff=1.499e+00)
  Line 327: -1.0891982943400942 != -1.095396544360563 (rel_diff=5.691e-03)

================================================================================
SUMMARY:
  Total mismatches found: 27
  Float threshold: 0.1%
================================================================================
PASSED
cattlefeed/py/test_shgo_with_constraints.py::test_optimization_ex 
x1=0.636,x2=0.000,x3=0.313,x4=0.052,f=29.894
g1=-0.000,g2=-0.000,g3=0.000
PASSED
constraint_dora/py/test_slsqp.py::test_constraint_dora 
============================================================
Brute force result: x1 = 0.89039 x2 = 0.45495 f(x*) = 1.52831
============================================================
Brute force result polar: r = 0.99989 theta = 0.47238 f(x*) = 1.52831
Brute force result polar Cartesian coordinate system: x1 = 0.89039 x2 = 0.45496 f(x*) = 1.52831
============================================================
============================================================
CONSTRAINED OPTIMIZATION RESULTS
============================================================
Success: True
Message: Optimization terminated successfully

Constraint value: -0.000000 (should be >= 0)
Constraint satisfied: True

Optimal solution: x1 = 0.894429 x2 = 0.447211 f(x*) = 1.527864

Expected result (analytical solution):
  2/√5 = 0.894427, 1/√5 = 0.447214
  (√5 - 1)² = 1.527864
x1 error    = 0.000154%
x2 error    = -0.000614%
f(x*) error = -0.000001%
============================================================
PASSED
deap/py/test_easimple.py::test_easimple 
======================================================================
GENETIC ALGORITHM OPTIMIZATION: Production Planning with DEAP
======================================================================

Genetic Algorithm Parameters:
  Population Size: 100
  Generations: 50
  Crossover Probability: 0.7
  Mutation Probability: 0.2

Running optimization...

gen	nevals	avg      	std     	min      	max      
0  	100   	-17,882.9	5,345.78	-34,403.7	-9,793.94
1  	78    	-13,706.9	3,803.25	-25,887.8	-9,229.37
2  	79    	-10,920.4	1,631.75	-17,608.7	-9,166.56
3  	69    	-10,021.5	918.106 	-15,691.2	-9,163.4 
4  	77    	-9,608.92	439.015 	-11,981  	-9,136.48
5  	79    	-9,110.25	1,758.07	-10,351.2	849.448  
6  	71    	-8,623.82	2,588.86	-10,257.4	849.448  
7  	75    	-7,798.73	3,611.4 	-10,205.1	849.448  
8  	71    	-7,482.8 	3,884.68	-9,826.9 	849.448  
9  	71    	-6,980.22	4,251.6 	-10,087.7	849.448  
10 	78    	-5,799.1 	4,829.73	-10,281.5	857.208  
11 	73    	-4,452.96	5,031.82	-9,646.03	860.065  
12 	72    	-3,548.58	4,991.52	-9,617.57	861.499  
13 	75    	-3,224.47	4,930.94	-9,711.18	861.499  
14 	64    	-2,110.89	4,570.22	-9,597.36	854.187  
15 	81    	-2,821.48	4,859.18	-9,744.03	854.289  
16 	68    	-1,702.78	4,393.7 	-9,930.19	858.274  
17 	82    	-1,377.2 	4,176.56	-9,543.52	859.407  
18 	72    	-1,071.61	3,961.87	-9,713.61	859.442  
19 	69    	-1,167.8 	4,032.96	-9,805.4 	859.442  
20 	73    	-1,365.25	4,174.43	-9,455.29	860.465  
21 	76    	-1,060.88	3,950.93	-9,571   	860.465  
22 	71    	-1,965.19	4,521.65	-9,557.11	860.505  
23 	72    	-1,060.48	3,946.77	-9,677.02	860.505  
24 	74    	-949.666 	3,854.22	-9,370.66	860.541  
25 	70    	-2,261   	4,648.15	-9,492.27	860.537  
26 	73    	-1,554.44	4,291.31	-9,331.48	860.547  
27 	82    	-1,970.13	4,537.99	-10,059  	860.571  
28 	83    	-1,550   	4,282.05	-9,505.49	860.59   
29 	77    	-1,467.99	4,258.49	-9,643.01	860.595  
30 	71    	-1,160.52	4,037.29	-9,548.6 	860.595  
31 	71    	-1,661.77	4,366.03	-9,516.29	860.595  
32 	84    	-2,257.76	4,651.06	-9,417.8 	860.595  
33 	73    	-1,258.37	4,102.59	-9,564.84	860.595  
34 	82    	-1,467.6 	4,256.54	-9,583.28	860.595  
35 	79    	-1,565.68	4,311.56	-9,890.08	860.585  
36 	73    	-1,051.86	3,941.49	-9,405.88	860.597  
37 	74    	-1,452.29	4,225.93	-9,493.4 	860.597  
38 	71    	-1,848.21	4,454.11	-9,502.58	860.597  
39 	80    	-2,861.49	4,854.62	-9,843.34	860.597  
40 	87    	-1,750.6 	4,401.63	-9,378.1 	860.585  
41 	82    	-2,758.03	4,823.83	-9,617.04	860.587  
42 	81    	-2,161.99	4,614.21	-9,791.68	860.589  
43 	66    	-1,048.29	3,937.38	-9,417.34	860.586  
44 	75    	-1,046.67	3,935.72	-9,568.58	860.592  
45 	87    	-2,555.23	4,758.67	-9,560.92	860.592  
46 	78    	-1,556.01	4,292.61	-9,575.45	860.592  
47 	74    	-1,453.36	4,229.98	-9,672.97	860.593  
48 	73    	-1,551.09	4,291.74	-9,380.59	860.594  
49 	78    	-1,553.38	4,292.55	-9,404.5 	860.594  
50 	67    	-1,751.95	4,405.98	-9,525.98	860.594  

======================================================================
OPTIMIZATION RESULTS
======================================================================

Best Profit: $861.50
Constraints Satisfied: True

----------------------------------------------------------------------
Optimal Production Quantities:
----------------------------------------------------------------------
Product  Quantity  Profit_per_Unit  Total_Profit
      A      7.79               30        233.75
      B      6.55               45        294.77
      C      2.19               50        109.30
      D      6.39               35        223.68

----------------------------------------------------------------------
Resource Utilization:
----------------------------------------------------------------------
     Resource  Used  Capacity  Utilization_%
  labor_hours 59.96       100          59.96
machine_hours 35.94        80          44.93
storage_space 15.16        50          30.31

----------------------------------------------------------------------
Constraint Verification:
----------------------------------------------------------------------
Total Weight: 59.96 (limit: 60)
Product B / Product A ratio: 84.07% (min: 30%)
All resource constraints: ✓ Satisfied
Minimum quantities: ✓ Satisfied

----------------------------------------------------------------------
Evolution Statistics:
----------------------------------------------------------------------
Generation 0 - Max Fitness: $-9793.94, Avg: $-17882.88
Generation 25 - Max Fitness: $860.54, Avg: $-2261.00
Generation 50 - Max Fitness: $860.59, Avg: $-1751.95

======================================================================
To use with your own CSV files:
  products_data = pd.read_csv('your_products.csv')
  resources_data = pd.read_csv('your_resources.csv')
  requirements_data = pd.read_csv('your_requirements.csv')
======================================================================
PASSED
eggholder/py/test_shgo.py::test_optimization_ex 
The first element of the sorted dataset:     512.0 404.00 -959.5797
Analytical solution [1]:                     512.0 404.23 -959.6407
Simplicial homology global optimization [2]: 512.0 404.23 -959.6407
Dual annealing [3]:                          512.0 404.23 -959.6407
Difference between SHGO and DA methods: -2.37e-14 %
[1] EGGHOLDER: https://www.sfu.ca/~ssurjano/egg.html
[2] SHGO: https://link.springer.com/article/10.1007/s10898-018-0645-y
[3] DA: https://www.jstatsoft.org/article/view/v060i06
[4] SCIPY: https://docs.scipy.org/doc/scipy/tutorial/optimize.html#global-optimization
PASSED
pyomo/py/test_bnh_csv.py::test_bnh_csv 
================================================================================
PARETO FRONT FROM CSV DATA (NO ANALYTICAL FUNCTIONS)
================================================================================

IMPORTANT: F1 and F2 are read ONLY from the DataFrame/CSV
No analytical functions are used for objectives!

================================================================================
RUNNING NSGA-II OPTIMIZATION
================================================================================

Loaded 10201 data points from CSV:
    X1    X2      F1       F2
0  0.0  0.00  0.0000  50.0000
1  0.0  0.03  0.0036  49.7009
2  0.0  0.06  0.0144  49.4036
3  0.0  0.09  0.0324  49.1081
4  0.0  0.12  0.0576  48.8144
5  0.0  0.15  0.0900  48.5225
6  0.0  0.18  0.1296  48.2324
7  0.0  0.21  0.1764  47.9441
8  0.0  0.24  0.2304  47.6576
9  0.0  0.27  0.2916  47.3729

================================================================================
CREATING INTERPOLATORS FROM CSV DATA
================================================================================

Optimization Settings:
  Objectives: F1 and F2 from CSV data (interpolated)
  Variables: X1 ∈ [0, 5], X2 ∈ [0, 3]
  Constraints:
    C1: (x1-5)² + x2² ≤ 25
    C2: (x1-8)² + (x2+3)² ≥ 7.7
  Population: 100
  Generations: 200

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |     33 |  0.000000E+00 |  0.1286804155 |             - |             -
     2 |      200 |     55 |  0.000000E+00 |  0.000000E+00 |  0.0094193678 |         ideal
     3 |      300 |     80 |  0.000000E+00 |  0.000000E+00 |  0.0133917184 |         ideal
     4 |      400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0049512504 |         ideal
     5 |      500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0346550020 |         nadir
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0042590207 |         ideal
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0264060472 |         nadir
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013496195 |             f
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029223176 |             f
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0602502458 |         nadir
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013677011 |             f
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024541785 |             f
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031088355 |             f
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030604978 |         nadir
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016427562 |             f
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026902471 |             f
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012212615 |             f
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024523804 |             f
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030806621 |             f
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020401815 |             f
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028333888 |             f
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017196336 |             f
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0034415200 |             f
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014990912 |             f
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023137096 |             f
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030067257 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0034426328 |         nadir
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016875754 |             f
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031067282 |             f
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019428838 |             f
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025044898 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015208656 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029258606 |             f
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016167522 |             f
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029026766 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015656362 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025949558 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011032953 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024195891 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0037000220 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014403648 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027243194 |             f
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013623455 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024324181 |             f
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033621633 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012273873 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018310109 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028007010 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018818039 |             f
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028060430 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012789861 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023066908 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0034180714 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014101766 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019330304 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027034354 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018232295 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032566788 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019233244 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028909781 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012305381 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023523093 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031773596 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021074091 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028248819 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014316787 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031257715 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015879427 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029896085 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016088139 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024748758 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029771257 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016266051 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022378814 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031184031 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014639635 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022377123 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027497942 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013219945 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024414493 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030531216 |             f
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016585290 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024319308 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029043670 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018893253 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027001371 |             f
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015681522 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022553954 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033454595 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022275939 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033037730 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014704339 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025391655 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011058446 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023136897 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028730885 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015509714 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031055775 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018220131 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033070451 |             f
   101 |    10100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016620725 |             f
   102 |    10200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030799063 |             f
   103 |    10300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015678476 |             f
   104 |    10400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025405400 |             f
   105 |    10500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020755373 |             f
   106 |    10600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032823534 |             f
   107 |    10700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010310932 |             f
   108 |    10800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018420800 |             f
   109 |    10900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030569520 |             f
   110 |    11000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012545302 |             f
   111 |    11100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022705865 |             f
   112 |    11200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030301488 |             f
   113 |    11300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014763647 |             f
   114 |    11400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023794887 |             f
   115 |    11500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027770762 |             f
   116 |    11600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018711824 |             f
   117 |    11700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029580051 |             f
   118 |    11800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015117153 |             f
   119 |    11900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022026519 |             f
   120 |    12000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029406710 |             f
   121 |    12100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017212980 |             f
   122 |    12200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025636759 |             f
   123 |    12300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014956680 |             f
   124 |    12400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028512845 |             f
   125 |    12500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013339962 |             f
   126 |    12600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026413103 |             f
   127 |    12700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014638281 |             f
   128 |    12800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029077235 |             f
   129 |    12900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011488020 |             f
   130 |    13000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020592693 |             f
   131 |    13100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028587589 |             f
   132 |    13200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010471028 |             f
   133 |    13300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025621935 |             f
   134 |    13400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013511135 |             f
   135 |    13500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021734668 |             f
   136 |    13600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030967327 |             f
   137 |    13700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014339744 |             f
   138 |    13800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022370097 |             f
   139 |    13900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031350592 |             f
   140 |    14000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013799313 |             f
   141 |    14100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024094746 |             f
   142 |    14200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0035562441 |             f
   143 |    14300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014658387 |             f
   144 |    14400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027588815 |             f
   145 |    14500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012150657 |             f
   146 |    14600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025219646 |             f
   147 |    14700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010186729 |             f
   148 |    14800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025620430 |             f
   149 |    14900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012961613 |             f
   150 |    15000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022772545 |             f
   151 |    15100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031359253 |             f
   152 |    15200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012463746 |             f
   153 |    15300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024226216 |             f
   154 |    15400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030005391 |             f
   155 |    15500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016712303 |             f
   156 |    15600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020544577 |             f
   157 |    15700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032833995 |             f
   158 |    15800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018285467 |             f
   159 |    15900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028244138 |             f
   160 |    16000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016832680 |             f
   161 |    16100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026202319 |             f
   162 |    16200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020147649 |             f
   163 |    16300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032114292 |             f
   164 |    16400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018578464 |             f
   165 |    16500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025752397 |             f
   166 |    16600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016368045 |             f
   167 |    16700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027822098 |             f
   168 |    16800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013437895 |             f
   169 |    16900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022574945 |             f
   170 |    17000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030589683 |             f
   171 |    17100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014711390 |             f
   172 |    17200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021399322 |             f
   173 |    17300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029197988 |             f
   174 |    17400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016159070 |             f
   175 |    17500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027599366 |             f
   176 |    17600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018863532 |             f
   177 |    17700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028668031 |             f
   178 |    17800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013986234 |             f
   179 |    17900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029365441 |             f
   180 |    18000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007667067 |             f
   181 |    18100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026868655 |             f
   182 |    18200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014725721 |             f
   183 |    18300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024614301 |             f
   184 |    18400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030987642 |             f
   185 |    18500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016168394 |             f
   186 |    18600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027080733 |             f
   187 |    18700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015221964 |             f
   188 |    18800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027041198 |             f
   189 |    18900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014591066 |             f
   190 |    19000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024765200 |             f
   191 |    19100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032100936 |             f
   192 |    19200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011812047 |             f
   193 |    19300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019472959 |             f
   194 |    19400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029805970 |             f
   195 |    19500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017631852 |             f
   196 |    19600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029766829 |             f
   197 |    19700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014623804 |             f
   198 |    19800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021882647 |             f
   199 |    19900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029620974 |             f
   200 |    20000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017576242 |             f

================================================================================
PARETO FRONT RESULTS
================================================================================

Number of Pareto optimal solutions: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (First 15):
--------------------------------------------------------------------------------
 Solution           X1           X2           F1        F2  All_constraints_OK
        1 5.000000e+00 3.000000e+00 1.360000e+02  4.000000                True
        2 1.218467e-08 2.046647e-10 2.461494e-09 50.000000                True
        3 8.711329e-02 1.309883e-01 1.017353e-01 47.844418                True
        4 8.492474e-01 8.269027e-01 5.620991e+00 34.643747                True
        5 1.657728e+00 1.791393e+00 2.383065e+01 21.466449                True
        6 2.198225e+00 2.300924e+00 4.050690e+01 15.135225                True
        7 5.854720e-01 6.804507e-01 3.226005e+00 38.147275                True
        8 2.145566e-01 1.816493e-01 3.183748e-01 46.117535                True
        9 7.311700e-01 6.804507e-01 3.993620e+00 36.882198                True
       10 5.063401e-01 6.804507e-01 2.879462e+00 38.851958                True
       11 1.491069e+00 1.272618e+00 1.537372e+01 26.206555                True
       12 1.172314e+00 9.889348e-01 9.411846e+00 30.740470                True
       13 6.187570e-03 1.958578e-10 1.237514e-03 49.938434                True
       14 4.639126e+00 2.993578e+00 1.219343e+02  4.156536                True
       15 7.960221e-01 1.029604e+00 6.776461e+00 33.437850                True

--------------------------------------------------------------------------------
Summary Statistics:
--------------------------------------------------------------------------------
X1 range: [0.000, 5.000]
X2 range: [0.000, 3.000]
F1 range: [0.000, 136.000]
F2 range: [4.000, 50.000]
Feasible solutions: 100 / 100

✓ Pareto front saved to 'pareto_front_results_tab.csv'

================================================================================
ANALYZING ORIGINAL CSV DATA
================================================================================

Original CSV data points:
  Total points: 10201
  Feasible points: 9500
  Infeasible points: 701

================================================================================
GENERATING VISUALIZATIONS
================================================================================
✓ Visualization saved as 'pareto_front_two_plots.png'

================================================================================
OPTIMIZATION COMPLETE
================================================================================

Generated files:
  1. pareto_front_results.csv - Pareto optimal solutions
  2. pareto_front_two_plots.png - Objective and variable space plots

✓ F1 and F2 were obtained ONLY from CSV data (no analytical functions)
✓ Interpolation used to estimate objectives for new X1, X2 combinations
✓ The Pareto front shows optimal trade-offs based purely on your data
================================================================================
PASSED
pyomo/py/test_bnh_csv_decision_tree.py::test_bnh_csv_decision_tree 
================================================================================
PARETO FRONT FROM CSV DATA (NO ANALYTICAL FUNCTIONS)
================================================================================

IMPORTANT: F1 and F2 are read ONLY from the DataFrame/CSV
No analytical functions are used for objectives!

================================================================================
RUNNING NSGA-II OPTIMIZATION
================================================================================

Loaded 10201 data points from CSV:
    X1    X2      F1       F2
0  0.0  0.00  0.0000  50.0000
1  0.0  0.03  0.0036  49.7009
2  0.0  0.06  0.0144  49.4036
3  0.0  0.09  0.0324  49.1081
4  0.0  0.12  0.0576  48.8144
5  0.0  0.15  0.0900  48.5225
6  0.0  0.18  0.1296  48.2324
7  0.0  0.21  0.1764  47.9441
8  0.0  0.24  0.2304  47.6576
9  0.0  0.27  0.2916  47.3729

================================================================================
CREATING INTERPOLATORS FROM CSV DATA
================================================================================
✓ Decision Tree Regressors trained from CSV data
  - Using 10201 data points for training
  - F1 model R² score: 0.9999
  - F2 model R² score: 0.9998
  - Max depth: 15, min samples split: 5, min samples leaf 2

Optimization Settings:
  Objectives: F1 and F2 from CSV data (interpolated)
  Variables: X1 ∈ [0, 5], X2 ∈ [0, 3]
  Constraints:
    C1: (x1-5)² + x2² ≤ 25
    C2: (x1-8)² + (x2+3)² ≥ 7.7
  Population: 100
  Generations: 200

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |     30 |  0.000000E+00 |  0.1286804155 |             - |             -
     2 |      200 |     44 |  0.000000E+00 |  0.000000E+00 |  0.0094302091 |         ideal
     3 |      300 |     62 |  0.000000E+00 |  0.000000E+00 |  0.0115410372 |         ideal
     4 |      400 |     89 |  0.000000E+00 |  0.000000E+00 |  0.0061365191 |         ideal
     5 |      500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0300713395 |         nadir
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025561371 |             f
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015821736 |             f
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0297960831 |         nadir
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0108426833 |         nadir
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006788078 |             f
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0307062109 |         nadir
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014237947 |             f
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0108341875 |         nadir
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0150329387 |         nadir
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010560022 |             f
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019146881 |             f
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023726160 |             f
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030623037 |             f
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010578016 |             f
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016716443 |             f
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021191422 |             f
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023290155 |             f
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111668636 |         nadir
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010925231 |             f
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016465655 |             f
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021847536 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027851710 |             f
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006823772 |             f
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011927399 |             f
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017389549 |             f
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018724726 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021919856 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023709962 |             f
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024149542 |             f
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024035939 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022656496 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026557657 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009055364 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014577361 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018502008 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024833898 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027665856 |             f
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007471782 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012179709 |             f
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019106063 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021880251 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024661914 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028030292 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009365372 |             f
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015955219 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017193171 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019994100 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021971399 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024016195 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025639223 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004218840 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011111038 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016110061 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016471329 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022822365 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025625011 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008594448 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018278516 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022253151 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029859610 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012168281 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016950571 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019811428 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022772639 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023808916 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025392180 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014131601 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018904458 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023862001 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029757585 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0005680436 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012114554 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018563322 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024784091 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027363714 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009636381 |             f
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017950013 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024910679 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028316150 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006911416 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012981216 |             f
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016537096 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023898157 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024944587 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027214293 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007986760 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010235565 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018140861 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022474642 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024911912 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026784074 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010610107 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016484846 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016449763 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020142413 |             f
   101 |    10100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025197123 |             f
   102 |    10200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004739624 |             f
   103 |    10300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012848794 |             f
   104 |    10400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016134634 |             f
   105 |    10500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016769258 |             f
   106 |    10600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019495074 |             f
   107 |    10700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024710987 |             f
   108 |    10800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028336154 |             f
   109 |    10900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007508756 |             f
   110 |    11000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011630096 |             f
   111 |    11100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017813123 |             f
   112 |    11200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020449588 |             f
   113 |    11300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022858351 |             f
   114 |    11400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025476706 |             f
   115 |    11500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006637029 |             f
   116 |    11600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008507181 |             f
   117 |    11700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014203787 |             f
   118 |    11800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025757795 |             f
   119 |    11900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006784762 |             f
   120 |    12000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012305567 |             f
   121 |    12100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014054034 |             f
   122 |    12200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018129463 |             f
   123 |    12300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021187618 |             f
   124 |    12400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023458375 |             f
   125 |    12500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024770517 |             f
   126 |    12600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023647830 |             f
   127 |    12700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027742671 |             f
   128 |    12800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008684354 |             f
   129 |    12900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015718511 |             f
   130 |    13000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018028645 |             f
   131 |    13100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020828005 |             f
   132 |    13200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023065896 |             f
   133 |    13300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026442110 |             f
   134 |    13400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009565265 |             f
   135 |    13500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017484165 |             f
   136 |    13600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024778599 |             f
   137 |    13700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025357670 |             f
   138 |    13800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007356919 |             f
   139 |    13900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010125807 |             f
   140 |    14000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014830162 |             f
   141 |    14100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018609696 |             f
   142 |    14200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021999137 |             f
   143 |    14300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022909521 |             f
   144 |    14400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024972844 |             f
   145 |    14500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023083013 |             f
   146 |    14600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023275379 |             f
   147 |    14700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027334576 |             f
   148 |    14800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008782173 |             f
   149 |    14900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012196709 |             f
   150 |    15000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013377808 |             f
   151 |    15100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017875654 |             f
   152 |    15200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021855626 |             f
   153 |    15300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023852722 |             f
   154 |    15400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027229499 |             f
   155 |    15500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007864234 |             f
   156 |    15600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011437087 |             f
   157 |    15700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016642774 |             f
   158 |    15800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020555117 |             f
   159 |    15900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023847449 |             f
   160 |    16000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027321414 |             f
   161 |    16100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017599639 |             f
   162 |    16200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023187083 |             f
   163 |    16300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027357618 |             f
   164 |    16400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011798045 |             f
   165 |    16500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016501768 |             f
   166 |    16600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022323287 |             f
   167 |    16700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028373689 |             f
   168 |    16800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006842937 |             f
   169 |    16900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009955446 |             f
   170 |    17000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016654359 |             f
   171 |    17100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022280324 |             f
   172 |    17200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026450557 |             f
   173 |    17300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004320553 |             f
   174 |    17400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013340689 |             f
   175 |    17500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020168706 |             f
   176 |    17600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024108760 |             f
   177 |    17700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026507340 |             f
   178 |    17800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007556377 |             f
   179 |    17900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017429982 |             f
   180 |    18000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021978117 |             f
   181 |    18100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026884756 |             f
   182 |    18200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008297853 |             f
   183 |    18300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013081424 |             f
   184 |    18400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017253512 |             f
   185 |    18500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017203848 |             f
   186 |    18600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018644606 |             f
   187 |    18700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019356770 |             f
   188 |    18800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022375102 |             f
   189 |    18900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023290506 |             f
   190 |    19000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024683832 |             f
   191 |    19100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028476087 |             f
   192 |    19200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009989903 |             f
   193 |    19300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019895200 |             f
   194 |    19400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022559882 |             f
   195 |    19500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026615906 |             f
   196 |    19600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0002334188 |             f
   197 |    19700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012037161 |             f
   198 |    19800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017147930 |             f
   199 |    19900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022623139 |             f
   200 |    20000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025946461 |             f

================================================================================
PARETO FRONT RESULTS
================================================================================

Number of Pareto optimal solutions: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (First 15):
--------------------------------------------------------------------------------
 Solution       X1       X2         F1       F2  All_constraints_OK
        1 0.081992 0.132386   0.038267 48.11870                True
        2 4.797486 2.994308 127.092600  4.01500                True
        3 4.820903 2.993248 127.092600  4.01500                True
        4 0.124493 0.108412   0.038267 48.11870                True
        5 3.060195 2.942259  70.828400  7.92400                True
        6 0.200072 0.180240   0.234800 46.17870                True
        7 4.496139 2.993707 115.932600  4.18375                True
        8 4.677925 2.993113 123.292600  4.07625                True
        9 0.323048 0.196272   0.508400 44.74820                True
       10 3.430052 2.931841  81.148400  6.72485                True
       11 1.202342 0.973858   9.567000 30.76220                True
       12 3.277230 2.808117  74.378000  7.49370                True
       13 2.212310 2.438640  43.276600 14.36915                True
       14 2.203604 2.580823  45.372400 13.77035                True
       15 0.772330 0.853866   4.828400 35.12000                True

--------------------------------------------------------------------------------
Summary Statistics:
--------------------------------------------------------------------------------
X1 range: [0.082, 4.821]
X2 range: [0.021, 3.000]
F1 range: [0.038, 127.093]
F2 range: [4.015, 48.119]
Feasible solutions: 100 / 100

✓ Pareto front saved to 'pareto_front_results_dt.csv'

================================================================================
ANALYZING ORIGINAL CSV DATA
================================================================================

Original CSV data points:
  Total points: 10201
  Feasible points: 9500
  Infeasible points: 701

================================================================================
GENERATING VISUALIZATIONS
================================================================================
✓ Visualization saved as 'pareto_front_two_plots_dt.png'

================================================================================
OPTIMIZATION COMPLETE
================================================================================

Generated files:
  1. pareto_front_results_dt.csv - Pareto optimal solutions
  2. pareto_front_two_plots_dt.png - Objective and variable space plots

✓ F1 and F2 were obtained ONLY from CSV data (no analytical functions)
✓ Interpolation used to estimate objectives for new X1, X2 combinations
✓ The Pareto front shows optimal trade-offs based purely on your data
================================================================================
PASSED
pyomo/py/test_bnh_csv_decision_tree_generated_constraints.py::test_bnh_csv_decision_tree_generated_constraints 
================================================================================
PARETO FRONT FROM CSV DATA (NO ANALYTICAL FUNCTIONS)
================================================================================

IMPORTANT: F1 and F2 are read ONLY from the DataFrame/CSV
No analytical functions are used for objectives!
================================================================================
GENERIC CONSTRAINT FUNCTION GENERATOR FROM JSON
================================================================================

# ============================================================================
# GENERATED CONSTRAINT FUNCTIONS FROM JSON ALPHA ATTRIBUTE
# ============================================================================
# Source: /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/pyomo/py/bnh.json
# Variables: X1, X2
# Alpha: (X1-5)*(X1-5)+X2*X2-25 and -(X1-8)*(X1-8)-(X2+3)*(X2+3)+7.7
# Number of constraints: 2
# ============================================================================

def constraint_C1(X1, X2):
    """
    C1: (X1-5)*(X1-5)+X2*X2-25
    Returns: True if constraint is satisfied, False otherwise
    """
    return (X1-5)*(X1-5)+X2*X2-25

def constraint_C2(X1, X2):
    """
    C2: -(X1-8)*(X1-8)-(X2+3)*(X2+3)+7.7
    Returns: True if constraint is satisfied, False otherwise
    """
    return -(X1-8)*(X1-8)-(X2+3)*(X2+3)+7.7


================================================================================
✓ Constraints successfully generated and saved to '/tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/pyomo/py/generated_constraints_claude.py'
================================================================================

================================================================================
RUNNING NSGA-II OPTIMIZATION
================================================================================

Loaded 10201 data points from CSV:
    X1    X2      F1       F2
0  0.0  0.00  0.0000  50.0000
1  0.0  0.03  0.0036  49.7009
2  0.0  0.06  0.0144  49.4036
3  0.0  0.09  0.0324  49.1081
4  0.0  0.12  0.0576  48.8144
5  0.0  0.15  0.0900  48.5225
6  0.0  0.18  0.1296  48.2324
7  0.0  0.21  0.1764  47.9441
8  0.0  0.24  0.2304  47.6576
9  0.0  0.27  0.2916  47.3729

================================================================================
CREATING MODEL FROM CSV DATA
================================================================================
✓ Decision Tree Regressors trained from CSV data
  - Using 10201 data points for training
  - F1 model R² score: 0.9999
  - F2 model R² score: 0.9998
  - Max depth: 15, min samples split: 5, min samples leaf 2

Optimization Settings:
  Objectives: F1 and F2 from CSV data (interpolated)
  Variables: X1 ∈ [0, 5], X2 ∈ [0, 3]
  Constraints:
    C1: (x1-5)² + x2² ≤ 25
    C2: (x1-8)² + (x2+3)² ≥ 7.7
  Population: 100
  Generations: 200

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |     30 |  0.000000E+00 |  0.1286804155 |             - |             -
     2 |      200 |     44 |  0.000000E+00 |  0.000000E+00 |  0.0094302091 |         ideal
     3 |      300 |     62 |  0.000000E+00 |  0.000000E+00 |  0.0115410372 |         ideal
     4 |      400 |     89 |  0.000000E+00 |  0.000000E+00 |  0.0061365191 |         ideal
     5 |      500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0300713395 |         nadir
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025561371 |             f
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015821736 |             f
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0297960831 |         nadir
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0108426833 |         nadir
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006788078 |             f
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0307062109 |         nadir
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014237947 |             f
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0108341875 |         nadir
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0150329387 |         nadir
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010560022 |             f
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019146881 |             f
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023726160 |             f
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030623037 |             f
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010578016 |             f
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016716443 |             f
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021191422 |             f
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023290155 |             f
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111668636 |         nadir
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010925231 |             f
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016465655 |             f
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021847536 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027851710 |             f
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006823772 |             f
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011927399 |             f
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017389549 |             f
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018724726 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021919856 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023709962 |             f
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024149542 |             f
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024035939 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022656496 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026557657 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009055364 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014577361 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018502008 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024833898 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027665856 |             f
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007471782 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012179709 |             f
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019106063 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021880251 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024661914 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028030292 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009365372 |             f
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015955219 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017193171 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019994100 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021971399 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024016195 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025639223 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004218840 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011111038 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016110061 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016471329 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022822365 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025625011 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008594448 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018278516 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022253151 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029859610 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012168281 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016950571 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019811428 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022772639 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023808916 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025392180 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014131601 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018904458 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023862001 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029757585 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0005680436 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012114554 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018563322 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024784091 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027363714 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009636381 |             f
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017950013 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024910679 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028316150 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006911416 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012981216 |             f
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016537096 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023898157 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024944587 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027214293 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007986760 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010235565 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018140861 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022474642 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024911912 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026784074 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010610107 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016484846 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016449763 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020142413 |             f
   101 |    10100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025197123 |             f
   102 |    10200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004739624 |             f
   103 |    10300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012848794 |             f
   104 |    10400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016134634 |             f
   105 |    10500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016769258 |             f
   106 |    10600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019495074 |             f
   107 |    10700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024710987 |             f
   108 |    10800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028336154 |             f
   109 |    10900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007508756 |             f
   110 |    11000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011630096 |             f
   111 |    11100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017813123 |             f
   112 |    11200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020449588 |             f
   113 |    11300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022858351 |             f
   114 |    11400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025476706 |             f
   115 |    11500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006637029 |             f
   116 |    11600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008507181 |             f
   117 |    11700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014203787 |             f
   118 |    11800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025757795 |             f
   119 |    11900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006784762 |             f
   120 |    12000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012305567 |             f
   121 |    12100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014054034 |             f
   122 |    12200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018129463 |             f
   123 |    12300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021187618 |             f
   124 |    12400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023458375 |             f
   125 |    12500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024770517 |             f
   126 |    12600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023647830 |             f
   127 |    12700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027742671 |             f
   128 |    12800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008684354 |             f
   129 |    12900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015718511 |             f
   130 |    13000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018028645 |             f
   131 |    13100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020828005 |             f
   132 |    13200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023065896 |             f
   133 |    13300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026442110 |             f
   134 |    13400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009565265 |             f
   135 |    13500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017484165 |             f
   136 |    13600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024778599 |             f
   137 |    13700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025357670 |             f
   138 |    13800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007356919 |             f
   139 |    13900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010125807 |             f
   140 |    14000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014830162 |             f
   141 |    14100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018609696 |             f
   142 |    14200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021999137 |             f
   143 |    14300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022909521 |             f
   144 |    14400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024972844 |             f
   145 |    14500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023083013 |             f
   146 |    14600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023275379 |             f
   147 |    14700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027334576 |             f
   148 |    14800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008782173 |             f
   149 |    14900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012196709 |             f
   150 |    15000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013377808 |             f
   151 |    15100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017875654 |             f
   152 |    15200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021855626 |             f
   153 |    15300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023852722 |             f
   154 |    15400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027229499 |             f
   155 |    15500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007864234 |             f
   156 |    15600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011437087 |             f
   157 |    15700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016642774 |             f
   158 |    15800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020555117 |             f
   159 |    15900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023847449 |             f
   160 |    16000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027321414 |             f
   161 |    16100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017599639 |             f
   162 |    16200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023187083 |             f
   163 |    16300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027357618 |             f
   164 |    16400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011798045 |             f
   165 |    16500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016501768 |             f
   166 |    16600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022323287 |             f
   167 |    16700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028373689 |             f
   168 |    16800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006842937 |             f
   169 |    16900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009955446 |             f
   170 |    17000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016654359 |             f
   171 |    17100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022280324 |             f
   172 |    17200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026450557 |             f
   173 |    17300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004320553 |             f
   174 |    17400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013340689 |             f
   175 |    17500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020168706 |             f
   176 |    17600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024108760 |             f
   177 |    17700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026507340 |             f
   178 |    17800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007556377 |             f
   179 |    17900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017429982 |             f
   180 |    18000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021978117 |             f
   181 |    18100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026884756 |             f
   182 |    18200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008297853 |             f
   183 |    18300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013081424 |             f
   184 |    18400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017253512 |             f
   185 |    18500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017203848 |             f
   186 |    18600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018644606 |             f
   187 |    18700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019356770 |             f
   188 |    18800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022375102 |             f
   189 |    18900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023290506 |             f
   190 |    19000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024683832 |             f
   191 |    19100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028476087 |             f
   192 |    19200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009989903 |             f
   193 |    19300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019895200 |             f
   194 |    19400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022559882 |             f
   195 |    19500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026615906 |             f
   196 |    19600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0002334188 |             f
   197 |    19700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012037161 |             f
   198 |    19800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017147930 |             f
   199 |    19900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022623139 |             f
   200 |    20000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025946461 |             f

================================================================================
PARETO FRONT RESULTS
================================================================================

Number of Pareto optimal solutions: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (First 15):
--------------------------------------------------------------------------------
 Solution       X1       X2         F1       F2  All_constraints_OK
        1 0.081992 0.132386   0.038267 48.11870                True
        2 4.797486 2.994308 127.092600  4.01500                True
        3 4.820903 2.993248 127.092600  4.01500                True
        4 0.124493 0.108412   0.038267 48.11870                True
        5 3.060195 2.942259  70.828400  7.92400                True
        6 0.200072 0.180240   0.234800 46.17870                True
        7 4.496139 2.993707 115.932600  4.18375                True
        8 4.677925 2.993113 123.292600  4.07625                True
        9 0.323048 0.196272   0.508400 44.74820                True
       10 3.430052 2.931841  81.148400  6.72485                True
       11 1.202342 0.973858   9.567000 30.76220                True
       12 3.277230 2.808117  74.378000  7.49370                True
       13 2.212310 2.438640  43.276600 14.36915                True
       14 2.203604 2.580823  45.372400 13.77035                True
       15 0.772330 0.853866   4.828400 35.12000                True

--------------------------------------------------------------------------------
Summary Statistics:
--------------------------------------------------------------------------------
X1 range: [0.082, 4.821]
X2 range: [0.021, 3.000]
F1 range: [0.038, 127.093]
F2 range: [4.015, 48.119]
Feasible solutions: 100 / 100

✓ Pareto front saved to 'pareto_front_results_dt_gc.csv'

================================================================================
ANALYZING ORIGINAL CSV DATA
================================================================================

Original CSV data points:
  Total points: 10201
  Feasible points: 9500
  Infeasible points: 701

================================================================================
GENERATING VISUALIZATIONS
================================================================================
✓ Visualization saved as 'pareto_front_two_plots_dt_gc.png'

================================================================================
OPTIMIZATION COMPLETE
================================================================================

Generated files:
  1. pareto_front_results_dt_gc.csv - Pareto optimal solutions
  2. pareto_front_two_plots_dt_gc.png - Objective and variable space plots

✓ F1 and F2 were obtained ONLY from CSV data (no analytical functions)
✓ Interpolation used to estimate objectives for new X1, X2 combinations
✓ The Pareto front shows optimal trade-offs based purely on your data
================================================================================
PASSED
pyomo/py/test_bnh_models_comparison.py::test_bnh_models_comparison 
PASSED
pyomo/py/test_glpk.py::test_glpk 
============================================================
OPTIMIZATION PROBLEM: Production Planning
============================================================

Solving optimization problem...

============================================================
OPTIMIZATION RESULTS
============================================================

Solver Status: ok
Termination Condition: optimal

Optimal Objective Value (Max Profit): $870.00

------------------------------------------------------------
Production Quantities:
------------------------------------------------------------
Product  Quantity  Profit  Total_Profit
      A  5.000000      30         150.0
      B 10.666667      45         480.0
      C  2.000000      50         100.0
      D  4.000000      35         140.0

------------------------------------------------------------
Resource Utilization:
------------------------------------------------------------
     Resource      Used  Capacity  Utilization_%
  labor_hours 60.000000       100      60.000000
machine_hours 37.333333        80      46.666667
storage_space 15.433333        50      30.866667

------------------------------------------------------------
Constraint Verification:
------------------------------------------------------------
Total Weight: 60.00 (limit: 60)
Product B / Product A ratio: 213.33% (min: 30%)

============================================================
To use with your own CSV files:
  products_data = pd.read_csv('your_products.csv')
  resources_data = pd.read_csv('your_resources.csv')
  requirements_data = pd.read_csv('your_requirements.csv')
============================================================
PASSED
pyomo/py/test_nsga2_df.py::test_nsga2 
================================================================================
NSGA-II MULTI-OBJECTIVE OPTIMIZATION with Pymoo
================================================================================

Setting up NSGA-II algorithm...
Running NSGA-II optimization...
  Population Size: 100
  Generations: 100
  Objectives: 3 (Profit, Emissions, Quality)
  Constraints: 9

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |      1 |  0.000000E+00 |  1.405063E+02 |             - |             -
     2 |      200 |      1 |  0.000000E+00 |  5.871091E+01 |  0.000000E+00 |             f
     3 |      300 |      3 |  0.000000E+00 |  2.968131E+01 |  1.0000000000 |         ideal
     4 |      400 |     15 |  0.000000E+00 |  1.438383E+01 |  0.8663739049 |         ideal
     5 |      500 |     46 |  0.000000E+00 |  3.1895751178 |  0.0213632413 |         ideal
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0633385179 |         ideal
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.1675341084 |         ideal
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0971151477 |         nadir
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0173319830 |         ideal
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0074915452 |         ideal
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0831973273 |         ideal
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0434253341 |         ideal
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0177580761 |             f
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0119907993 |             f
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0292705914 |         ideal
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0130508966 |         ideal
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0365704115 |         ideal
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0064704558 |         nadir
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0099130916 |         ideal
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0044571499 |         ideal
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0127607467 |         ideal
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0079197994 |         ideal
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111494029 |             f
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0110656748 |             f
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0345565169 |         ideal
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0150300846 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0039750073 |         ideal
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0056948934 |         nadir
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0035248970 |         ideal
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0117494893 |         nadir
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0154072744 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0115748516 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0072931827 |         ideal
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0094452834 |         ideal
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0110703187 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0132012233 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0128807759 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111748974 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0134096917 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0085329387 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0121686302 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0062587191 |         nadir
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0107251897 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031146258 |         nadir
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0130376021 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0098589653 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0128495561 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0099560026 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0042195308 |         ideal
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0122619070 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0120548593 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0113353003 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0107073630 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0114403677 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0109685667 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0098758049 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0122097522 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0132137424 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0116049992 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0110956267 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0098383333 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0114612549 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0141964468 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0128061911 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0101193021 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0118854748 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0138770225 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0112770188 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0141636146 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0138517275 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0101291967 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0100351919 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0138315250 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0126369839 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0127177096 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0123849642 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0094258837 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0096888419 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0144853994 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0102574943 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0138367986 |         nadir
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0094277306 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0083293585 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0105128488 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0157448417 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0101479509 |         nadir
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0101823888 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0136433955 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0118741224 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111319930 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0100575579 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0118796126 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0089559400 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0095083385 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0102531959 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0084108972 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0105037920 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0090552867 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0134454852 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0386629614 |         nadir

================================================================================
OPTIMIZATION RESULTS - PARETO FRONT
================================================================================

Number of Pareto optimal solutions found: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (Sample - First 10):
--------------------------------------------------------------------------------
 Solution     Profit  Carbon_Emissions  Avg_Quality
        1 868.291985        154.036554    87.399855
        2 827.334450        144.739932    83.924250
        3 812.221362        150.430919    88.665592
        4 525.153169         93.029359    86.072987
        5 868.911019        154.033937    87.386937
        6 764.078671        138.987875    87.833355
        7 801.278654        148.021527    88.544441
        8 806.617586        141.526360    84.204452
        9 854.619795        151.506018    87.329735
       10 738.401730        135.407842    88.028520

================================================================================
DETAILED ANALYSIS OF KEY SOLUTIONS
================================================================================

--------------------------------------------------------------------------------
Maximum Profit Solution:
--------------------------------------------------------------------------------
Profit: $868.91
Carbon Emissions: 154.03 units
Average Quality: 87.39

Production Quantities:
Product  Quantity  Profit  Emissions
      A      5.38  161.54      26.92
      B     10.36  466.05      82.85
      C      2.02  101.08      20.22
      D      4.01  140.25      24.04

Resource Utilization:
  labor_hours: 59.94 / 100 (59.9%)
  machine_hours: 37.16 / 80 (46.5%)
  storage_space: 15.40 / 50 (30.8%)

--------------------------------------------------------------------------------
Minimum Emissions Solution:
--------------------------------------------------------------------------------
Profit: $525.15
Carbon Emissions: 93.03 units
Average Quality: 86.07

Production Quantities:
Product  Quantity  Profit  Emissions
      A       5.0  150.00      25.00
      B       3.0  135.05      24.01
      C       2.0  100.10      20.02
      D       4.0  140.00      24.00

Resource Utilization:
  labor_hours: 37.01 / 100 (37.0%)
  machine_hours: 22.01 / 80 (27.5%)
  storage_space: 9.30 / 50 (18.6%)

--------------------------------------------------------------------------------
Maximum Quality Solution:
--------------------------------------------------------------------------------
Profit: $812.22
Carbon Emissions: 150.43 units
Average Quality: 88.67

Production Quantities:
Product  Quantity  Profit  Emissions
      A      5.00  150.09      25.02
      B      3.01  135.45      24.08
      C      7.73  386.67      77.33
      D      4.00  140.01      24.00

Resource Utilization:
  labor_hours: 59.97 / 100 (60.0%)
  machine_hours: 36.36 / 80 (45.4%)
  storage_space: 15.04 / 50 (30.1%)

================================================================================
TRADE-OFF ANALYSIS
================================================================================

Profit Range: $525.15 - $868.91
Emissions Range: 93.03 - 154.04 units
Quality Range: 83.92 - 88.67

Key Insights:
  • Maximizing profit results in 154.03 emissions
  • Minimizing emissions reduces profit to $525.15
  • High quality solution achieves 88.67 quality score

================================================================================
RECOMMENDATION
================================================================================
Use the Pareto front to select a solution based on your priorities:
  • High profit with acceptable environmental impact
  • Balance between all three objectives
  • Environmentally friendly with reasonable profit

Export pareto_df to CSV for further analysis or visualization.
================================================================================

================================================================================
GENERATING PARETO FRONT VISUALIZATIONS
================================================================================

✓ Pareto front visualizations saved as 'pareto_front_analysis.png'
✓ Optimal solutions comparison saved as 'optimal_solutions_comparison.png'

Visualization complete! Two PNG files have been generated:
  1. pareto_front_analysis.png - Multi-view Pareto front analysis
  2. optimal_solutions_comparison.png - Detailed comparison of key solutions
================================================================================
PASSED
pyomo/py/test_nsga2_mixed.py::test_nsga2_mixed 
==========================================================
n_gen  |  n_eval  | n_nds  |      eps      |   indicator  
==========================================================
     1 |      200 |     60 |             - |             -
     2 |      400 |    108 |  0.0042049841 |             f
     3 |      600 |    175 |  0.0029542472 |         nadir
     4 |      800 |    200 |  0.0087476680 |         nadir
     5 |     1000 |    200 |  0.0005978725 |             f
     6 |     1200 |    200 |  0.0011942070 |             f
     7 |     1400 |    200 |  0.0014457724 |             f
     8 |     1600 |    200 |  0.0018036864 |             f
     9 |     1800 |    200 |  0.0020209954 |             f
    10 |     2000 |    200 |  0.0020984509 |             f
    11 |     2200 |    200 |  0.0021674494 |             f
    12 |     2400 |    200 |  0.0022506652 |             f
    13 |     2600 |    200 |  0.0022396065 |             f
    14 |     2800 |    200 |  0.0023025077 |             f
    15 |     3000 |    200 |  0.0022226597 |             f
    16 |     3200 |    200 |  0.0021475266 |             f
    17 |     3400 |    200 |  0.0023017864 |             f
    18 |     3600 |    200 |  0.0022372250 |             f
    19 |     3800 |    200 |  0.0022157910 |             f
    20 |     4000 |    200 |  0.0023951164 |             f
    21 |     4200 |    200 |  0.0023439753 |             f
    22 |     4400 |    200 |  0.0022568071 |             f
    23 |     4600 |    200 |  0.0022619142 |             f
    24 |     4800 |    200 |  0.0022679477 |             f
    25 |     5000 |    200 |  0.0022303665 |             f
    26 |     5200 |    200 |  0.0022049692 |             f
    27 |     5400 |    200 |  0.0023266123 |             f
    28 |     5600 |    200 |  0.0023976508 |             f
    29 |     5800 |    200 |  0.0024228821 |             f
    30 |     6000 |    200 |  0.0024951809 |             f
    31 |     6200 |    200 |  0.0024960498 |             f
    32 |     6400 |    200 |  0.0024461566 |             f
    33 |     6600 |    200 |  0.0025842198 |             f
    34 |     6800 |    200 |  0.0003501300 |             f
    35 |     7000 |    200 |  0.0005665319 |             f
    36 |     7200 |    200 |  0.0007819341 |             f
    37 |     7400 |    200 |  0.0009748304 |             f
    38 |     7600 |    200 |  0.0010447385 |             f
    39 |     7800 |    200 |  0.0011152461 |             f
    40 |     8000 |    200 |  0.0011728017 |             f
    41 |     8200 |    200 |  0.0012879648 |             f
    42 |     8400 |    200 |  0.0013597746 |             f
    43 |     8600 |    200 |  0.0014055846 |             f
    44 |     8800 |    200 |  0.0014009590 |             f
    45 |     9000 |    200 |  0.0015084065 |             f
    46 |     9200 |    200 |  0.0015804164 |             f
    47 |     9400 |    200 |  0.0015767791 |             f
    48 |     9600 |    200 |  0.0016105678 |             f
    49 |     9800 |    200 |  0.0016724679 |             f
    50 |    10000 |    200 |  0.0016729697 |             f
    51 |    10200 |    200 |  0.0016229921 |             f
    52 |    10400 |    200 |  0.0016910234 |             f
    53 |    10600 |    200 |  0.0016701758 |             f
    54 |    10800 |    200 |  0.0016345966 |             f
    55 |    11000 |    200 |  0.0015908178 |             f
    56 |    11200 |    200 |  0.0016554186 |             f
    57 |    11400 |    200 |  0.0017518635 |             f
    58 |    11600 |    200 |  0.0016406202 |             f
    59 |    11800 |    200 |  0.0015967262 |             f
    60 |    12000 |    200 |  0.0016065238 |             f
    61 |    12200 |    200 |  0.0016432181 |             f
    62 |    12400 |    200 |  0.0016357686 |             f
    63 |    12600 |    200 |  0.0016861599 |             f
    64 |    12800 |    200 |  0.0017355475 |             f
    65 |    13000 |    200 |  0.0016305885 |             f
    66 |    13200 |    200 |  0.0016743403 |             f
    67 |    13400 |    200 |  0.0016765833 |             f
    68 |    13600 |    200 |  0.0016128383 |             f
    69 |    13800 |    200 |  0.0016082325 |             f
    70 |    14000 |    200 |  0.0016651272 |             f
    71 |    14200 |    200 |  0.0016807775 |             f
    72 |    14400 |    200 |  0.0016838505 |             f
    73 |    14600 |    200 |  0.0015880587 |             f
    74 |    14800 |    200 |  0.0015638964 |             f
    75 |    15000 |    200 |  0.0015795946 |             f
    76 |    15200 |    200 |  0.0016116861 |             f
    77 |    15400 |    200 |  0.0015844354 |             f
    78 |    15600 |    200 |  0.0015963635 |             f
    79 |    15800 |    200 |  0.0016438301 |             f
    80 |    16000 |    200 |  0.0016744381 |             f
    81 |    16200 |    200 |  0.0016930107 |             f
    82 |    16400 |    200 |  0.0015552151 |             f
    83 |    16600 |    200 |  0.0015334283 |             f
    84 |    16800 |    200 |  0.0016070666 |             f
    85 |    17000 |    200 |  0.0016267605 |             f
    86 |    17200 |    200 |  0.0015474487 |             f
    87 |    17400 |    200 |  0.0015544784 |             f
    88 |    17600 |    200 |  0.0015712639 |             f
    89 |    17800 |    200 |  0.0016166501 |             f
    90 |    18000 |    200 |  0.0016232793 |             f
    91 |    18200 |    200 |  0.0016160165 |             f
    92 |    18400 |    200 |  0.0014671493 |             f
    93 |    18600 |    200 |  0.0014845553 |             f
    94 |    18800 |    200 |  0.0015083051 |             f
    95 |    19000 |    200 |  0.0015714766 |             f
    96 |    19200 |    200 |  0.0014922642 |             f
    97 |    19400 |    200 |  0.0015699272 |             f
    98 |    19600 |    200 |  0.0014983372 |             f
    99 |    19800 |    200 |  0.0015446214 |             f
   100 |    20000 |    200 |  0.0015616296 |             f
   101 |    20200 |    200 |  0.0015901148 |             f
   102 |    20400 |    200 |  0.0015701406 |             f
   103 |    20600 |    200 |  0.0015718828 |             f
   104 |    20800 |    200 |  0.0016102288 |             f
   105 |    21000 |    200 |  0.0016649448 |             f
   106 |    21200 |    200 |  0.0017322247 |             f
   107 |    21400 |    200 |  0.0017242285 |             f
   108 |    21600 |    200 |  0.0017255144 |             f
   109 |    21800 |    200 |  0.0016686634 |             f
   110 |    22000 |    200 |  0.0016261893 |             f
   111 |    22200 |    200 |  0.0016879618 |             f
   112 |    22400 |    200 |  0.0017458972 |             f
   113 |    22600 |    200 |  0.0016950529 |             f
   114 |    22800 |    200 |  0.0016304107 |             f
   115 |    23000 |    200 |  0.0015853259 |             f
   116 |    23200 |    200 |  0.0015467562 |             f
   117 |    23400 |    200 |  0.0016582081 |             f
   118 |    23600 |    200 |  0.0015998071 |             f
   119 |    23800 |    200 |  0.0016337372 |             f
   120 |    24000 |    200 |  0.0016661636 |             f
   121 |    24200 |    200 |  0.0016839355 |             f
   122 |    24400 |    200 |  0.0016883097 |             f
   123 |    24600 |    200 |  0.0017053530 |             f
   124 |    24800 |    200 |  0.0016743775 |             f
   125 |    25000 |    200 |  0.0016190857 |             f
   126 |    25200 |    200 |  0.0016342496 |             f
   127 |    25400 |    200 |  0.0016560330 |             f
   128 |    25600 |    200 |  0.0015755668 |             f
   129 |    25800 |    200 |  0.0016078766 |             f
   130 |    26000 |    200 |  0.0016369152 |             f
   131 |    26200 |    200 |  0.0015820111 |             f
   132 |    26400 |    200 |  0.0016002532 |             f
   133 |    26600 |    200 |  0.0015789140 |             f
   134 |    26800 |    200 |  0.0015913541 |             f
   135 |    27000 |    200 |  0.0015412312 |             f
   136 |    27200 |    200 |  0.0015403721 |             f
   137 |    27400 |    200 |  0.0015369193 |             f
   138 |    27600 |    200 |  0.0015520522 |             f
   139 |    27800 |    200 |  0.0015514826 |             f
   140 |    28000 |    200 |  0.0015896001 |             f
   141 |    28200 |    200 |  0.0016216211 |             f
   142 |    28400 |    200 |  0.0015880663 |             f
   143 |    28600 |    200 |  0.0016167843 |             f
   144 |    28800 |    200 |  0.0016461931 |             f
   145 |    29000 |    200 |  0.0016400516 |             f
   146 |    29200 |    200 |  0.0016625656 |             f
   147 |    29400 |    200 |  0.0015879008 |             f
   148 |    29600 |    200 |  0.0015960134 |             f
   149 |    29800 |    200 |  0.0016064131 |             f
   150 |    30000 |    200 |  0.0016119688 |             f
   151 |    30200 |    200 |  0.0016139802 |             f
   152 |    30400 |    200 |  0.0015988627 |             f
   153 |    30600 |    200 |  0.0016119974 |             f
   154 |    30800 |    200 |  0.0017040823 |             f
   155 |    31000 |    200 |  0.0016403129 |             f
   156 |    31200 |    200 |  0.0016583403 |             f
   157 |    31400 |    200 |  0.0016923945 |             f
   158 |    31600 |    200 |  0.0016461758 |             f
   159 |    31800 |    200 |  0.0015933194 |             f
   160 |    32000 |    200 |  0.0015336003 |             f
   161 |    32200 |    200 |  0.0014858073 |             f
   162 |    32400 |    200 |  0.0014486090 |             f
   163 |    32600 |    200 |  0.0015538812 |             f
   164 |    32800 |    200 |  0.0015779931 |             f
   165 |    33000 |    200 |  0.0015672054 |             f
   166 |    33200 |    200 |  0.0015934514 |             f
   167 |    33400 |    200 |  0.0015553133 |             f
   168 |    33600 |    200 |  0.0015700553 |             f
   169 |    33800 |    200 |  0.0015326715 |             f
   170 |    34000 |    200 |  0.0014760015 |             f
   171 |    34200 |    200 |  0.0014416320 |             f
   172 |    34400 |    200 |  0.0015117406 |             f
   173 |    34600 |    200 |  0.0014733734 |             f
   174 |    34800 |    200 |  0.0014848592 |             f
   175 |    35000 |    200 |  0.0014908640 |             f
   176 |    35200 |    200 |  0.0014814745 |             f
   177 |    35400 |    200 |  0.0015161258 |             f
   178 |    35600 |    200 |  0.0014810795 |             f
   179 |    35800 |    200 |  0.0014323076 |             f
   180 |    36000 |    200 |  0.0014887668 |             f
   181 |    36200 |    200 |  0.0015353075 |             f
   182 |    36400 |    200 |  0.0016036803 |             f
   183 |    36600 |    200 |  0.0016489457 |             f
   184 |    36800 |    200 |  0.0016350151 |             f
   185 |    37000 |    200 |  0.0016063964 |             f
   186 |    37200 |    200 |  0.0016832150 |             f
   187 |    37400 |    200 |  0.0017379206 |             f
   188 |    37600 |    200 |  0.0016713278 |             f
   189 |    37800 |    200 |  0.0015874822 |             f
   190 |    38000 |    200 |  0.0015932995 |             f
   191 |    38200 |    200 |  0.0014546722 |             f
   192 |    38400 |    200 |  0.0015887585 |             f
   193 |    38600 |    200 |  0.0015215466 |             f
   194 |    38800 |    200 |  0.0015423558 |             f
   195 |    39000 |    200 |  0.0015215547 |             f
   196 |    39200 |    200 |  0.0016065630 |             f
   197 |    39400 |    200 |  0.0015456258 |             f
   198 |    39600 |    200 |  0.0016135564 |             f
   199 |    39800 |    200 |  0.0016729092 |             f
   200 |    40000 |    200 |  0.0016503253 |             f
   201 |    40200 |    200 |  0.0016615084 |             f
   202 |    40400 |    200 |  0.0017061426 |             f
   203 |    40600 |    200 |  0.0016682311 |             f
   204 |    40800 |    200 |  0.0017029948 |             f
   205 |    41000 |    200 |  0.0016533659 |             f
   206 |    41200 |    200 |  0.0016700925 |             f
   207 |    41400 |    200 |  0.0017516611 |             f
   208 |    41600 |    200 |  0.0017328575 |             f
   209 |    41800 |    200 |  0.0016867957 |             f
   210 |    42000 |    200 |  0.0016404901 |             f
   211 |    42200 |    200 |  0.0015964862 |             f
   212 |    42400 |    200 |  0.0015787529 |             f
   213 |    42600 |    200 |  0.0016260093 |             f
   214 |    42800 |    200 |  0.0016305090 |             f
   215 |    43000 |    200 |  0.0016469764 |             f
   216 |    43200 |    200 |  0.0016540204 |             f
   217 |    43400 |    200 |  0.0015191264 |             f
   218 |    43600 |    200 |  0.0016039113 |             f
   219 |    43800 |    200 |  0.0015327281 |             f
   220 |    44000 |    200 |  0.0015924137 |             f
   221 |    44200 |    200 |  0.0016536925 |             f
   222 |    44400 |    200 |  0.0016173207 |             f
   223 |    44600 |    200 |  0.0016283930 |             f
   224 |    44800 |    200 |  0.0016040633 |             f
   225 |    45000 |    200 |  0.0015763778 |             f
   226 |    45200 |    200 |  0.0015685508 |             f
   227 |    45400 |    200 |  0.0015880584 |             f
   228 |    45600 |    200 |  0.0015280517 |             f
   229 |    45800 |    200 |  0.0015101177 |             f
   230 |    46000 |    200 |  0.0015657568 |             f
   231 |    46200 |    200 |  0.0015099021 |             f
   232 |    46400 |    200 |  0.0015836673 |             f
   233 |    46600 |    200 |  0.0016308552 |             f
   234 |    46800 |    200 |  0.0016238428 |             f
   235 |    47000 |    200 |  0.0016398258 |             f
   236 |    47200 |    200 |  0.0015420645 |             f
   237 |    47400 |    200 |  0.0015592705 |             f
   238 |    47600 |    200 |  0.0015430983 |             f
   239 |    47800 |    200 |  0.0015940483 |             f
   240 |    48000 |    200 |  0.0014828229 |             f
   241 |    48200 |    200 |  0.0015821291 |             f
   242 |    48400 |    200 |  0.0015635239 |             f
   243 |    48600 |    200 |  0.0015491296 |             f
   244 |    48800 |    200 |  0.0015602531 |             f
   245 |    49000 |    200 |  0.0015443552 |             f
   246 |    49200 |    200 |  0.0015273844 |             f
   247 |    49400 |    200 |  0.0014763486 |             f
   248 |    49600 |    200 |  0.0015326284 |             f
   249 |    49800 |    200 |  0.0015546954 |             f
   250 |    50000 |    200 |  0.0015545825 |             f

======================================================================
OPTIMIZATION RESULTS
======================================================================

Total Pareto front solutions: 200

======================================================================
SOLUTIONS BY CATEGORY
======================================================================

Category A: 76 solutions
  X1= 0.000  →  F1=  2.000, F2= 40.000
  X1= 0.100  →  F1=  2.003, F2= 39.749
  X1= 0.264  →  F1=  2.021, F2= 39.339
  X1= 0.358  →  F1=  2.039, F2= 39.104
  X1= 0.459  →  F1=  2.063, F2= 38.853
  X1= 0.488  →  F1=  2.071, F2= 38.780
  X1= 0.544  →  F1=  2.089, F2= 38.639
  X1= 0.599  →  F1=  2.108, F2= 38.503
  ... and 68 more solutions

Category B: 54 solutions
  X1= 3.000  →  F1=  5.000, F2= 29.000
  X1= 3.062  →  F1=  5.001, F2= 28.876
  X1= 3.188  →  F1=  5.005, F2= 28.625
  X1= 3.324  →  F1=  5.016, F2= 28.351
  X1= 3.390  →  F1=  5.023, F2= 28.220
  X1= 3.501  →  F1=  5.038, F2= 27.997
  X1= 3.549  →  F1=  5.045, F2= 27.902
  X1= 3.648  →  F1=  5.063, F2= 27.704
  ... and 46 more solutions

Category C: 35 solutions
  X1= 7.000  →  F1=  7.000, F2= 19.500
  X1= 7.296  →  F1=  7.013, F2= 19.056
  X1= 7.507  →  F1=  7.039, F2= 18.739
  X1= 7.703  →  F1=  7.074, F2= 18.445
  X1= 7.894  →  F1=  7.120, F2= 18.159
  X1= 7.980  →  F1=  7.144, F2= 18.030
  X1= 8.062  →  F1=  7.169, F2= 17.907
  X1= 8.158  →  F1=  7.201, F2= 17.763
  ... and 27 more solutions

Category D: 35 solutions
  X1=10.000  →  F1= 10.000, F2=  3.200
  X1= 9.934  →  F1= 10.066, F2=  3.122
  X1= 9.919  →  F1= 10.081, F2=  3.104
  X1= 9.865  →  F1= 10.135, F2=  3.044
  X1= 9.797  →  F1= 10.203, F2=  2.969
  X1= 9.744  →  F1= 10.256, F2=  2.912
  X1= 9.704  →  F1= 10.296, F2=  2.871
  X1= 9.659  →  F1= 10.341, F2=  2.826
  ... and 27 more solutions
✓ Optimal solutions comparison saved as /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/pyomo/py/nsga2_mixed_models_comparison.png

======================================================================
STATISTICAL SUMMARY
======================================================================

Category A: 76/200 solutions (38.0%)
  X1  → min:  0.00, max:  3.16, mean:  1.89, std:  0.87
  F1  → min:   2.00, max:   5.00, mean:   3.30
  F2  → min:  32.10, max:  40.00, mean:  35.28

Category B: 54/200 solutions (27.0%)
  X1  → min:  3.00, max:  6.65, mean:  5.01, std:  1.06
  F1  → min:   5.00, max:   7.00, mean:   5.78
  F2  → min:  21.70, max:  29.00, mean:  24.98

Category C: 35/200 solutions (17.5%)
  X1  → min:  7.00, max: 10.00, mean:  8.86, std:  0.80
  F1  → min:   7.00, max:   8.35, mean:   7.62
  F2  → min:  15.00, max:  19.50, mean:  16.71

Category D: 35/200 solutions (17.5%)
  X1  → min:  8.00, max: 10.00, mean:  9.06, std:  0.57
  F1  → min:  10.00, max:  12.00, mean:  10.94
  F2  → min:   2.00, max:   3.20, mean:   2.44

======================================================================
PROBLEM DESIGN - HOW CATEGORIES COMPETE
======================================================================
Category A: Dominates LEFT side (low F1, high F2)
Category B: Competitive in MID-LEFT region
Category C: Competitive in MID-RIGHT region
Category D: Dominates RIGHT side (high F1, low F2)

Each category is non-dominated in its specialized region!
PASSED
pyomo/py/test_nsga3_mixed.py::test_nsga3_mixed 
Number of reference directions: 496
WARNING: pop_size=300 is less than the number of reference directions ref_dirs=496.
This might cause unwanted behavior of the algorithm. 
Please make sure pop_size is equal or larger than the number of reference directions. 
==========================================================
n_gen  |  n_eval  | n_nds  |      eps      |   indicator  
==========================================================
     1 |      300 |     39 |             - |             -
     2 |      600 |     44 |  0.0367877232 |         nadir
     3 |      900 |     44 |  0.0023070974 |             f
     4 |     1200 |     45 |  0.0035211125 |             f
     5 |     1500 |     45 |  0.0002588012 |             f
     6 |     1800 |     45 |  0.0045527970 |         nadir
     7 |     2100 |     45 |  0.0010665891 |             f
     8 |     2400 |     45 |  0.0004567215 |             f
     9 |     2700 |     45 |  0.0006302142 |             f
    10 |     3000 |     45 |  0.0008078675 |             f
    11 |     3300 |     45 |  0.0009062124 |             f
    12 |     3600 |     45 |  0.0010105192 |             f
    13 |     3900 |     45 |  0.0011465905 |             f
    14 |     4200 |     46 |  0.0014756638 |             f
    15 |     4500 |     46 |  0.0015875214 |             f
    16 |     4800 |     46 |  0.0015736692 |             f
    17 |     5100 |     46 |  0.0014561735 |             f
    18 |     5400 |     46 |  0.0014205093 |             f
    19 |     5700 |     46 |  0.0013584048 |             f
    20 |     6000 |     46 |  0.0016013582 |             f
    21 |     6300 |     46 |  0.0016759459 |             f
    22 |     6600 |     46 |  0.0016560576 |             f
    23 |     6900 |     46 |  0.0015481683 |             f
    24 |     7200 |     46 |  0.0016176573 |             f
    25 |     7500 |     46 |  0.0019166319 |             f
    26 |     7800 |     46 |  0.0018274352 |             f
    27 |     8100 |     46 |  0.0015496038 |             f
    28 |     8400 |     46 |  0.0015530363 |             f
    29 |     8700 |     46 |  0.0015629162 |             f
    30 |     9000 |     46 |  0.0015252842 |             f
    31 |     9300 |     46 |  0.0014853959 |             f
    32 |     9600 |     46 |  0.0014853959 |             f
    33 |     9900 |     46 |  0.0015135522 |             f
    34 |    10200 |     46 |  0.0016201857 |             f
    35 |    10500 |     46 |  0.0016700199 |             f
    36 |    10800 |     46 |  0.0016692338 |             f
    37 |    11100 |     46 |  0.0016343289 |             f
    38 |    11400 |     46 |  0.0016316568 |             f
    39 |    11700 |     46 |  0.0016500208 |             f
    40 |    12000 |     46 |  0.0015665913 |             f
    41 |    12300 |     46 |  0.0022594546 |             f
    42 |    12600 |     46 |  0.0023660138 |             f
    43 |    12900 |     46 |  0.0016301250 |             f
    44 |    13200 |     46 |  0.0017648354 |             f
    45 |    13500 |     46 |  0.0021761102 |             f
    46 |    13800 |     46 |  0.0017023525 |             f
    47 |    14100 |     46 |  0.0016610817 |             f
    48 |    14400 |     46 |  0.0015453881 |             f
    49 |    14700 |     46 |  0.0014737412 |             f
    50 |    15000 |     46 |  0.0015200052 |             f
    51 |    15300 |     46 |  0.0015156335 |             f
    52 |    15600 |     46 |  0.0015324574 |             f
    53 |    15900 |     46 |  0.0015246837 |             f
    54 |    16200 |     46 |  0.0016105324 |             f
    55 |    16500 |     46 |  0.0015776779 |             f
    56 |    16800 |     46 |  0.0015809102 |             f
    57 |    17100 |     46 |  0.0015809913 |             f
    58 |    17400 |     46 |  0.0015605588 |             f
    59 |    17700 |     46 |  0.0017063410 |             f
    60 |    18000 |     46 |  0.0017272036 |             f
    61 |    18300 |     46 |  0.0016520806 |             f
    62 |    18600 |     46 |  0.0016548055 |             f
    63 |    18900 |     46 |  0.0016773627 |             f
    64 |    19200 |     46 |  0.0016092955 |             f
    65 |    19500 |     46 |  0.0016168405 |             f
    66 |    19800 |     46 |  0.0015565709 |             f
    67 |    20100 |     46 |  0.0016979611 |             f
    68 |    20400 |     46 |  0.0016399444 |             f
    69 |    20700 |     46 |  0.0018661643 |             f
    70 |    21000 |     46 |  0.0016330749 |             f
    71 |    21300 |     46 |  0.0016494453 |             f
    72 |    21600 |     46 |  0.0015073257 |             f
    73 |    21900 |     46 |  0.0014400363 |             f
    74 |    22200 |     46 |  0.0014930726 |             f
    75 |    22500 |     46 |  0.0014494946 |             f
    76 |    22800 |     46 |  0.0014338615 |             f
    77 |    23100 |     46 |  0.0014808133 |             f
    78 |    23400 |     46 |  0.0014629162 |             f
    79 |    23700 |     46 |  0.0014947193 |             f
    80 |    24000 |     46 |  0.0014340734 |             f
    81 |    24300 |     46 |  0.0014340734 |             f
    82 |    24600 |     46 |  0.0025364323 |             f
    83 |    24900 |     46 |  0.0012447383 |             f
    84 |    25200 |     46 |  0.0012328370 |             f
    85 |    25500 |     46 |  0.0010175412 |             f
    86 |    25800 |     46 |  0.0010701498 |             f
    87 |    26100 |     46 |  0.0012902771 |             f
    88 |    26400 |     46 |  0.0012859992 |             f
    89 |    26700 |     46 |  0.0008316357 |             f
    90 |    27000 |     46 |  0.0012158959 |             f
    91 |    27300 |     46 |  0.0012753951 |             f
    92 |    27600 |     46 |  0.0012968897 |             f
    93 |    27900 |     46 |  0.0010608956 |             f
    94 |    28200 |     46 |  0.0012640060 |             f
    95 |    28500 |     46 |  0.0012852629 |             f
    96 |    28800 |     46 |  0.0013147510 |             f
    97 |    29100 |     46 |  0.0010473550 |             f
    98 |    29400 |     46 |  0.0005656195 |             f
    99 |    29700 |     46 |  0.0010579516 |             f
   100 |    30000 |     46 |  0.0011782627 |             f
   101 |    30300 |     46 |  0.0011687199 |             f
   102 |    30600 |     46 |  0.0011164488 |             f
   103 |    30900 |     46 |  0.0011210855 |             f
   104 |    31200 |     46 |  0.0007926707 |             f
   105 |    31500 |     46 |  0.0010683340 |             f
   106 |    31800 |     46 |  0.0012095387 |             f
   107 |    32100 |     46 |  0.0011368988 |             f
   108 |    32400 |     46 |  0.0011422944 |             f
   109 |    32700 |     46 |  0.0011434130 |             f
   110 |    33000 |     46 |  0.0011434130 |             f
   111 |    33300 |     46 |  0.0011128038 |             f
   112 |    33600 |     46 |  0.0011128038 |             f
   113 |    33900 |     46 |  0.0011226669 |             f
   114 |    34200 |     46 |  0.0010345609 |             f
   115 |    34500 |     46 |  0.0006264288 |             f
   116 |    34800 |     46 |  0.0006725700 |             f
   117 |    35100 |     46 |  0.0009520528 |             f
   118 |    35400 |     46 |  0.0006200654 |             f
   119 |    35700 |     46 |  0.0005550373 |             f
   120 |    36000 |     46 |  0.0008920785 |             f
   121 |    36300 |     46 |  0.0009064358 |             f
   122 |    36600 |     46 |  0.0008956362 |             f
   123 |    36900 |     46 |  0.0008973969 |             f
   124 |    37200 |     46 |  0.0008825462 |             f
   125 |    37500 |     46 |  0.0008902132 |             f
   126 |    37800 |     46 |  0.0009480907 |             f
   127 |    38100 |     46 |  0.0013382278 |             f
   128 |    38400 |     46 |  0.0012578132 |             f
   129 |    38700 |     46 |  0.0014125891 |             f
   130 |    39000 |     46 |  0.0013939285 |             f
   131 |    39300 |     46 |  0.0014015469 |             f
   132 |    39600 |     46 |  0.0013654022 |             f
   133 |    39900 |     46 |  0.0013781044 |             f
   134 |    40200 |     46 |  0.0013969025 |             f
   135 |    40500 |     46 |  0.0013646166 |             f
   136 |    40800 |     46 |  0.0012570711 |             f
   137 |    41100 |     46 |  0.0012901289 |             f
   138 |    41400 |     46 |  0.0012519292 |             f
   139 |    41700 |     46 |  0.0012834063 |             f
   140 |    42000 |     46 |  0.0006647590 |             f
   141 |    42300 |     46 |  0.0011822516 |             f
   142 |    42600 |     46 |  0.0005390969 |             f
   143 |    42900 |     46 |  0.0011319068 |             f
   144 |    43200 |     46 |  0.0011636616 |             f
   145 |    43500 |     46 |  0.0012064205 |             f
   146 |    43800 |     46 |  0.0011716834 |             f
   147 |    44100 |     46 |  0.0010626471 |             f
   148 |    44400 |     46 |  0.0014415253 |             f
   149 |    44700 |     46 |  0.0014420687 |             f
   150 |    45000 |     46 |  0.0013730785 |             f
   151 |    45300 |     46 |  0.0012711345 |             f
   152 |    45600 |     46 |  0.0012397543 |             f
   153 |    45900 |     46 |  0.0007766753 |             f
   154 |    46200 |     46 |  0.0009570425 |             f
   155 |    46500 |     46 |  0.0006843118 |             f
   156 |    46800 |     46 |  0.0006843118 |             f
   157 |    47100 |     46 |  0.0011511462 |             f
   158 |    47400 |     46 |  0.0011511462 |             f
   159 |    47700 |     46 |  0.0011324964 |             f
   160 |    48000 |     46 |  0.0010871108 |             f
   161 |    48300 |     46 |  0.0010489748 |             f
   162 |    48600 |     46 |  0.0010057935 |             f
   163 |    48900 |     46 |  0.0012065121 |             f
   164 |    49200 |     46 |  0.0012280625 |             f
   165 |    49500 |     46 |  0.0012026463 |             f
   166 |    49800 |     46 |  0.0012114676 |             f
   167 |    50100 |     46 |  0.0013449695 |             f
   168 |    50400 |     46 |  0.0010337396 |             f
   169 |    50700 |     46 |  0.0009926450 |             f
   170 |    51000 |     46 |  0.0009870506 |             f
   171 |    51300 |     46 |  0.0008035187 |             f
   172 |    51600 |     46 |  0.0010128048 |             f
   173 |    51900 |     46 |  0.0010143565 |             f
   174 |    52200 |     46 |  0.0010047561 |             f
   175 |    52500 |     46 |  0.0012381813 |             f
   176 |    52800 |     46 |  0.0012408373 |             f
   177 |    53100 |     46 |  0.0011785313 |             f
   178 |    53400 |     46 |  0.0007695675 |             f
   179 |    53700 |     46 |  0.0012478106 |             f
   180 |    54000 |     46 |  0.0011783255 |             f
   181 |    54300 |     46 |  0.0014339288 |             f
   182 |    54600 |     46 |  0.0010375418 |             f
   183 |    54900 |     46 |  0.0010397665 |             f
   184 |    55200 |     46 |  0.0010577322 |             f
   185 |    55500 |     46 |  0.0011056501 |             f
   186 |    55800 |     46 |  0.0011015840 |             f
   187 |    56100 |     46 |  0.0011099389 |             f
   188 |    56400 |     46 |  0.0011101310 |             f
   189 |    56700 |     46 |  0.0009737514 |             f
   190 |    57000 |     46 |  0.0010159831 |             f
   191 |    57300 |     46 |  0.0008680937 |             f
   192 |    57600 |     46 |  0.0008706271 |             f
   193 |    57900 |     46 |  0.0014384755 |             f
   194 |    58200 |     46 |  0.0014309141 |             f
   195 |    58500 |     46 |  0.0013884342 |             f
   196 |    58800 |     46 |  0.0010111501 |             f
   197 |    59100 |     46 |  0.0010111501 |             f
   198 |    59400 |     46 |  0.0009869646 |             f
   199 |    59700 |     46 |  0.0014952559 |             f
   200 |    60000 |     46 |  0.0014263084 |             f
   201 |    60300 |     46 |  0.0013621052 |             f
   202 |    60600 |     46 |  0.0013651931 |             f
   203 |    60900 |     46 |  0.0013310257 |             f
   204 |    61200 |     46 |  0.0012626045 |             f
   205 |    61500 |     46 |  0.0012693543 |             f
   206 |    61800 |     46 |  0.0011441628 |             f
   207 |    62100 |     46 |  0.0010207100 |             f
   208 |    62400 |     46 |  0.0009782996 |             f
   209 |    62700 |     46 |  0.0012633389 |             f
   210 |    63000 |     46 |  0.0013235404 |             f
   211 |    63300 |     46 |  0.0012633389 |             f
   212 |    63600 |     46 |  0.0013222494 |             f
   213 |    63900 |     46 |  0.0012650444 |             f
   214 |    64200 |     46 |  0.0013376445 |             f
   215 |    64500 |     46 |  0.0012856680 |             f
   216 |    64800 |     46 |  0.0013870067 |             f
   217 |    65100 |     46 |  0.0014378990 |             f
   218 |    65400 |     46 |  0.0013867401 |             f
   219 |    65700 |     46 |  0.0013141429 |             f
   220 |    66000 |     46 |  0.0008749459 |             f
   221 |    66300 |     46 |  0.0013190917 |             f
   222 |    66600 |     46 |  0.0009502098 |             f
   223 |    66900 |     46 |  0.0009846626 |             f
   224 |    67200 |     46 |  0.0011712286 |             f
   225 |    67500 |     46 |  0.0011542957 |             f
   226 |    67800 |     46 |  0.0011693778 |             f
   227 |    68100 |     46 |  0.0011404627 |             f
   228 |    68400 |     46 |  0.0011774367 |             f
   229 |    68700 |     46 |  0.0008625515 |             f
   230 |    69000 |     46 |  0.0007798842 |             f
   231 |    69300 |     46 |  0.0013864984 |             f
   232 |    69600 |     46 |  0.0013396409 |             f
   233 |    69900 |     46 |  0.0011004505 |             f
   234 |    70200 |     46 |  0.0013905947 |             f
   235 |    70500 |     46 |  0.0013521965 |             f
   236 |    70800 |     46 |  0.0013570311 |             f
   237 |    71100 |     46 |  0.0010781287 |             f
   238 |    71400 |     46 |  0.0013187361 |             f
   239 |    71700 |     46 |  0.0013421438 |             f
   240 |    72000 |     46 |  0.0012921592 |             f
   241 |    72300 |     46 |  0.0013091531 |             f
   242 |    72600 |     46 |  0.0013381189 |             f
   243 |    72900 |     46 |  0.0013253034 |             f
   244 |    73200 |     46 |  0.0013225646 |             f
   245 |    73500 |     46 |  0.0013225646 |             f
   246 |    73800 |     46 |  0.0013458922 |             f
   247 |    74100 |     46 |  0.0013134070 |             f
   248 |    74400 |     46 |  0.0014722060 |             f
   249 |    74700 |     46 |  0.0014722060 |             f
   250 |    75000 |     46 |  0.0014385116 |             f

======================================================================
OPTIMIZATION RESULTS
======================================================================

Total Pareto front solutions: 46

======================================================================
SOLUTIONS BY CATEGORY
======================================================================

Category A: 12 solutions
  X1= 0.000  →  F1=  2.000, F2= 32.500, F3= 40.000
  X1= 1.253  →  F1=  2.471, F2= 27.022, F3= 36.869
  X1= 2.313  →  F1=  3.605, F2= 23.610, F3= 34.218
  X1= 3.146  →  F1=  4.970, F2= 21.718, F3= 32.134
  X1= 3.750  →  F1=  6.219, F2= 20.781, F3= 30.625
  X1= 4.374  →  F1=  7.740, F2= 20.196, F3= 29.064
  X1= 4.759  →  F1=  8.793, F2= 20.029, F3= 28.103
  X1= 4.959  →  F1=  9.378, F2= 20.001, F3= 27.602
  ... and 4 more solutions

Category B: 17 solutions
  X1= 3.159  →  F1= 15.005, F2=  6.993, F3= 28.681
  X1= 2.552  →  F1= 15.040, F2=  5.606, F3= 29.895
  X1= 3.476  →  F1= 15.045, F2=  7.834, F3= 28.047
  X1= 3.689  →  F1= 15.095, F2=  8.444, F3= 27.622
  X1= 3.979  →  F1= 15.192, F2=  9.333, F3= 27.042
  X1= 4.178  →  F1= 15.278, F2=  9.984, F3= 26.643
  X1= 1.776  →  F1= 15.299, F2=  4.262, F3= 31.447
  X1= 4.450  →  F1= 15.420, F2= 10.920, F3= 26.101
  ... and 9 more solutions

Category C: 8 solutions
  X1= 7.634  →  F1= 18.548, F2= 18.801, F3= 22.399
  X1= 7.338  →  F1= 18.993, F2= 18.537, F3= 20.846
  X1= 6.787  →  F1= 19.820, F2= 18.186, F3= 18.120
  X1= 4.012  →  F1= 23.982, F2= 19.186, F3=  7.634
  X1= 3.230  →  F1= 25.155, F2= 20.301, F3=  5.652
  X1= 2.550  →  F1= 26.176, F2= 21.572, F3=  4.275
  X1= 1.425  →  F1= 27.862, F2= 24.278, F3=  2.711
  X1= 0.000  →  F1= 30.000, F2= 28.800, F3=  2.000

Category D: 9 solutions
  X1= 7.373  →  F1= 20.035, F2= 18.728, F3=  8.414
  X1= 6.511  →  F1= 20.060, F2= 20.281, F3=  6.891
  X1= 7.863  →  F1= 20.186, F2= 17.847, F3=  9.476
  X1= 8.098  →  F1= 20.301, F2= 17.424, F3= 10.038
  X1= 8.525  →  F1= 20.581, F2= 16.656, F3= 11.142
  X1= 5.243  →  F1= 20.772, F2= 22.563, F3=  5.463
  X1= 8.761  →  F1= 20.775, F2= 16.231, F3= 11.799
  X1= 9.150  →  F1= 21.156, F2= 15.530, F3= 12.958
  ... and 1 more solutions
✓ Optimal solutions comparison saved as /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/pyomo/py/nsga3_mixed_models_comparison.png
PASSED
pyomo/py/test_scip.py::test_scip 

--- Running with SCIP (Global Optimizer) ---
SCIP Solution: x=1.000038, y=0.999962
Objective: 0.000000

--- Running with IPOPT (Local Optimizer) from different starts ---
IPOPT (start 0.5, 0.5): x=1.000000, y=1.000000
Objective: 0.000000
IPOPT (start 0.1, 1.9): x=1.000000, y=1.000000
Objective: 0.000000


======================================================================
EXAMPLE 2: Multi-Modal Function (Multiple Local Optima)
Minimize: sin(5*x)*cos(5*y)/5 + (x-1)² + (y-1)²
======================================================================

--- Running with SCIP (Global Optimizer) ---
SCIP Solution: x=0.959588, y=1.181927
Objective: -0.150796

--- Running with IPOPT from different starting points ---
IPOPT (start center): x=0.959780, y=1.181882, obj=-0.150796
IPOPT (start bottom-left): x=0.959780, y=1.181882, obj=-0.150796
IPOPT (start top-right): x=0.959780, y=1.181882, obj=-0.150796
IPOPT (start near optimum): x=0.959780, y=1.181882, obj=-0.150796

Best IPOPT solution: obj=-0.150796
SCIP solution: obj=-0.150796
Difference: 0.000000


======================================================================
EXAMPLE 3: Rosenbrock Function
Minimize: (1-x)² + 100*(y-x²)²  subject to x²+y²<=4
======================================================================

--- Running with SCIP ---
SCIP Solution: x=1.000077, y=1.000136
Objective: 0.000000

--- Running with IPOPT from different starting points ---
IPOPT (start standard): x=1.000000, y=1.000000, obj=0.000000
IPOPT (start origin): x=1.000000, y=1.000000, obj=0.000000
IPOPT (start near boundary): x=1.000000, y=1.000000, obj=0.000000


Generating comprehensive comparison visualizations...

Visualization saved as 'scip_vs_ipopt_comparison.png'

======================================================================
COMPARISON COMPLETE!
======================================================================

Key Insight: SCIP guarantees global optimum, IPOPT is faster but
may find different solutions depending on starting point!
======================================================================
PASSED
pyomo/py/test_scip_nsga2.py::test_scip_nsga2 

======================================================================
MULTI-OBJECTIVE BIOPROCESS OPTIMIZATION
======================================================================
METHOD 1: Weighted Sum (Single Run)
======================================================================
Objectives: Maximize Yield AND Minimize Cost
======================================================================

Testing different weight combinations:
Weights (yield=0.8, cost=0.2):
  Glucose: 10.00, Temp: 29.90
  Yield: 9.999457525513202, Cost: 22.989583453949123
Weights (yield=0.5, cost=0.5):
  Glucose: 10.00, Temp: 29.58
  Yield: 9.99131922859124, Cost: 22.95833290122058
Weights (yield=0.2, cost=0.8):
  Glucose: 3.04, Temp: 28.38
  Yield: 3.0000000223801684, Cost: 8.917778601509964

======================================================================
METHOD 2: Epsilon-Constraint (Pareto Front Generation)
======================================================================

Generating Pareto front (varying cost constraint):

-------------------------------- live log call ---------------------------------
Loading a SolverResults object with a warning status into model.name="Epsilon_Constraint";
  - termination condition: infeasible
  - message from solver: Ipopt 3.14.19\x3a Converged to a locally infeasible point. Problem may be infeasible.
Loading a SolverResults object with a warning status into model.name="Epsilon_Constraint";
  - termination condition: infeasible
  - message from solver: Ipopt 3.14.19\x3a Converged to a locally infeasible point. Problem may be infeasible.
Loading a SolverResults object with a warning status into model.name="Epsilon_Constraint";
  - termination condition: infeasible
  - message from solver: Ipopt 3.14.19\x3a Converged to a locally infeasible point. Problem may be infeasible.
Loading a SolverResults object with a warning status into model.name="Epsilon_Constraint";
  - termination condition: infeasible
  - message from solver: Ipopt 3.14.19\x3a Converged to a locally infeasible point. Problem may be infeasible.
Loading a SolverResults object with a warning status into model.name="Epsilon_Constraint";
  - termination condition: infeasible
  - message from solver: Ipopt 3.14.19\x3a Converged to a locally infeasible point. Problem may be infeasible.
ε=9.5: Yield=3.2745847588235004, Cost=9.473684300168728
ε=10.4: Yield=3.717538789992287, Cost=10.368421151249251
ε=11.3: Yield=4.161404668664917, Cost=11.263158002314803
ε=12.2: Yield=4.605929208078503, Cost=12.157894853375758
ε=13.1: Yield=5.05094419008997, Cost=13.052631704434383
ε=13.9: Yield=5.496333755554645, Cost=13.947368555491579
ε=14.8: Yield=5.942015638591265, Cost=14.842105406547812
ε=15.7: Yield=6.3879298839916725, Cost=15.7368422576034
ε=16.6: Yield=6.834031802135927, Cost=16.63157910865851
ε=17.5: Yield=7.280287422759853, Cost=17.52631595971331
ε=18.4: Yield=7.726670475532958, Cost=18.421052810767947
ε=19.3: Yield=8.173160333139313, Cost=19.3157896618226
ε=20.2: Yield=8.619740578149942, Cost=20.21052651287754
ε=21.1: Yield=9.066397984260815, Cost=21.10526336393353
ε=22.0: Yield=9.513121778926397, Cost=22.00000021499397

Found 15 Pareto-optimal solutions

======================================================================
METHOD 3: pymoo with NSGA-II (Evolutionary Algorithm)
======================================================================

Running NSGA-II evolutionary algorithm...
Found 100 Pareto-optimal solutions
Best yield: 10.00
Lowest cost: 8.93

======================================================================
GENERATING COMPREHENSIVE VISUALIZATION
======================================================================

Visualization saved as 'multi_objective_comparison.png'

======================================================================
MULTI-OBJECTIVE OPTIMIZATION COMPLETE!
======================================================================

All three methods successfully demonstrated:
1. Weighted Sum - Fast single solutions
2. Epsilon-Constraint - Complete Pareto front
3. NSGA-II (pymoo) - Evolutionary approach
======================================================================
PASSED
s2_rx_anonym/py/test_s2_rx_anonym.py::test_s2_rx_anonym_csv 
Data shape: (6809448, 11)
Columns: ['CH', 'RANK', 'Byte', 'p0', 'p1', 'p2', 'p3', 'p4', 'p5', 'o0', 'o1']

First few rows:
   CH  RANK  Byte  p0  p1  p2  p3  p4  p5   o0    o1
0   0     0     0  40  56  60   9   0   0 -1.0   0.0
1   0     0     0  40  56  60   9   0   0 -1.0   5.0
2   0     0     0  40  56  60   9   0   0 -1.0  10.0
3   0     0     0  40  56  60   9   0   0 -1.0  15.0
4   0     0     0  40  56  60   9   0   0 -1.0  20.0

Number of unique fixed parameter combinations: 32

Training surrogate models...
max_depth=17; n_estimators=18; min_samples_split=20
Model o0 R² score: 0.2315
Model o1 R² score: 0.1007
PASSED
s2_tx_anonym/py/test_s2_tx_anonym.py::test_s2_tx_anonym_csv 
Data shape: (2240060, 9)
Columns: ['CH', 'RANK', 'Byte', 'p0', 'p1', 'p2', 'p3', 'o0', 'o1']

First few rows:
   CH  RANK  Byte  p0  p1  p2  p3    o0    o1
0   0     0     0  15   3  60  40 -8.25   0.0
1   0     0     0  15   3  60  40 -8.25   4.0
2   0     0     0  15   3  60  40 -8.25   8.0
3   0     0     0  15   3  60  40 -8.25  12.0
4   0     0     0  15   3  60  40 -8.25  16.0

Number of unique fixed parameter combinations: 32

Training surrogate models...
max_depth=17; n_estimators=18; min_samples_split=20
Model o0 R² score: 0.3885
Model o1 R² score: 0.0994
PASSED
shekel/py/test_pytorch.py::test_pytorch 

-------------------------------- live log call ---------------------------------
Random seeds set for reproducibility (seed=42)
============================================================
Using device: cpu
Loading data from /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/shekel/py/shekel_meshgrid_26.csv.expected.gz...
Data shape: (456976, 5)
Columns: ['X1', 'X2', 'X3', 'X4', 'Y']

First few rows:
    X1   X2   X3   X4         Y
0  0.0  0.0  0.0  0.0 -0.321729
1  0.0  0.0  0.0  0.4 -0.367250
2  0.0  0.0  0.0  0.8 -0.397599
3  0.0  0.0  0.0  1.2 -0.400056
4  0.0  0.0  0.0  1.6 -0.374583

Features shape: (456976, 4)
Target shape: (456976,)
Target range: [-10.5363, -0.0809]

Train set size: 365580
Test set size: 91396

Creating neural network model...

Model Summary:
Total parameters: 5153
Trainable parameters: 5153
ShekelModel(
  (fc1): Linear(in_features=4, out_features=32, bias=True)
  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
  (fc2): Linear(in_features=32, out_features=64, bias=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
  (fc3): Linear(in_features=64, out_features=32, bias=True)
  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
  (fc4): Linear(in_features=32, out_features=16, bias=True)
  (fc5): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)

Training model...
Initial learning rate: 0.005 (using NAdam optimizer with cosine annealing)
Epoch 1/150 - loss: 0.135587 - mae: 0.163769 - val_loss: 0.413166 - val_mae: 0.508322 - lr: 0.005000
Epoch 2/150 - loss: 0.089121 - mae: 0.127562 - val_loss: 0.130448 - val_mae: 0.274239 - lr: 0.004999
Epoch 3/150 - loss: 0.076678 - mae: 0.118861 - val_loss: 0.417887 - val_mae: 0.412922 - lr: 0.004998
Epoch 4/150 - loss: 0.080152 - mae: 0.119791 - val_loss: 0.095018 - val_mae: 0.190138 - lr: 0.004995
Epoch 5/150 - loss: 0.066851 - mae: 0.109830 - val_loss: 0.082349 - val_mae: 0.164814 - lr: 0.004991
Epoch 6/150 - loss: 0.067974 - mae: 0.110568 - val_loss: 0.114679 - val_mae: 0.222621 - lr: 0.004986
Epoch 7/150 - loss: 0.067148 - mae: 0.111111 - val_loss: 0.058083 - val_mae: 0.152316 - lr: 0.004980
Epoch 8/150 - loss: 0.063059 - mae: 0.106338 - val_loss: 0.176450 - val_mae: 0.282097 - lr: 0.004973
Epoch 9/150 - loss: 0.074368 - mae: 0.116426 - val_loss: 0.090131 - val_mae: 0.137562 - lr: 0.004965
Epoch 10/150 - loss: 0.061905 - mae: 0.105909 - val_loss: 0.039214 - val_mae: 0.099452 - lr: 0.004956
Epoch 11/150 - loss: 0.059968 - mae: 0.103428 - val_loss: 0.041603 - val_mae: 0.124036 - lr: 0.004945
Epoch 12/150 - loss: 0.057017 - mae: 0.104029 - val_loss: 0.051474 - val_mae: 0.115193 - lr: 0.004934
Epoch 13/150 - loss: 0.055926 - mae: 0.101731 - val_loss: 0.028260 - val_mae: 0.093743 - lr: 0.004921
Epoch 14/150 - loss: 0.058886 - mae: 0.103365 - val_loss: 0.061896 - val_mae: 0.168033 - lr: 0.004908
Epoch 15/150 - loss: 0.056791 - mae: 0.102391 - val_loss: 0.022273 - val_mae: 0.079798 - lr: 0.004893
Epoch 16/150 - loss: 0.052589 - mae: 0.099097 - val_loss: 0.031415 - val_mae: 0.097506 - lr: 0.004878
Epoch 17/150 - loss: 0.052803 - mae: 0.098956 - val_loss: 0.043347 - val_mae: 0.094731 - lr: 0.004861
Epoch 18/150 - loss: 0.051928 - mae: 0.097785 - val_loss: 0.054340 - val_mae: 0.125796 - lr: 0.004843
Epoch 19/150 - loss: 0.055243 - mae: 0.101433 - val_loss: 0.042756 - val_mae: 0.139760 - lr: 0.004824
Epoch 20/150 - loss: 0.055618 - mae: 0.100270 - val_loss: 0.030283 - val_mae: 0.091171 - lr: 0.004805
Epoch 21/150 - loss: 0.049400 - mae: 0.097323 - val_loss: 0.055611 - val_mae: 0.172881 - lr: 0.004784
Epoch 22/150 - loss: 0.057546 - mae: 0.102724 - val_loss: 0.023960 - val_mae: 0.064779 - lr: 0.004762
Epoch 23/150 - loss: 0.052222 - mae: 0.098741 - val_loss: 0.025844 - val_mae: 0.112219 - lr: 0.004739
Epoch 24/150 - loss: 0.050118 - mae: 0.096237 - val_loss: 0.042682 - val_mae: 0.107915 - lr: 0.004716
Epoch 25/150 - loss: 0.055896 - mae: 0.101008 - val_loss: 0.030752 - val_mae: 0.088532 - lr: 0.004691
Epoch 26/150 - loss: 0.055722 - mae: 0.102247 - val_loss: 0.031248 - val_mae: 0.084709 - lr: 0.004665
Epoch 27/150 - loss: 0.053766 - mae: 0.100622 - val_loss: 0.022116 - val_mae: 0.073442 - lr: 0.004638
Epoch 28/150 - loss: 0.050501 - mae: 0.099632 - val_loss: 0.027630 - val_mae: 0.091483 - lr: 0.004611
Epoch 29/150 - loss: 0.052467 - mae: 0.099118 - val_loss: 0.031215 - val_mae: 0.087048 - lr: 0.004582
Epoch 30/150 - loss: 0.050828 - mae: 0.098001 - val_loss: 0.015981 - val_mae: 0.057503 - lr: 0.004553
Epoch 31/150 - loss: 0.047840 - mae: 0.095229 - val_loss: 0.051390 - val_mae: 0.123820 - lr: 0.004523
Epoch 32/150 - loss: 0.049472 - mae: 0.098182 - val_loss: 0.033248 - val_mae: 0.124844 - lr: 0.004491
Epoch 33/150 - loss: 0.047672 - mae: 0.095671 - val_loss: 0.026382 - val_mae: 0.117589 - lr: 0.004459
Epoch 34/150 - loss: 0.044775 - mae: 0.094283 - val_loss: 0.019916 - val_mae: 0.089615 - lr: 0.004426
Epoch 35/150 - loss: 0.047743 - mae: 0.095298 - val_loss: 0.046224 - val_mae: 0.102003 - lr: 0.004393
Epoch 36/150 - loss: 0.048837 - mae: 0.095738 - val_loss: 0.040475 - val_mae: 0.156963 - lr: 0.004358
Epoch 37/150 - loss: 0.047510 - mae: 0.097136 - val_loss: 0.020232 - val_mae: 0.067106 - lr: 0.004323
Epoch 38/150 - loss: 0.047504 - mae: 0.093384 - val_loss: 0.030858 - val_mae: 0.127506 - lr: 0.004286
Epoch 39/150 - loss: 0.046383 - mae: 0.096158 - val_loss: 0.016732 - val_mae: 0.059954 - lr: 0.004249
Epoch 40/150 - loss: 0.046546 - mae: 0.092788 - val_loss: 0.031023 - val_mae: 0.127849 - lr: 0.004212
Epoch 41/150 - loss: 0.049157 - mae: 0.095503 - val_loss: 0.018906 - val_mae: 0.077386 - lr: 0.004173
Epoch 42/150 - loss: 0.050920 - mae: 0.097181 - val_loss: 0.024950 - val_mae: 0.072470 - lr: 0.004134
Epoch 43/150 - loss: 0.050483 - mae: 0.095015 - val_loss: 0.025022 - val_mae: 0.078455 - lr: 0.004094
Epoch 44/150 - loss: 0.047604 - mae: 0.094918 - val_loss: 0.023362 - val_mae: 0.075056 - lr: 0.004053
Epoch 45/150 - loss: 0.047546 - mae: 0.095118 - val_loss: 0.041409 - val_mae: 0.129831 - lr: 0.004012
Epoch 46/150 - loss: 0.044752 - mae: 0.095572 - val_loss: 0.069406 - val_mae: 0.146963 - lr: 0.003970
Epoch 47/150 - loss: 0.050937 - mae: 0.096994 - val_loss: 0.018669 - val_mae: 0.065403 - lr: 0.003927
Epoch 48/150 - loss: 0.044394 - mae: 0.092324 - val_loss: 0.020278 - val_mae: 0.071465 - lr: 0.003884
Epoch 49/150 - loss: 0.046551 - mae: 0.093308 - val_loss: 0.017928 - val_mae: 0.058181 - lr: 0.003840
Epoch 50/150 - loss: 0.046241 - mae: 0.093586 - val_loss: 0.016366 - val_mae: 0.083404 - lr: 0.003795
Epoch 51/150 - loss: 0.044552 - mae: 0.092844 - val_loss: 0.028340 - val_mae: 0.122209 - lr: 0.003750
Epoch 52/150 - loss: 0.044739 - mae: 0.092649 - val_loss: 0.031839 - val_mae: 0.133127 - lr: 0.003705
Epoch 53/150 - loss: 0.041988 - mae: 0.091060 - val_loss: 0.055362 - val_mae: 0.137535 - lr: 0.003659
Epoch 54/150 - loss: 0.051496 - mae: 0.098319 - val_loss: 0.027076 - val_mae: 0.077934 - lr: 0.003612
Epoch 55/150 - loss: 0.044119 - mae: 0.090815 - val_loss: 0.018502 - val_mae: 0.062964 - lr: 0.003565
Epoch 56/150 - loss: 0.042816 - mae: 0.090966 - val_loss: 0.010907 - val_mae: 0.058870 - lr: 0.003517
Epoch 57/150 - loss: 0.041375 - mae: 0.090779 - val_loss: 0.020239 - val_mae: 0.070791 - lr: 0.003469
Epoch 58/150 - loss: 0.043465 - mae: 0.091434 - val_loss: 0.024808 - val_mae: 0.121065 - lr: 0.003421
Epoch 59/150 - loss: 0.041847 - mae: 0.090720 - val_loss: 0.016087 - val_mae: 0.071228 - lr: 0.003372
Epoch 60/150 - loss: 0.038808 - mae: 0.088966 - val_loss: 0.021634 - val_mae: 0.070566 - lr: 0.003323
Epoch 61/150 - loss: 0.044233 - mae: 0.092855 - val_loss: 0.039605 - val_mae: 0.095508 - lr: 0.003273
Epoch 62/150 - loss: 0.040677 - mae: 0.089979 - val_loss: 0.019851 - val_mae: 0.082708 - lr: 0.003223
Epoch 63/150 - loss: 0.040072 - mae: 0.088811 - val_loss: 0.024763 - val_mae: 0.116866 - lr: 0.003173
Epoch 64/150 - loss: 0.039624 - mae: 0.090058 - val_loss: 0.012729 - val_mae: 0.067648 - lr: 0.003122
Epoch 65/150 - loss: 0.040485 - mae: 0.088470 - val_loss: 0.029504 - val_mae: 0.119767 - lr: 0.003071
Epoch 66/150 - loss: 0.042010 - mae: 0.089901 - val_loss: 0.014412 - val_mae: 0.054159 - lr: 0.003020
Epoch 67/150 - loss: 0.038552 - mae: 0.088525 - val_loss: 0.011971 - val_mae: 0.054457 - lr: 0.002969
Epoch 68/150 - loss: 0.042274 - mae: 0.089375 - val_loss: 0.010204 - val_mae: 0.059020 - lr: 0.002917
Epoch 69/150 - loss: 0.042369 - mae: 0.090933 - val_loss: 0.032296 - val_mae: 0.091784 - lr: 0.002866
Epoch 70/150 - loss: 0.041818 - mae: 0.089084 - val_loss: 0.018368 - val_mae: 0.086914 - lr: 0.002814
Epoch 71/150 - loss: 0.038792 - mae: 0.087558 - val_loss: 0.013446 - val_mae: 0.055349 - lr: 0.002762
Epoch 72/150 - loss: 0.038754 - mae: 0.086818 - val_loss: 0.009654 - val_mae: 0.052867 - lr: 0.002710
Epoch 73/150 - loss: 0.038758 - mae: 0.087055 - val_loss: 0.017563 - val_mae: 0.059263 - lr: 0.002657
Epoch 74/150 - loss: 0.037880 - mae: 0.085808 - val_loss: 0.020041 - val_mae: 0.083654 - lr: 0.002605
Epoch 75/150 - loss: 0.038410 - mae: 0.088910 - val_loss: 0.016292 - val_mae: 0.072781 - lr: 0.002553
Epoch 76/150 - loss: 0.038079 - mae: 0.086533 - val_loss: 0.020027 - val_mae: 0.062297 - lr: 0.002501
Epoch 77/150 - loss: 0.039674 - mae: 0.086696 - val_loss: 0.018581 - val_mae: 0.084266 - lr: 0.002448
Epoch 78/150 - loss: 0.039304 - mae: 0.086695 - val_loss: 0.012104 - val_mae: 0.067055 - lr: 0.002396
Epoch 79/150 - loss: 0.040268 - mae: 0.086641 - val_loss: 0.012182 - val_mae: 0.051105 - lr: 0.002344
Epoch 80/150 - loss: 0.038581 - mae: 0.085380 - val_loss: 0.013652 - val_mae: 0.054205 - lr: 0.002291
Epoch 81/150 - loss: 0.038498 - mae: 0.086055 - val_loss: 0.010611 - val_mae: 0.051375 - lr: 0.002239
Epoch 82/150 - loss: 0.035291 - mae: 0.083743 - val_loss: 0.022618 - val_mae: 0.065275 - lr: 0.002187
Epoch 83/150 - loss: 0.035003 - mae: 0.083933 - val_loss: 0.009560 - val_mae: 0.055073 - lr: 0.002135
Epoch 84/150 - loss: 0.036992 - mae: 0.086152 - val_loss: 0.009657 - val_mae: 0.053620 - lr: 0.002084
Epoch 85/150 - loss: 0.035982 - mae: 0.083976 - val_loss: 0.009829 - val_mae: 0.049060 - lr: 0.002032
Epoch 86/150 - loss: 0.036072 - mae: 0.083289 - val_loss: 0.009730 - val_mae: 0.055960 - lr: 0.001981
Epoch 87/150 - loss: 0.034382 - mae: 0.083634 - val_loss: 0.012380 - val_mae: 0.050104 - lr: 0.001930
Epoch 88/150 - loss: 0.038195 - mae: 0.084982 - val_loss: 0.012797 - val_mae: 0.074389 - lr: 0.001879
Epoch 89/150 - loss: 0.035582 - mae: 0.082745 - val_loss: 0.009814 - val_mae: 0.045221 - lr: 0.001828
Epoch 90/150 - loss: 0.035222 - mae: 0.084518 - val_loss: 0.009564 - val_mae: 0.049677 - lr: 0.001778
Epoch 91/150 - loss: 0.035952 - mae: 0.083316 - val_loss: 0.009874 - val_mae: 0.057546 - lr: 0.001728
Epoch 92/150 - loss: 0.034984 - mae: 0.082761 - val_loss: 0.008828 - val_mae: 0.048389 - lr: 0.001678
Epoch 93/150 - loss: 0.033757 - mae: 0.083071 - val_loss: 0.022326 - val_mae: 0.076378 - lr: 0.001629
Epoch 94/150 - loss: 0.034964 - mae: 0.083624 - val_loss: 0.007936 - val_mae: 0.042431 - lr: 0.001580
Epoch 95/150 - loss: 0.033572 - mae: 0.082397 - val_loss: 0.007748 - val_mae: 0.044559 - lr: 0.001532
Epoch 96/150 - loss: 0.035984 - mae: 0.081918 - val_loss: 0.010622 - val_mae: 0.053365 - lr: 0.001484
Epoch 97/150 - loss: 0.034526 - mae: 0.081922 - val_loss: 0.009217 - val_mae: 0.058394 - lr: 0.001436
Epoch 98/150 - loss: 0.032884 - mae: 0.081368 - val_loss: 0.011939 - val_mae: 0.079977 - lr: 0.001389
Epoch 99/150 - loss: 0.036266 - mae: 0.083609 - val_loss: 0.008445 - val_mae: 0.042022 - lr: 0.001342
Epoch 100/150 - loss: 0.032898 - mae: 0.081185 - val_loss: 0.007606 - val_mae: 0.044783 - lr: 0.001296
Epoch 101/150 - loss: 0.033913 - mae: 0.082370 - val_loss: 0.008826 - val_mae: 0.055441 - lr: 0.001251
Epoch 102/150 - loss: 0.036251 - mae: 0.081262 - val_loss: 0.007934 - val_mae: 0.055032 - lr: 0.001206
Epoch 103/150 - loss: 0.031895 - mae: 0.080700 - val_loss: 0.008336 - val_mae: 0.051032 - lr: 0.001161
Epoch 104/150 - loss: 0.032700 - mae: 0.081788 - val_loss: 0.008146 - val_mae: 0.058413 - lr: 0.001117
Epoch 105/150 - loss: 0.031458 - mae: 0.079812 - val_loss: 0.007506 - val_mae: 0.047256 - lr: 0.001074
Epoch 106/150 - loss: 0.034580 - mae: 0.080240 - val_loss: 0.007572 - val_mae: 0.051100 - lr: 0.001031
Epoch 107/150 - loss: 0.032312 - mae: 0.080632 - val_loss: 0.009595 - val_mae: 0.054765 - lr: 0.000989
Epoch 108/150 - loss: 0.031059 - mae: 0.080472 - val_loss: 0.006107 - val_mae: 0.041627 - lr: 0.000948
Epoch 109/150 - loss: 0.030701 - mae: 0.079011 - val_loss: 0.007741 - val_mae: 0.051210 - lr: 0.000907
Epoch 110/150 - loss: 0.032399 - mae: 0.079878 - val_loss: 0.006470 - val_mae: 0.039500 - lr: 0.000867
Epoch 111/150 - loss: 0.033177 - mae: 0.080064 - val_loss: 0.005812 - val_mae: 0.040225 - lr: 0.000828
Epoch 112/150 - loss: 0.032066 - mae: 0.079364 - val_loss: 0.006674 - val_mae: 0.043785 - lr: 0.000789
Epoch 113/150 - loss: 0.030423 - mae: 0.079534 - val_loss: 0.005833 - val_mae: 0.041971 - lr: 0.000752
Epoch 114/150 - loss: 0.031003 - mae: 0.079613 - val_loss: 0.008107 - val_mae: 0.054010 - lr: 0.000715
Epoch 115/150 - loss: 0.033059 - mae: 0.079802 - val_loss: 0.006770 - val_mae: 0.040277 - lr: 0.000678
Epoch 116/150 - loss: 0.030150 - mae: 0.077381 - val_loss: 0.006270 - val_mae: 0.041004 - lr: 0.000643
Epoch 117/150 - loss: 0.030026 - mae: 0.078193 - val_loss: 0.005945 - val_mae: 0.043943 - lr: 0.000608
Epoch 118/150 - loss: 0.030078 - mae: 0.078249 - val_loss: 0.006609 - val_mae: 0.041048 - lr: 0.000575
Epoch 119/150 - loss: 0.030103 - mae: 0.078935 - val_loss: 0.006796 - val_mae: 0.042899 - lr: 0.000542
Epoch 120/150 - loss: 0.030858 - mae: 0.078924 - val_loss: 0.005680 - val_mae: 0.040832 - lr: 0.000510
Epoch 121/150 - loss: 0.028251 - mae: 0.077294 - val_loss: 0.006000 - val_mae: 0.040057 - lr: 0.000478
Epoch 122/150 - loss: 0.031325 - mae: 0.078909 - val_loss: 0.006245 - val_mae: 0.041855 - lr: 0.000448
Epoch 123/150 - loss: 0.032548 - mae: 0.078720 - val_loss: 0.005693 - val_mae: 0.038506 - lr: 0.000419
Epoch 124/150 - loss: 0.032416 - mae: 0.077956 - val_loss: 0.005621 - val_mae: 0.038589 - lr: 0.000390
Epoch 125/150 - loss: 0.028703 - mae: 0.076866 - val_loss: 0.005273 - val_mae: 0.038015 - lr: 0.000363
Epoch 126/150 - loss: 0.029951 - mae: 0.077593 - val_loss: 0.006889 - val_mae: 0.039456 - lr: 0.000336
Epoch 127/150 - loss: 0.029916 - mae: 0.077880 - val_loss: 0.005657 - val_mae: 0.041228 - lr: 0.000310
Epoch 128/150 - loss: 0.030680 - mae: 0.077268 - val_loss: 0.005844 - val_mae: 0.039134 - lr: 0.000285
Epoch 129/150 - loss: 0.029600 - mae: 0.077466 - val_loss: 0.005787 - val_mae: 0.038311 - lr: 0.000262
Epoch 130/150 - loss: 0.030589 - mae: 0.077360 - val_loss: 0.006252 - val_mae: 0.039239 - lr: 0.000239
Epoch 131/150 - loss: 0.030566 - mae: 0.077719 - val_loss: 0.005385 - val_mae: 0.040223 - lr: 0.000217
Epoch 132/150 - loss: 0.030825 - mae: 0.076856 - val_loss: 0.004879 - val_mae: 0.037456 - lr: 0.000196
Epoch 133/150 - loss: 0.027377 - mae: 0.076015 - val_loss: 0.005308 - val_mae: 0.037965 - lr: 0.000177
Epoch 134/150 - loss: 0.030004 - mae: 0.077211 - val_loss: 0.005091 - val_mae: 0.040203 - lr: 0.000158
Epoch 135/150 - loss: 0.028026 - mae: 0.075752 - val_loss: 0.004750 - val_mae: 0.038257 - lr: 0.000140
Epoch 136/150 - loss: 0.029355 - mae: 0.077020 - val_loss: 0.004886 - val_mae: 0.037299 - lr: 0.000123
Epoch 137/150 - loss: 0.027915 - mae: 0.076582 - val_loss: 0.004811 - val_mae: 0.037665 - lr: 0.000108
Epoch 138/150 - loss: 0.031321 - mae: 0.076209 - val_loss: 0.005136 - val_mae: 0.037682 - lr: 0.000093
Epoch 139/150 - loss: 0.030267 - mae: 0.074857 - val_loss: 0.005073 - val_mae: 0.037274 - lr: 0.000080
Epoch 140/150 - loss: 0.030841 - mae: 0.076847 - val_loss: 0.005084 - val_mae: 0.037462 - lr: 0.000067
Epoch 141/150 - loss: 0.029110 - mae: 0.076197 - val_loss: 0.004833 - val_mae: 0.037469 - lr: 0.000056
Epoch 142/150 - loss: 0.029074 - mae: 0.077173 - val_loss: 0.004867 - val_mae: 0.037138 - lr: 0.000045
Epoch 143/150 - loss: 0.031849 - mae: 0.077416 - val_loss: 0.004824 - val_mae: 0.036955 - lr: 0.000036
Epoch 144/150 - loss: 0.028199 - mae: 0.076745 - val_loss: 0.004718 - val_mae: 0.037447 - lr: 0.000028
Epoch 145/150 - loss: 0.028336 - mae: 0.076419 - val_loss: 0.004864 - val_mae: 0.037143 - lr: 0.000021
Epoch 146/150 - loss: 0.028039 - mae: 0.075938 - val_loss: 0.004847 - val_mae: 0.037244 - lr: 0.000015
Epoch 147/150 - loss: 0.027983 - mae: 0.076169 - val_loss: 0.004921 - val_mae: 0.037041 - lr: 0.000010
Epoch 148/150 - loss: 0.028707 - mae: 0.076258 - val_loss: 0.004969 - val_mae: 0.037112 - lr: 0.000006
Epoch 149/150 - loss: 0.028335 - mae: 0.076192 - val_loss: 0.004906 - val_mae: 0.037434 - lr: 0.000003
Epoch 150/150 - loss: 0.027341 - mae: 0.075726 - val_loss: 0.004693 - val_mae: 0.036873 - lr: 0.000002
Restoring best weights from epoch 150

============================================================
Model Evaluation:

Metrics on original scale:
MSE: 0.000142
RMSE: 0.011900
MAE: 0.006405
R² Score: 0.995147
Correlation Coefficient: 0.997629

============================================================
Testing at known minimum [4, 4, 4, 4]:
Predicted: -6.7975
Expected: ~-10.5363

Training history plot saved as 'training_history.png'
Ignoring fixed x limits to fulfill fixed data aspect with adjustable data limits.
Ignoring fixed x limits to fulfill fixed data aspect with adjustable data limits.

Correlation plot saved as 'correlation_plot.png'

Scalers saved to scaler_X_pytorch.pkl and scaler_y_pytorch.pkl
W0127 15:01:37.363000 642875 torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 15 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features
W0127 15:01:38.046000 642875 torch/onnx/_internal/exporter/_registration.py:110] torchvision is not installed. Skipping torchvision::nms
[torch.onnx] Obtain model graph for `ShekelModel([...]` with `torch.export.export(..., strict=False)`...
[torch.onnx] Obtain model graph for `ShekelModel([...]` with `torch.export.export(..., strict=False)`... ✅
[torch.onnx] Run decomposition...
[torch.onnx] Run decomposition... ✅
[torch.onnx] Translate the graph into ONNX...
[torch.onnx] Translate the graph into ONNX... ✅
The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 15).
Removed 23 unused nodes
No unused functions to remove
Skipping constant folding for node 'node_linear' because it is graph input to preserve graph signature
Removed 12 unused nodes
Applied 3 of general pattern rewrite rules.
No unused functions to remove
Skipping constant folding for node 'node_Gemm_0' because it is graph input to preserve graph signature
No unused functions to remove
PassManager: No more graph changes detected after step 1
ONNX model exported to /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/shekel/py/shekel_model.pytorch

============================================================
Training complete!
PASSED
shekel/py/test_shgo_shekel.py::test_optimization_ex2 
Optimizing Shekel function using SHGO algorithm...
============================================================

Optimization Results:
Success: True
Message: Optimization terminated successfully.

Optimal solution (x*):
  x = [4.00074686 3.99950947 4.00074686 3.99950947]

Optimal function value (f(x*)):
  f(x*) = -10.5364431535

Number of function evaluations: 2254
Number of iterations: 5

============================================================
Note: The known global minimum is approximately:
  x* ≈ [4, 4, 4, 4]
  f(x*) ≈ -10.5363

============================================================
Function evaluations at test points:
  f([4.0, 4.0, 4.0, 4.0]) = -10.536284
  f([1.2, 2.0, 3.2, 4.0]) = -0.323428
  f([8.0, 7.2, 6.0, 4.8]) = -0.416895
Creating CSV with 456976 points...
Grid size: 26 points per dimension
Total points: 456976

CSV file '/tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/shekel/py/shekel_meshgrid_26.csv' created successfully!
Total rows: 456977 (including header)

Minimum value found in grid:
  X = [4. 4. 4. 4.]
  Y = -10.5362837262
PASSED
shekel/py/test_shgo_shekel_pytorch.py::test_shgo_shekel_onnx 

-------------------------------- live log call ---------------------------------

[2/2] Running SHGO with ONNX NEURAL NETWORK model...
--------------------------------------------------------------------------------
ONNX model loaded from shekel_model_expected.pytorch
Scalers loaded

============================================================
Optimizing with PyTorch ONNX Neural Network Model using SHGO
============================================================
[Elapsed time: 0.445] Elapsed CPU time: 0.511 seconds

Optimization Results (ONNX Model):
Success: True
Message: Optimization terminated successfully.

Optimal solution (x*):
  x = [3.75       3.75       3.75179509 3.75      ]

Optimal function value (f(x*)):
  f(x*) = -3.8327736855

Number of function evaluations: 1056
Number of iterations: 5

================================================================================
COMPARISON RESULTS
================================================================================

Metric                         Actual Function      NN Model             Difference     
-------------------------------------------------------------------------------------
Optimal X:                    
  x[0]                         4.000747             3.750000             0.250747       
  x[1]                         3.999509             3.750000             0.249509       
  x[2]                         4.000747             3.751795             0.248952       
  x[3]                         3.999509             3.750000             0.249509       

Optimal f(x):                 
  Function result             -10.5364431535       -3.8327736855        6.7036694680   
  Actual Shekel at x*         -10.5364431535       -3.3650080854        7.1714350681   

Performance:                  
  Function evaluations        2254                 1056                 1198           
  Iterations                  5                    5                    0              
  Success                     True                 True                

================================================================================
SUMMARY
================================================================================
Distance between optimal solutions (L2 norm): 0.499361
Difference in actual function values: 7.1714350681
Distance from [4,4,4,4]             0.001264             0.499105            
✗ ONNX neural network model solution differs significantly from actual optimum.
⚡ Speedup: 2.13x fewer function evaluations with ONNX NN model
PASSED

=============================== warnings summary ===============================
../../../../../usr/lib/python3/dist-packages/z3/z3core.py:5
  /usr/lib/python3/dist-packages/z3/z3core.py:5: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
    import pkg_resources

c3dtlz4/py/test_gpsampler.py::test_gpsampler
  /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/c3dtlz4/py/c3dtlz4_ex.py:38: ExperimentalWarning: GPSampler is experimental (supported from v3.6.0). The interface can change in the future.
    sampler=optuna.samplers.GPSampler(seed=42, constraints_func=c3dtlz4.constraints_func, deterministic_objective=True),

c3dtlz4/py/test_gpsampler.py::test_gpsampler
  /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/c3dtlz4/py/c3dtlz4_ex.py:38: ExperimentalWarning: Argument ``constraints_func`` is an experimental feature. The interface can change in the future.
    sampler=optuna.samplers.GPSampler(seed=42, constraints_func=c3dtlz4.constraints_func, deterministic_objective=True),

shekel/py/test_pytorch.py::test_pytorch
  /tmp/smlp_tutorial_mdmitry_638344/smlp/tutorial/examples/shekel/py/pytorch_ex.py:452: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.
    torch.onnx.export(

shekel/py/test_pytorch.py::test_pytorch
  /usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/exporter/_compat.py:104: DeprecationWarning: from_dynamic_axes_to_dynamic_shapes is deprecated and will be removed in a future release. This function converts 'dynamic_axes' format (including custom axis names) to 'dynamic_shapes' format. Instead of relying on this conversion, provide 'dynamic_shapes' directly with custom names.
    _dynamic_shapes.from_dynamic_axes_to_dynamic_shapes(

shekel/py/test_pytorch.py::test_pytorch
  /usr/lib/python3.12/copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.
    return cls.__new__(cls, *args)

shekel/py/test_pytorch.py::test_pytorch
  /usr/local/lib/python3.12/dist-packages/torch/onnx/__init__.py:296: DeprecationWarning: Saving ONNX model to a BytesIO object is deprecated. Please use a file path instead.
    return _compat.export_compat(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
========== 22 passed, 10 deselected, 7 warnings in 1659.06s (0:27:39) ==========
