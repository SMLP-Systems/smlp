Run directory: /tmp/smlp_tutorial_mdmitry_494180
Cloning into 'smlp'...
Switched to a new branch 'SMLP_TUTORIAL'
branch 'SMLP_TUTORIAL' set up to track 'origin/SMLP_TUTORIAL'.
env PYTHONDONTWRITEBYTECODE=1 /usr/bin/pytest -v --forked -m forked /tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples
/usr/lib/python3/dist-packages/_pytest/config/__init__.py:482: PytestConfigWarning: pytest-catchlog plugin has been merged into the core, please remove it from your requirements.
  warnings.warn(
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-7.4.4, pluggy-1.4.0 -- /usr/bin/python3
cachedir: .pytest_cache
rootdir: /tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples
plugins: anyio-4.12.0, catchlog-1.2.2, typeguard-4.4.4, forked-1.6.0, mock-3.12.0
collecting ... 
----------------------------- live log collection ------------------------------
NumExpr defaulting to 4 threads.
collected 26 items / 18 deselected / 8 selected

bnh/py/test_z3.py::test_z3 
-------------------------------- live log call ---------------------------------
============================================================
BNH Multi-Objective Optimization Problem with Z3
============================================================

1. Lexicographic Optimization (f1 primary, f2 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

2. Lexicographic Optimization (f2 primary, f1 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

3. Weighted Sum Method (various weights):
------------------------------------------------------------

Weights (w1=0.2, w2=0.8):
  x1 = 2.750000, x2 = 2.625000
  f1 = 57.812500, f2 = 10.703125

Weights (w1=0.4, w2=0.6):
  x1 = 1.000000, x2 = 1.000000
  f1 = 8.000000, f2 = 32.000000

Weights (w1=0.5, w2=0.5):
  x1 = 1.000000, x2 = 2.000000
  f1 = 20.000000, f2 = 25.000000

Weights (w1=0.6, w2=0.4):
  x1 = 1.000000, x2 = 1.000000
  f1 = 8.000000, f2 = 32.000000

Weights (w1=0.8, w2=0.2):
  x1 = 0.250000, x2 = 0.250000
  f1 = 0.500000, f2 = 45.125000

4. Constraint Verification:
------------------------------------------------------------
Solution 1:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 2:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 3:
  C1 = 11.953125 (should be ≤ 25): ✓
  C2 = 59.203125 (should be ≥ 7.7): ✓
Solution 4:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 5:
  C1 = 20.000000 (should be ≤ 25): ✓
  C2 = 74.000000 (should be ≥ 7.7): ✓
Solution 6:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 7:
  C1 = 22.625000 (should be ≤ 25): ✓
  C2 = 70.625000 (should be ≥ 7.7): ✓

5. Pareto Front Plot Generated
------------------------------------------------------------

============================================================
Found 7 Pareto optimal solutions
============================================================
(0.0, 0.0, 0.0, 50.0) (0.0, 0.0, 0.0, 50.0) (2.75, 2.625, 57.8125, 10.703125) (1.0, 1.0, 8.0, 32.0) (1.0, 2.0, 20.0, 25.0) (1.0, 1.0, 8.0, 32.0) (0.25, 0.25, 0.5, 45.125)
PASSED                                        [ 12%]
bnh/py/test_z3_gradient.py::test_z3_gradient 
-------------------------------- live log call ---------------------------------
============================================================
BNH Multi-Objective Optimization Problem with Z3
============================================================

1. Lexicographic Optimization (f1 primary, f2 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

2. Lexicographic Optimization (f2 primary, f1 secondary):
------------------------------------------------------------
x1 = 0.000000
x2 = 0.000000
f1 = 0.000000
f2 = 50.000000

3. Weighted Sum Method (various weights):
------------------------------------------------------------

Trying weights (w1=0.1, w2=0.9)...
  ✓ x1 = 2.5020, x2 = 1.0000
    f1 = 29.0391, f2 = 22.2402

Trying weights (w1=0.2, w2=0.8)...
  ✓ x1 = 1.0000, x2 = 2.8301
    f1 = 36.0374, f2 = 20.7086

Trying weights (w1=0.4, w2=0.6)...
  ✓ x1 = 1.0000, x2 = 1.0000
    f1 = 8.0000, f2 = 32.0000

Trying weights (w1=0.5, w2=0.5)...
  ✓ x1 = 1.0000, x2 = 2.0000
    f1 = 20.0000, f2 = 25.0000

Trying weights (w1=0.6, w2=0.4)...
  ✓ x1 = 1.0000, x2 = 1.0000
    f1 = 8.0000, f2 = 32.0000

Trying weights (w1=0.8, w2=0.2)...
  ✓ x1 = 0.2500, x2 = 0.2500
    f1 = 0.5000, f2 = 45.1250

Trying weights (w1=0.9, w2=0.1)...
  ✓ x1 = 0.0010, x2 = 0.0010
    f1 = 0.0000, f2 = 49.9805

4. Constraint Verification:
------------------------------------------------------------
Solution 1:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 2:
  C1 = 25.000000 (should be ≤ 25): ✓
  C2 = 73.000000 (should be ≥ 7.7): ✓
Solution 3:
  C1 = 7.240238 (should be ≤ 25): ✓
  C2 = 46.228519 (should be ≥ 7.7): ✓
Solution 4:
  C1 = 24.009342 (should be ≤ 25): ✓
  C2 = 82.989811 (should be ≥ 7.7): ✓
Solution 5:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 6:
  C1 = 20.000000 (should be ≤ 25): ✓
  C2 = 74.000000 (should be ≥ 7.7): ✓
Solution 7:
  C1 = 17.000000 (should be ≤ 25): ✓
  C2 = 65.000000 (should be ≥ 7.7): ✓
Solution 8:
  C1 = 22.625000 (should be ≤ 25): ✓
  C2 = 70.625000 (should be ≥ 7.7): ✓
Solution 9:
  C1 = 24.990236 (should be ≤ 25): ✓
  C2 = 72.990236 (should be ≥ 7.7): ✓

5. Stability Analysis:
============================================================

Solution 1: x = (0.000000, 0.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  0.0000,   0.0000], ||∇f1|| = 0.0001
  ∇f2 = [-10.0000, -10.0000], ||∇f2|| = 14.1421

Constraint Status:
  C1 active: True (value: 0.000000)
  C2 active: False (value: -65.300000)
    ∇C1 = [-10.0000,   0.0000]

Sensitivity Analysis:
  Avg sensitivity (f1): 0.0005
  Avg sensitivity (f2): 13.0444
  Max sensitivity (f1): 0.0007
  Max sensitivity (f2): 13.9646
  Feasible perturbations tested: 5/20

Stability Assessment:
  At boundary: True
  Stability score: 13.5935
  Classification: MODERATELY STABLE

Solution 2: x = (0.000000, 0.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  0.0000,   0.0000], ||∇f1|| = 0.0001
  ∇f2 = [-10.0000, -10.0000], ||∇f2|| = 14.1421

Constraint Status:
  C1 active: True (value: 0.000000)
  C2 active: False (value: -65.300000)
    ∇C1 = [-10.0000,   0.0000]

Sensitivity Analysis:
  Avg sensitivity (f1): 0.0004
  Avg sensitivity (f2): 12.0474
  Max sensitivity (f1): 0.0006
  Max sensitivity (f2): 13.9164
  Feasible perturbations tested: 8/20

Stability Assessment:
  At boundary: True
  Stability score: 13.0950
  Classification: MODERATELY STABLE

Solution 3: x = (2.501953, 1.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [ 20.0157,   8.0000], ||∇f1|| = 21.5552
  ∇f2 = [ -4.9961,  -8.0000], ||∇f2|| = 9.4319

Constraint Status:
  C1 active: False (value: -17.759762)
  C2 active: False (value: -38.528519)

Sensitivity Analysis:
  Avg sensitivity (f1): 10.7396
  Avg sensitivity (f2): 6.1212
  Max sensitivity (f1): 21.1891
  Max sensitivity (f2): 9.4230
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 23.9240
  Classification: WEAKLY STABLE

Solution 4: x = (1.000000, 2.830078)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,  22.6407], ||∇f1|| = 24.0125
  ∇f2 = [ -8.0000,  -4.3398], ||∇f2|| = 9.1013

Constraint Status:
  C1 active: False (value: -0.990658)
  C2 active: False (value: -75.289811)

Sensitivity Analysis:
  Avg sensitivity (f1): 13.8811
  Avg sensitivity (f2): 6.3034
  Max sensitivity (f1): 24.0000
  Max sensitivity (f2): 9.1008
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 26.6492
  Classification: WEAKLY STABLE

Solution 5: x = (1.000000, 1.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,   8.0000], ||∇f1|| = 11.3138
  ∇f2 = [ -8.0000,  -8.0000], ||∇f2|| = 11.3137

Constraint Status:
  C1 active: False (value: -8.000000)
  C2 active: False (value: -57.300000)

Sensitivity Analysis:
  Avg sensitivity (f1): 6.3865
  Avg sensitivity (f2): 6.3864
  Max sensitivity (f1): 11.3118
  Max sensitivity (f2): 11.3110
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 17.7002
  Classification: WEAKLY STABLE

Solution 6: x = (1.000000, 2.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,  16.0000], ||∇f1|| = 17.8886
  ∇f2 = [ -8.0000,  -6.0000], ||∇f2|| = 10.0000

Constraint Status:
  C1 active: False (value: -5.000000)
  C2 active: False (value: -66.300000)

Sensitivity Analysis:
  Avg sensitivity (f1): 13.5254
  Avg sensitivity (f2): 6.5607
  Max sensitivity (f1): 17.8871
  Max sensitivity (f2): 9.8904
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 23.9873
  Classification: WEAKLY STABLE

Solution 7: x = (1.000000, 1.000000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  8.0000,   8.0000], ||∇f1|| = 11.3138
  ∇f2 = [ -8.0000,  -8.0000], ||∇f2|| = 11.3137

Constraint Status:
  C1 active: False (value: -8.000000)
  C2 active: False (value: -57.300000)

Sensitivity Analysis:
  Avg sensitivity (f1): 7.7223
  Avg sensitivity (f2): 7.7226
  Max sensitivity (f1): 11.3065
  Max sensitivity (f2): 11.3066
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 19.0362
  Classification: WEAKLY STABLE

Solution 8: x = (0.250000, 0.250000)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  2.0000,   2.0000], ||∇f1|| = 2.8285
  ∇f2 = [ -9.5000,  -9.5000], ||∇f2|| = 13.4350

Constraint Status:
  C1 active: False (value: -2.375000)
  C2 active: False (value: -62.925000)

Sensitivity Analysis:
  Avg sensitivity (f1): 1.7423
  Avg sensitivity (f2): 8.2753
  Max sensitivity (f1): 2.8218
  Max sensitivity (f2): 13.3975
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: False
  Stability score: 13.1406
  Classification: MODERATELY STABLE

Solution 9: x = (0.000977, 0.000977)
------------------------------------------------------------
Gradient Information:
  ∇f1 = [  0.0079,   0.0079], ||∇f1|| = 0.0111
  ∇f2 = [ -9.9980,  -9.9980], ||∇f2|| = 14.1394

Constraint Status:
  C1 active: False (value: -0.009764)
  C2 active: False (value: -65.290236)

Sensitivity Analysis:
  Avg sensitivity (f1): 0.0060
  Avg sensitivity (f2): 7.5938
  Max sensitivity (f1): 0.0113
  Max sensitivity (f2): 14.1127
  Feasible perturbations tested: 20/20

Stability Assessment:
  At boundary: True
  Stability score: 10.8752
  Classification: MODERATELY STABLE

============================================================
STABILITY SUMMARY TABLE
============================================================
Sol   x1         x2         ||∇f1||    ||∇f2||    Score      Class               
------------------------------------------------------------
1     0.0000     0.0000     0.0001     14.1421    13.5935    MODERATELY STABLE   
2     0.0000     0.0000     0.0001     14.1421    13.0950    MODERATELY STABLE   
3     2.5020     1.0000     21.5552    9.4319     23.9240    WEAKLY STABLE       
4     1.0000     2.8301     24.0125    9.1013     26.6492    WEAKLY STABLE       
5     1.0000     1.0000     11.3138    11.3137    17.7002    WEAKLY STABLE       
6     1.0000     2.0000     17.8886    10.0000    23.9873    WEAKLY STABLE       
7     1.0000     1.0000     11.3138    11.3137    19.0362    WEAKLY STABLE       
8     0.2500     0.2500     2.8285     13.4350    13.1406    MODERATELY STABLE   
9     0.0010     0.0010     0.0111     14.1394    10.8752    MODERATELY STABLE   
============================================================

6. Decision Space, Pareto Front, and Stability Plots Generated
------------------------------------------------------------

============================================================
Found 9 Pareto optimal solutions

Most stable solution: #9
  x = (0.000977, 0.000977)
  Stability class: MODERATELY STABLE
  Stability score: 10.8752
============================================================
PASSED                      [ 25%]
bnh/py/test_z3_minimax.py::test_z3_minimax 
-------------------------------- live log call ---------------------------------
======================================================================
BNH Multi-Objective Optimization with Minimax Stability Analysis
======================================================================

Minimax Philosophy:
  - Optimizes for WORST-CASE scenario
  - Ensures robustness under uncertainty
  - Provides strong guarantees against adversarial perturbations
======================================================================

1. Lexicographic Optimization (f1 primary):
----------------------------------------------------------------------
  Solution: x=(0.000000, 0.000000)
  Objectives: f1=0.0000, f2=50.0000

2. Lexicographic Optimization (f2 primary):
----------------------------------------------------------------------
  Solution: x=(0.000000, 0.000000)
  Objectives: f1=0.0000, f2=50.0000

3. Weighted Sum Method:
----------------------------------------------------------------------
  w=(0.1,0.9): x=(3.0000,1.0000), f1=40.0000, f2=20.0000
  w=(0.2,0.8): x=(1.0000,2.8301), f1=36.0374, f2=20.7086
  w=(0.4,0.6): x=(1.0000,1.0000), f1=8.0000, f2=32.0000
  w=(0.5,0.5): x=(1.0000,1.0000), f1=8.0000, f2=32.0000
  w=(0.6,0.4): x=(1.0000,1.0000), f1=8.0000, f2=32.0000
  w=(0.8,0.2): x=(0.0000,0.0000), f1=0.0000, f2=50.0000
  w=(0.9,0.1): x=(0.1250,0.1250), f1=0.1250, f2=47.5312

======================================================================
MINIMAX STABILITY ANALYSIS
======================================================================

Solution 1: x = (0.000000, 0.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.000000
    f2 = 50.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.000004 (399.98%)
    Δf2 = 0.000000 (0.00%)

  Robustness Assessment:
    Score: 3.999756 (lower is better)
    Class: NOT ROBUST

Solution 2: x = (0.000000, 0.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.000000
    f2 = 50.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.000004 (399.98%)
    Δf2 = 0.000000 (0.00%)

  Robustness Assessment:
    Score: 3.999756 (lower is better)
    Class: NOT ROBUST

Solution 3: x = (3.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 40.000000
    f2 = 20.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.025301 (0.06%)
    Δf2 = 0.008942 (0.04%)

  Robustness Assessment:
    Score: 0.001080 (lower is better)
    Class: HIGHLY ROBUST

Solution 4: x = (1.000000, 2.830078)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 36.037369
    f2 = 20.708561

  Worst-case increases (within ε=0.001):
    Δf1 = 0.024008 (0.07%)
    Δf2 = 0.009102 (0.04%)

  Robustness Assessment:
    Score: 0.001106 (lower is better)
    Class: HIGHLY ROBUST

Solution 5: x = (1.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 8.000000
    f2 = 32.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.011312 (0.14%)
    Δf2 = 0.011309 (0.04%)

  Robustness Assessment:
    Score: 0.001767 (lower is better)
    Class: HIGHLY ROBUST

Solution 6: x = (1.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 8.000000
    f2 = 32.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.011312 (0.14%)
    Δf2 = 0.011309 (0.04%)

  Robustness Assessment:
    Score: 0.001767 (lower is better)
    Class: HIGHLY ROBUST

Solution 7: x = (1.000000, 1.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 8.000000
    f2 = 32.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.011312 (0.14%)
    Δf2 = 0.011309 (0.04%)

  Robustness Assessment:
    Score: 0.001767 (lower is better)
    Class: HIGHLY ROBUST

Solution 8: x = (0.000000, 0.000000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.000000
    f2 = 50.000000

  Worst-case increases (within ε=0.001):
    Δf1 = 0.000004 (399.98%)
    Δf2 = 0.000000 (0.00%)

  Robustness Assessment:
    Score: 3.999756 (lower is better)
    Class: NOT ROBUST

Solution 9: x = (0.125000, 0.125000)
----------------------------------------------------------------------
    Searching worst-case in 100 directions...
    Optimization-based worst-case search...
    ✓ Worst-case analysis complete

  Nominal values:
    f1 = 0.125000
    f2 = 47.531250

  Worst-case increases (within ε=0.001):
    Δf1 = 0.001417 (1.13%)
    Δf2 = 0.013782 (0.03%)

  Robustness Assessment:
    Score: 0.011630 (lower is better)
    Class: HIGHLY ROBUST

======================================================================
MINIMAX ROBUSTNESS SUMMARY
======================================================================
Sol   x1         x2         Δf1(%)     Δf2(%)     Score      Robustness        
----------------------------------------------------------------------
1     0.0000     0.0000     399.98     0.00       3.9998     NOT ROBUST        
2     0.0000     0.0000     399.98     0.00       3.9998     NOT ROBUST        
3     3.0000     1.0000     0.06       0.04       0.0011     HIGHLY ROBUST     
4     1.0000     2.8301     0.07       0.04       0.0011     HIGHLY ROBUST     
5     1.0000     1.0000     0.14       0.04       0.0018     HIGHLY ROBUST     
6     1.0000     1.0000     0.14       0.04       0.0018     HIGHLY ROBUST     
7     1.0000     1.0000     0.14       0.04       0.0018     HIGHLY ROBUST     
8     0.0000     0.0000     399.98     0.00       3.9998     NOT ROBUST        
9     0.1250     0.1250     1.13       0.03       0.0116     HIGHLY ROBUST     
======================================================================

Most Robust Solution: #3
  x = (3.000000, 1.000000)
  Worst-case score: 0.001080
  Class: HIGHLY ROBUST
======================================================================

✓ Visualization complete
PASSED                        [ 37%]
constraint_dora/py/test_z3_nonlinear.py::test_constraint_dora 
-------------------------------- live log call ---------------------------------
============================================================
METHOD 1: Z3 Solver
============================================================
Z3 Solution:
  x1 = 0.894400
  x2 = 0.447266
  f(x1, x2) = 1.527867
  Constraint check: x1^2 + x2^2 = 0.999998

============================================================
METHOD 2: Scipy (Recommended for this problem)
============================================================
Scipy Solution:
  x1 = 0.894429
  x2 = 0.447211
  f(x1, x2) = 1.527864
  Constraint check: x1^2 + x2^2 = 1.000000
  Success: True

============================================================
ANALYTICAL SOLUTION
============================================================
Analytical Solution:
  x1 = 0.894427
  x2 = 0.447214
  f(x1, x2) = 1.527864
  Constraint check: x1^2 + x2^2 = 1.000000

============================================================
EXPLANATION
============================================================
The minimum occurs at the point on the unit circle
closest to (2, 1). This is found by moving from the
origin toward (2, 1) until hitting the circle boundary.
============================================================
{'z3': (0.8944, 0.4473), 'slsqp': (0.8944, 0.4472), 'linalg': (0.8944, 0.4472)}
PASSED     [ 50%]
shekel/py/test_keras.py::test_keras 
-------------------------------- live log call ---------------------------------
Random seeds set for reproducibility (seed=42)
============================================================
Loading data from /tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples/shekel/py/shekel_meshgrid_26.csv.expected.gz...
Data shape: (456976, 5)
Columns: ['X1', 'X2', 'X3', 'X4', 'Y']

First few rows:
    X1   X2   X3   X4         Y
0  0.0  0.0  0.0  0.0 -0.321729
1  0.0  0.0  0.0  0.4 -0.367250
2  0.0  0.0  0.0  0.8 -0.397599
3  0.0  0.0  0.0  1.2 -0.400056
4  0.0  0.0  0.0  1.6 -0.374583

Features shape: (456976, 4)
Target shape: (456976,)
Target range: [-10.5363, -0.0809]

Train set size: 365580
Test set size: 91396

Creating neural network model...
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ dense (Dense)                   │ (None, 32)             │           160 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization             │ (None, 32)             │           128 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 64)             │         2,112 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_1           │ (None, 64)             │           256 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_2 (Dense)                 │ (None, 32)             │         2,080 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_2           │ (None, 32)             │           128 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_3 (Dense)                 │ (None, 16)             │           528 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_4 (Dense)                 │ (None, 1)              │            17 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 5,409 (21.13 KB)
 Trainable params: 5,153 (20.13 KB)
 Non-trainable params: 256 (1.00 KB)

None

Training model...
Initial learning rate: 0.005 (using Nadam optimizer with cosine annealing)
Epoch 1/150
715/715 - 5s - 7ms/step - loss: 0.1270 - mae: 0.1686 - mse: 0.1270 - val_loss: 0.1096 - val_mae: 0.2063 - val_mse: 0.1096 - learning_rate: 0.0050 - lr: 0.0050

Epoch 2/150
715/715 - 2s - 3ms/step - loss: 0.0715 - mae: 0.1208 - mse: 0.0715 - val_loss: 0.0769 - val_mae: 0.1451 - val_mse: 0.0769 - learning_rate: 0.0050 - lr: 0.0050

Epoch 3/150
715/715 - 2s - 3ms/step - loss: 0.0642 - mae: 0.1106 - mse: 0.0642 - val_loss: 0.0481 - val_mae: 0.1090 - val_mse: 0.0481 - learning_rate: 0.0050 - lr: 0.0050

Epoch 4/150
715/715 - 2s - 3ms/step - loss: 0.0594 - mae: 0.1053 - mse: 0.0594 - val_loss: 0.0411 - val_mae: 0.1079 - val_mse: 0.0411 - learning_rate: 0.0050 - lr: 0.0050

Epoch 5/150
715/715 - 2s - 3ms/step - loss: 0.0607 - mae: 0.1077 - mse: 0.0607 - val_loss: 0.0346 - val_mae: 0.0988 - val_mse: 0.0346 - learning_rate: 0.0050 - lr: 0.0050

Epoch 6/150
715/715 - 2s - 3ms/step - loss: 0.0584 - mae: 0.1065 - mse: 0.0584 - val_loss: 0.0306 - val_mae: 0.0829 - val_mse: 0.0306 - learning_rate: 0.0050 - lr: 0.0050

Epoch 7/150
715/715 - 2s - 3ms/step - loss: 0.0559 - mae: 0.1022 - mse: 0.0559 - val_loss: 0.0362 - val_mae: 0.0828 - val_mse: 0.0362 - learning_rate: 0.0050 - lr: 0.0050

Epoch 8/150
715/715 - 2s - 3ms/step - loss: 0.0536 - mae: 0.1001 - mse: 0.0536 - val_loss: 0.0539 - val_mae: 0.0972 - val_mse: 0.0539 - learning_rate: 0.0050 - lr: 0.0050

Epoch 9/150
715/715 - 2s - 3ms/step - loss: 0.0649 - mae: 0.1109 - mse: 0.0649 - val_loss: 0.0355 - val_mae: 0.0897 - val_mse: 0.0355 - learning_rate: 0.0050 - lr: 0.0050

Epoch 10/150
715/715 - 2s - 3ms/step - loss: 0.0512 - mae: 0.1004 - mse: 0.0512 - val_loss: 0.0444 - val_mae: 0.1036 - val_mse: 0.0444 - learning_rate: 0.0050 - lr: 0.0050

Epoch 11/150
715/715 - 2s - 3ms/step - loss: 0.0504 - mae: 0.0997 - mse: 0.0504 - val_loss: 0.0332 - val_mae: 0.0880 - val_mse: 0.0332 - learning_rate: 0.0049 - lr: 0.0049

Epoch 12/150
715/715 - 2s - 3ms/step - loss: 0.0531 - mae: 0.1004 - mse: 0.0531 - val_loss: 0.0323 - val_mae: 0.0878 - val_mse: 0.0323 - learning_rate: 0.0049 - lr: 0.0049

Epoch 13/150
715/715 - 2s - 3ms/step - loss: 0.0436 - mae: 0.0930 - mse: 0.0436 - val_loss: 0.0293 - val_mae: 0.0786 - val_mse: 0.0293 - learning_rate: 0.0049 - lr: 0.0049

Epoch 14/150
715/715 - 2s - 3ms/step - loss: 0.0431 - mae: 0.0915 - mse: 0.0431 - val_loss: 0.0312 - val_mae: 0.0850 - val_mse: 0.0312 - learning_rate: 0.0049 - lr: 0.0049

Epoch 15/150
715/715 - 2s - 3ms/step - loss: 0.0440 - mae: 0.0924 - mse: 0.0440 - val_loss: 0.0430 - val_mae: 0.1077 - val_mse: 0.0430 - learning_rate: 0.0049 - lr: 0.0049

Epoch 16/150
715/715 - 2s - 3ms/step - loss: 0.0450 - mae: 0.0922 - mse: 0.0450 - val_loss: 0.0267 - val_mae: 0.0895 - val_mse: 0.0267 - learning_rate: 0.0049 - lr: 0.0049

Epoch 17/150
715/715 - 2s - 3ms/step - loss: 0.0425 - mae: 0.0903 - mse: 0.0425 - val_loss: 0.0277 - val_mae: 0.0845 - val_mse: 0.0277 - learning_rate: 0.0049 - lr: 0.0049

Epoch 18/150
715/715 - 2s - 3ms/step - loss: 0.0427 - mae: 0.0917 - mse: 0.0427 - val_loss: 0.0423 - val_mae: 0.1226 - val_mse: 0.0423 - learning_rate: 0.0048 - lr: 0.0048

Epoch 19/150
715/715 - 2s - 3ms/step - loss: 0.0421 - mae: 0.0906 - mse: 0.0421 - val_loss: 0.0275 - val_mae: 0.0912 - val_mse: 0.0275 - learning_rate: 0.0048 - lr: 0.0048

Epoch 20/150
715/715 - 2s - 3ms/step - loss: 0.0416 - mae: 0.0894 - mse: 0.0416 - val_loss: 0.0359 - val_mae: 0.0939 - val_mse: 0.0359 - learning_rate: 0.0048 - lr: 0.0048

Epoch 21/150
715/715 - 2s - 3ms/step - loss: 0.0416 - mae: 0.0885 - mse: 0.0416 - val_loss: 0.0382 - val_mae: 0.0810 - val_mse: 0.0382 - learning_rate: 0.0048 - lr: 0.0048

Epoch 22/150
715/715 - 2s - 3ms/step - loss: 0.0410 - mae: 0.0874 - mse: 0.0410 - val_loss: 0.0215 - val_mae: 0.0731 - val_mse: 0.0215 - learning_rate: 0.0048 - lr: 0.0048

Epoch 23/150
715/715 - 2s - 3ms/step - loss: 0.0359 - mae: 0.0855 - mse: 0.0359 - val_loss: 0.0351 - val_mae: 0.0955 - val_mse: 0.0351 - learning_rate: 0.0047 - lr: 0.0047

Epoch 24/150
715/715 - 3s - 4ms/step - loss: 0.0470 - mae: 0.0931 - mse: 0.0470 - val_loss: 0.0412 - val_mae: 0.1292 - val_mse: 0.0412 - learning_rate: 0.0047 - lr: 0.0047

Epoch 25/150
715/715 - 3s - 4ms/step - loss: 0.0386 - mae: 0.0860 - mse: 0.0386 - val_loss: 0.0681 - val_mae: 0.1321 - val_mse: 0.0681 - learning_rate: 0.0047 - lr: 0.0047

Epoch 26/150
715/715 - 3s - 4ms/step - loss: 0.0377 - mae: 0.0863 - mse: 0.0377 - val_loss: 0.0303 - val_mae: 0.0908 - val_mse: 0.0303 - learning_rate: 0.0047 - lr: 0.0047

Epoch 27/150
715/715 - 3s - 4ms/step - loss: 0.0431 - mae: 0.0898 - mse: 0.0431 - val_loss: 0.0395 - val_mae: 0.1097 - val_mse: 0.0395 - learning_rate: 0.0046 - lr: 0.0046

Epoch 28/150
715/715 - 3s - 4ms/step - loss: 0.0381 - mae: 0.0865 - mse: 0.0381 - val_loss: 0.0438 - val_mae: 0.0908 - val_mse: 0.0438 - learning_rate: 0.0046 - lr: 0.0046

Epoch 29/150
715/715 - 3s - 4ms/step - loss: 0.0415 - mae: 0.0891 - mse: 0.0415 - val_loss: 0.0904 - val_mae: 0.1510 - val_mse: 0.0904 - learning_rate: 0.0046 - lr: 0.0046

Epoch 30/150
715/715 - 3s - 4ms/step - loss: 0.0376 - mae: 0.0838 - mse: 0.0376 - val_loss: 0.0298 - val_mae: 0.0856 - val_mse: 0.0298 - learning_rate: 0.0046 - lr: 0.0046

Epoch 31/150
715/715 - 3s - 4ms/step - loss: 0.0375 - mae: 0.0856 - mse: 0.0375 - val_loss: 0.0254 - val_mae: 0.0926 - val_mse: 0.0254 - learning_rate: 0.0045 - lr: 0.0045

Epoch 32/150
715/715 - 3s - 4ms/step - loss: 0.0373 - mae: 0.0865 - mse: 0.0373 - val_loss: 0.0353 - val_mae: 0.0998 - val_mse: 0.0353 - learning_rate: 0.0045 - lr: 0.0045

Epoch 33/150
715/715 - 3s - 4ms/step - loss: 0.0366 - mae: 0.0855 - mse: 0.0366 - val_loss: 0.0258 - val_mae: 0.0749 - val_mse: 0.0258 - learning_rate: 0.0045 - lr: 0.0045

Epoch 34/150
715/715 - 3s - 4ms/step - loss: 0.0352 - mae: 0.0846 - mse: 0.0352 - val_loss: 0.0238 - val_mae: 0.0671 - val_mse: 0.0238 - learning_rate: 0.0044 - lr: 0.0044

Epoch 35/150
715/715 - 3s - 4ms/step - loss: 0.0357 - mae: 0.0852 - mse: 0.0357 - val_loss: 0.0341 - val_mae: 0.0851 - val_mse: 0.0341 - learning_rate: 0.0044 - lr: 0.0044

Epoch 36/150
715/715 - 3s - 4ms/step - loss: 0.0331 - mae: 0.0829 - mse: 0.0331 - val_loss: 0.0312 - val_mae: 0.0793 - val_mse: 0.0312 - learning_rate: 0.0044 - lr: 0.0044

Epoch 37/150

Epoch 37: ReduceLROnPlateau reducing learning rate to 0.002161278622224927.
715/715 - 3s - 4ms/step - loss: 0.0291 - mae: 0.0814 - mse: 0.0291 - val_loss: 0.0234 - val_mae: 0.0826 - val_mse: 0.0234 - learning_rate: 0.0043 - lr: 0.0043

Epoch 38/150
715/715 - 3s - 4ms/step - loss: 0.0430 - mae: 0.0912 - mse: 0.0430 - val_loss: 0.0435 - val_mae: 0.1235 - val_mse: 0.0435 - learning_rate: 0.0043 - lr: 0.0043

Epoch 39/150
715/715 - 3s - 4ms/step - loss: 0.0330 - mae: 0.0826 - mse: 0.0330 - val_loss: 0.0385 - val_mae: 0.1258 - val_mse: 0.0385 - learning_rate: 0.0042 - lr: 0.0042

Epoch 40/150
715/715 - 3s - 4ms/step - loss: 0.0323 - mae: 0.0829 - mse: 0.0323 - val_loss: 0.0315 - val_mae: 0.0910 - val_mse: 0.0315 - learning_rate: 0.0042 - lr: 0.0042

Epoch 41/150
715/715 - 3s - 4ms/step - loss: 0.0350 - mae: 0.0855 - mse: 0.0350 - val_loss: 0.0355 - val_mae: 0.1018 - val_mse: 0.0355 - learning_rate: 0.0042 - lr: 0.0042

Epoch 42/150
715/715 - 3s - 4ms/step - loss: 0.0307 - mae: 0.0816 - mse: 0.0307 - val_loss: 0.0218 - val_mae: 0.0788 - val_mse: 0.0218 - learning_rate: 0.0041 - lr: 0.0041

Epoch 43/150
715/715 - 3s - 4ms/step - loss: 0.0266 - mae: 0.0800 - mse: 0.0266 - val_loss: 0.0266 - val_mae: 0.0946 - val_mse: 0.0266 - learning_rate: 0.0041 - lr: 0.0041

Epoch 44/150
715/715 - 3s - 4ms/step - loss: 0.0307 - mae: 0.0824 - mse: 0.0307 - val_loss: 0.0262 - val_mae: 0.0975 - val_mse: 0.0262 - learning_rate: 0.0041 - lr: 0.0041

Epoch 45/150
715/715 - 3s - 4ms/step - loss: 0.0302 - mae: 0.0823 - mse: 0.0302 - val_loss: 0.0219 - val_mae: 0.0870 - val_mse: 0.0219 - learning_rate: 0.0040 - lr: 0.0040

Epoch 46/150
715/715 - 3s - 4ms/step - loss: 0.0285 - mae: 0.0823 - mse: 0.0285 - val_loss: 0.0313 - val_mae: 0.0957 - val_mse: 0.0313 - learning_rate: 0.0040 - lr: 0.0040

Epoch 47/150
715/715 - 3s - 4ms/step - loss: 0.0258 - mae: 0.0795 - mse: 0.0258 - val_loss: 0.0313 - val_mae: 0.0993 - val_mse: 0.0313 - learning_rate: 0.0039 - lr: 0.0039

Epoch 48/150
715/715 - 3s - 4ms/step - loss: 0.0338 - mae: 0.0840 - mse: 0.0338 - val_loss: 0.0346 - val_mae: 0.0913 - val_mse: 0.0346 - learning_rate: 0.0039 - lr: 0.0039

Epoch 49/150
715/715 - 3s - 4ms/step - loss: 0.0254 - mae: 0.0778 - mse: 0.0254 - val_loss: 0.0244 - val_mae: 0.0852 - val_mse: 0.0244 - learning_rate: 0.0038 - lr: 0.0038

Epoch 50/150
715/715 - 3s - 4ms/step - loss: 0.0218 - mae: 0.0758 - mse: 0.0218 - val_loss: 0.0242 - val_mae: 0.0764 - val_mse: 0.0242 - learning_rate: 0.0038 - lr: 0.0038

Epoch 51/150
715/715 - 3s - 4ms/step - loss: 0.0226 - mae: 0.0763 - mse: 0.0226 - val_loss: 0.0292 - val_mae: 0.0801 - val_mse: 0.0292 - learning_rate: 0.0038 - lr: 0.0038

Epoch 52/150

Epoch 52: ReduceLROnPlateau reducing learning rate to 0.001852321671321988.
715/715 - 3s - 4ms/step - loss: 0.0248 - mae: 0.0773 - mse: 0.0248 - val_loss: 0.0360 - val_mae: 0.0993 - val_mse: 0.0360 - learning_rate: 0.0037 - lr: 0.0037

Epoch 53/150
715/715 - 3s - 4ms/step - loss: 0.0236 - mae: 0.0765 - mse: 0.0236 - val_loss: 0.0193 - val_mae: 0.0752 - val_mse: 0.0193 - learning_rate: 0.0037 - lr: 0.0037

Epoch 54/150
715/715 - 3s - 4ms/step - loss: 0.0214 - mae: 0.0749 - mse: 0.0214 - val_loss: 0.0294 - val_mae: 0.0865 - val_mse: 0.0294 - learning_rate: 0.0036 - lr: 0.0036

Epoch 55/150
715/715 - 3s - 4ms/step - loss: 0.0286 - mae: 0.0791 - mse: 0.0286 - val_loss: 0.0308 - val_mae: 0.0969 - val_mse: 0.0308 - learning_rate: 0.0036 - lr: 0.0036

Epoch 56/150
715/715 - 3s - 4ms/step - loss: 0.0246 - mae: 0.0790 - mse: 0.0246 - val_loss: 0.0379 - val_mae: 0.0864 - val_mse: 0.0379 - learning_rate: 0.0035 - lr: 0.0035

Epoch 57/150
715/715 - 3s - 4ms/step - loss: 0.0267 - mae: 0.0762 - mse: 0.0267 - val_loss: 0.0276 - val_mae: 0.0735 - val_mse: 0.0276 - learning_rate: 0.0035 - lr: 0.0035

Epoch 58/150
715/715 - 3s - 4ms/step - loss: 0.0190 - mae: 0.0728 - mse: 0.0190 - val_loss: 0.0223 - val_mae: 0.0727 - val_mse: 0.0223 - learning_rate: 0.0034 - lr: 0.0034

Epoch 59/150
715/715 - 3s - 4ms/step - loss: 0.0188 - mae: 0.0726 - mse: 0.0188 - val_loss: 0.0212 - val_mae: 0.0690 - val_mse: 0.0212 - learning_rate: 0.0034 - lr: 0.0034

Epoch 60/150
715/715 - 3s - 4ms/step - loss: 0.0205 - mae: 0.0734 - mse: 0.0205 - val_loss: 0.0258 - val_mae: 0.0774 - val_mse: 0.0258 - learning_rate: 0.0033 - lr: 0.0033

Epoch 61/150
715/715 - 3s - 4ms/step - loss: 0.0231 - mae: 0.0747 - mse: 0.0231 - val_loss: 0.0277 - val_mae: 0.0815 - val_mse: 0.0277 - learning_rate: 0.0033 - lr: 0.0033

Epoch 62/150
715/715 - 3s - 4ms/step - loss: 0.0250 - mae: 0.0763 - mse: 0.0250 - val_loss: 0.0254 - val_mae: 0.0634 - val_mse: 0.0254 - learning_rate: 0.0032 - lr: 0.0032

Epoch 63/150
715/715 - 3s - 4ms/step - loss: 0.0211 - mae: 0.0732 - mse: 0.0211 - val_loss: 0.0300 - val_mae: 0.0855 - val_mse: 0.0300 - learning_rate: 0.0032 - lr: 0.0032

Epoch 64/150
715/715 - 3s - 4ms/step - loss: 0.0196 - mae: 0.0720 - mse: 0.0196 - val_loss: 0.0234 - val_mae: 0.0697 - val_mse: 0.0234 - learning_rate: 0.0031 - lr: 0.0031

Epoch 65/150
715/715 - 3s - 4ms/step - loss: 0.0179 - mae: 0.0712 - mse: 0.0179 - val_loss: 0.0319 - val_mae: 0.0894 - val_mse: 0.0319 - learning_rate: 0.0031 - lr: 0.0031

Epoch 66/150
715/715 - 3s - 4ms/step - loss: 0.0196 - mae: 0.0720 - mse: 0.0196 - val_loss: 0.0230 - val_mae: 0.0693 - val_mse: 0.0230 - learning_rate: 0.0030 - lr: 0.0030

Epoch 67/150
715/715 - 3s - 4ms/step - loss: 0.0173 - mae: 0.0708 - mse: 0.0173 - val_loss: 0.0247 - val_mae: 0.0763 - val_mse: 0.0247 - learning_rate: 0.0030 - lr: 0.0030

Epoch 68/150

Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0014586691977456212.
715/715 - 3s - 4ms/step - loss: 0.0177 - mae: 0.0708 - mse: 0.0177 - val_loss: 0.0214 - val_mae: 0.0732 - val_mse: 0.0214 - learning_rate: 0.0029 - lr: 0.0029

Epoch 69/150
715/715 - 3s - 4ms/step - loss: 0.0186 - mae: 0.0713 - mse: 0.0186 - val_loss: 0.0202 - val_mae: 0.0751 - val_mse: 0.0202 - learning_rate: 0.0029 - lr: 0.0029

Epoch 70/150
715/715 - 3s - 4ms/step - loss: 0.0250 - mae: 0.0752 - mse: 0.0250 - val_loss: 0.0186 - val_mae: 0.0765 - val_mse: 0.0186 - learning_rate: 0.0028 - lr: 0.0028

Epoch 71/150
715/715 - 3s - 4ms/step - loss: 0.0234 - mae: 0.0723 - mse: 0.0234 - val_loss: 0.0197 - val_mae: 0.0641 - val_mse: 0.0197 - learning_rate: 0.0028 - lr: 0.0028

Epoch 72/150
715/715 - 3s - 4ms/step - loss: 0.0206 - mae: 0.0711 - mse: 0.0206 - val_loss: 0.0174 - val_mae: 0.0619 - val_mse: 0.0174 - learning_rate: 0.0027 - lr: 0.0027

Epoch 73/150
715/715 - 3s - 4ms/step - loss: 0.0197 - mae: 0.0715 - mse: 0.0197 - val_loss: 0.0249 - val_mae: 0.0687 - val_mse: 0.0249 - learning_rate: 0.0027 - lr: 0.0027

Epoch 74/150
715/715 - 3s - 4ms/step - loss: 0.0161 - mae: 0.0691 - mse: 0.0161 - val_loss: 0.0204 - val_mae: 0.0715 - val_mse: 0.0204 - learning_rate: 0.0026 - lr: 0.0026

Epoch 75/150
715/715 - 3s - 4ms/step - loss: 0.0165 - mae: 0.0692 - mse: 0.0165 - val_loss: 0.0176 - val_mae: 0.0626 - val_mse: 0.0176 - learning_rate: 0.0026 - lr: 0.0026

Epoch 76/150
715/715 - 3s - 4ms/step - loss: 0.0171 - mae: 0.0693 - mse: 0.0171 - val_loss: 0.0234 - val_mae: 0.0710 - val_mse: 0.0234 - learning_rate: 0.0025 - lr: 0.0025

Epoch 77/150
715/715 - 3s - 4ms/step - loss: 0.0169 - mae: 0.0691 - mse: 0.0169 - val_loss: 0.0188 - val_mae: 0.0682 - val_mse: 0.0188 - learning_rate: 0.0024 - lr: 0.0024

Epoch 78/150
715/715 - 3s - 4ms/step - loss: 0.0175 - mae: 0.0695 - mse: 0.0175 - val_loss: 0.0207 - val_mae: 0.0680 - val_mse: 0.0207 - learning_rate: 0.0024 - lr: 0.0024

Epoch 79/150
715/715 - 3s - 4ms/step - loss: 0.0157 - mae: 0.0680 - mse: 0.0157 - val_loss: 0.0176 - val_mae: 0.0592 - val_mse: 0.0176 - learning_rate: 0.0023 - lr: 0.0023

Epoch 80/150
715/715 - 3s - 4ms/step - loss: 0.0160 - mae: 0.0680 - mse: 0.0160 - val_loss: 0.0190 - val_mae: 0.0587 - val_mse: 0.0190 - learning_rate: 0.0023 - lr: 0.0023

Epoch 81/150
715/715 - 3s - 4ms/step - loss: 0.0175 - mae: 0.0687 - mse: 0.0175 - val_loss: 0.0163 - val_mae: 0.0584 - val_mse: 0.0163 - learning_rate: 0.0022 - lr: 0.0022

Epoch 82/150
715/715 - 3s - 4ms/step - loss: 0.0147 - mae: 0.0670 - mse: 0.0147 - val_loss: 0.0145 - val_mae: 0.0607 - val_mse: 0.0145 - learning_rate: 0.0022 - lr: 0.0022

Epoch 83/150
715/715 - 3s - 4ms/step - loss: 0.0151 - mae: 0.0671 - mse: 0.0151 - val_loss: 0.0145 - val_mae: 0.0590 - val_mse: 0.0145 - learning_rate: 0.0021 - lr: 0.0021

Epoch 84/150
715/715 - 3s - 4ms/step - loss: 0.0162 - mae: 0.0675 - mse: 0.0162 - val_loss: 0.0161 - val_mae: 0.0557 - val_mse: 0.0161 - learning_rate: 0.0021 - lr: 0.0021

Epoch 85/150
715/715 - 3s - 4ms/step - loss: 0.0153 - mae: 0.0669 - mse: 0.0153 - val_loss: 0.0162 - val_mae: 0.0623 - val_mse: 0.0162 - learning_rate: 0.0020 - lr: 0.0020

Epoch 86/150
715/715 - 3s - 4ms/step - loss: 0.0153 - mae: 0.0669 - mse: 0.0153 - val_loss: 0.0159 - val_mae: 0.0603 - val_mse: 0.0159 - learning_rate: 0.0020 - lr: 0.0020

Epoch 87/150
715/715 - 3s - 4ms/step - loss: 0.0139 - mae: 0.0658 - mse: 0.0139 - val_loss: 0.0139 - val_mae: 0.0592 - val_mse: 0.0139 - learning_rate: 0.0019 - lr: 0.0019

Epoch 88/150
715/715 - 3s - 4ms/step - loss: 0.0138 - mae: 0.0656 - mse: 0.0138 - val_loss: 0.0151 - val_mae: 0.0565 - val_mse: 0.0151 - learning_rate: 0.0019 - lr: 0.0019

Epoch 89/150
715/715 - 3s - 4ms/step - loss: 0.0146 - mae: 0.0661 - mse: 0.0146 - val_loss: 0.0156 - val_mae: 0.0533 - val_mse: 0.0156 - learning_rate: 0.0018 - lr: 0.0018

Epoch 90/150
715/715 - 3s - 4ms/step - loss: 0.0144 - mae: 0.0658 - mse: 0.0144 - val_loss: 0.0146 - val_mae: 0.0578 - val_mse: 0.0146 - learning_rate: 0.0018 - lr: 0.0018

Epoch 91/150
715/715 - 3s - 4ms/step - loss: 0.0143 - mae: 0.0656 - mse: 0.0143 - val_loss: 0.0144 - val_mae: 0.0532 - val_mse: 0.0144 - learning_rate: 0.0017 - lr: 0.0017

Epoch 92/150
715/715 - 3s - 4ms/step - loss: 0.0135 - mae: 0.0649 - mse: 0.0135 - val_loss: 0.0141 - val_mae: 0.0575 - val_mse: 0.0141 - learning_rate: 0.0017 - lr: 0.0017

Epoch 93/150
715/715 - 3s - 4ms/step - loss: 0.0131 - mae: 0.0646 - mse: 0.0131 - val_loss: 0.0134 - val_mae: 0.0542 - val_mse: 0.0134 - learning_rate: 0.0016 - lr: 0.0016

Epoch 94/150
715/715 - 3s - 4ms/step - loss: 0.0130 - mae: 0.0643 - mse: 0.0130 - val_loss: 0.0145 - val_mae: 0.0549 - val_mse: 0.0145 - learning_rate: 0.0016 - lr: 0.0016

Epoch 95/150
715/715 - 3s - 4ms/step - loss: 0.0133 - mae: 0.0644 - mse: 0.0133 - val_loss: 0.0141 - val_mae: 0.0539 - val_mse: 0.0141 - learning_rate: 0.0015 - lr: 0.0015

Epoch 96/150
715/715 - 3s - 4ms/step - loss: 0.0135 - mae: 0.0644 - mse: 0.0135 - val_loss: 0.0122 - val_mae: 0.0509 - val_mse: 0.0122 - learning_rate: 0.0015 - lr: 0.0015

Epoch 97/150
715/715 - 3s - 4ms/step - loss: 0.0131 - mae: 0.0640 - mse: 0.0131 - val_loss: 0.0116 - val_mae: 0.0530 - val_mse: 0.0116 - learning_rate: 0.0014 - lr: 0.0014

Epoch 98/150
715/715 - 3s - 4ms/step - loss: 0.0127 - mae: 0.0636 - mse: 0.0127 - val_loss: 0.0117 - val_mae: 0.0516 - val_mse: 0.0117 - learning_rate: 0.0014 - lr: 0.0014

Epoch 99/150
715/715 - 3s - 4ms/step - loss: 0.0125 - mae: 0.0634 - mse: 0.0125 - val_loss: 0.0116 - val_mae: 0.0510 - val_mse: 0.0116 - learning_rate: 0.0013 - lr: 0.0013

Epoch 100/150
715/715 - 3s - 4ms/step - loss: 0.0125 - mae: 0.0633 - mse: 0.0125 - val_loss: 0.0109 - val_mae: 0.0497 - val_mse: 0.0109 - learning_rate: 0.0013 - lr: 0.0013

Epoch 101/150
715/715 - 3s - 4ms/step - loss: 0.0121 - mae: 0.0630 - mse: 0.0121 - val_loss: 0.0115 - val_mae: 0.0494 - val_mse: 0.0115 - learning_rate: 0.0013 - lr: 0.0013

Epoch 102/150
715/715 - 3s - 4ms/step - loss: 0.0123 - mae: 0.0629 - mse: 0.0123 - val_loss: 0.0111 - val_mae: 0.0496 - val_mse: 0.0111 - learning_rate: 0.0012 - lr: 0.0012

Epoch 103/150
715/715 - 3s - 4ms/step - loss: 0.0122 - mae: 0.0628 - mse: 0.0122 - val_loss: 0.0108 - val_mae: 0.0490 - val_mse: 0.0108 - learning_rate: 0.0012 - lr: 0.0012

Epoch 104/150
715/715 - 3s - 4ms/step - loss: 0.0120 - mae: 0.0625 - mse: 0.0120 - val_loss: 0.0110 - val_mae: 0.0483 - val_mse: 0.0110 - learning_rate: 0.0011 - lr: 0.0011

Epoch 105/150
715/715 - 3s - 4ms/step - loss: 0.0118 - mae: 0.0622 - mse: 0.0118 - val_loss: 0.0110 - val_mae: 0.0484 - val_mse: 0.0110 - learning_rate: 0.0011 - lr: 0.0011

Epoch 106/150
715/715 - 3s - 4ms/step - loss: 0.0119 - mae: 0.0622 - mse: 0.0119 - val_loss: 0.0104 - val_mae: 0.0474 - val_mse: 0.0104 - learning_rate: 0.0010 - lr: 0.0010

Epoch 107/150
715/715 - 3s - 4ms/step - loss: 0.0115 - mae: 0.0619 - mse: 0.0115 - val_loss: 0.0103 - val_mae: 0.0470 - val_mse: 0.0103 - learning_rate: 9.8930e-04 - lr: 9.8930e-04

Epoch 108/150
715/715 - 3s - 4ms/step - loss: 0.0115 - mae: 0.0617 - mse: 0.0115 - val_loss: 0.0103 - val_mae: 0.0465 - val_mse: 0.0103 - learning_rate: 9.4794e-04 - lr: 9.4794e-04

Epoch 109/150
715/715 - 3s - 4ms/step - loss: 0.0114 - mae: 0.0616 - mse: 0.0114 - val_loss: 0.0099 - val_mae: 0.0464 - val_mse: 0.0099 - learning_rate: 9.0726e-04 - lr: 9.0726e-04

Epoch 110/150
715/715 - 3s - 4ms/step - loss: 0.0114 - mae: 0.0614 - mse: 0.0114 - val_loss: 0.0101 - val_mae: 0.0462 - val_mse: 0.0101 - learning_rate: 8.6728e-04 - lr: 8.6728e-04

Epoch 111/150
715/715 - 3s - 4ms/step - loss: 0.0113 - mae: 0.0613 - mse: 0.0113 - val_loss: 0.0093 - val_mae: 0.0453 - val_mse: 0.0093 - learning_rate: 8.2801e-04 - lr: 8.2801e-04

Epoch 112/150
715/715 - 3s - 4ms/step - loss: 0.0113 - mae: 0.0612 - mse: 0.0113 - val_loss: 0.0091 - val_mae: 0.0450 - val_mse: 0.0091 - learning_rate: 7.8947e-04 - lr: 7.8947e-04

Epoch 113/150
715/715 - 3s - 4ms/step - loss: 0.0111 - mae: 0.0610 - mse: 0.0111 - val_loss: 0.0087 - val_mae: 0.0443 - val_mse: 0.0087 - learning_rate: 7.5169e-04 - lr: 7.5169e-04

Epoch 114/150
715/715 - 3s - 4ms/step - loss: 0.0111 - mae: 0.0609 - mse: 0.0111 - val_loss: 0.0088 - val_mae: 0.0440 - val_mse: 0.0088 - learning_rate: 7.1468e-04 - lr: 7.1468e-04

Epoch 115/150
715/715 - 3s - 4ms/step - loss: 0.0110 - mae: 0.0607 - mse: 0.0110 - val_loss: 0.0083 - val_mae: 0.0433 - val_mse: 0.0083 - learning_rate: 6.7844e-04 - lr: 6.7844e-04

Epoch 116/150
715/715 - 3s - 4ms/step - loss: 0.0110 - mae: 0.0606 - mse: 0.0110 - val_loss: 0.0082 - val_mae: 0.0426 - val_mse: 0.0082 - learning_rate: 6.4301e-04 - lr: 6.4301e-04

Epoch 117/150
715/715 - 3s - 4ms/step - loss: 0.0108 - mae: 0.0604 - mse: 0.0108 - val_loss: 0.0079 - val_mae: 0.0423 - val_mse: 0.0079 - learning_rate: 6.0839e-04 - lr: 6.0839e-04

Epoch 118/150
715/715 - 3s - 4ms/step - loss: 0.0108 - mae: 0.0603 - mse: 0.0108 - val_loss: 0.0080 - val_mae: 0.0417 - val_mse: 0.0080 - learning_rate: 5.7460e-04 - lr: 5.7460e-04

Epoch 119/150
715/715 - 3s - 4ms/step - loss: 0.0107 - mae: 0.0601 - mse: 0.0107 - val_loss: 0.0079 - val_mae: 0.0415 - val_mse: 0.0079 - learning_rate: 5.4166e-04 - lr: 5.4166e-04

Epoch 120/150
715/715 - 3s - 4ms/step - loss: 0.0106 - mae: 0.0600 - mse: 0.0106 - val_loss: 0.0076 - val_mae: 0.0409 - val_mse: 0.0076 - learning_rate: 5.0957e-04 - lr: 5.0957e-04

Epoch 121/150
715/715 - 3s - 4ms/step - loss: 0.0106 - mae: 0.0598 - mse: 0.0106 - val_loss: 0.0074 - val_mae: 0.0405 - val_mse: 0.0074 - learning_rate: 4.7836e-04 - lr: 4.7836e-04

Epoch 122/150
715/715 - 3s - 4ms/step - loss: 0.0105 - mae: 0.0597 - mse: 0.0105 - val_loss: 0.0075 - val_mae: 0.0402 - val_mse: 0.0075 - learning_rate: 4.4804e-04 - lr: 4.4804e-04

Epoch 123/150
715/715 - 3s - 4ms/step - loss: 0.0104 - mae: 0.0595 - mse: 0.0104 - val_loss: 0.0072 - val_mae: 0.0398 - val_mse: 0.0072 - learning_rate: 4.1861e-04 - lr: 4.1861e-04

Epoch 124/150
715/715 - 3s - 4ms/step - loss: 0.0104 - mae: 0.0595 - mse: 0.0104 - val_loss: 0.0070 - val_mae: 0.0393 - val_mse: 0.0070 - learning_rate: 3.9010e-04 - lr: 3.9010e-04

Epoch 125/150
715/715 - 3s - 4ms/step - loss: 0.0103 - mae: 0.0593 - mse: 0.0103 - val_loss: 0.0069 - val_mae: 0.0390 - val_mse: 0.0069 - learning_rate: 3.6252e-04 - lr: 3.6252e-04

Epoch 126/150
715/715 - 3s - 4ms/step - loss: 0.0103 - mae: 0.0592 - mse: 0.0103 - val_loss: 0.0068 - val_mae: 0.0387 - val_mse: 0.0068 - learning_rate: 3.3587e-04 - lr: 3.3587e-04

Epoch 127/150
715/715 - 3s - 4ms/step - loss: 0.0102 - mae: 0.0591 - mse: 0.0102 - val_loss: 0.0066 - val_mae: 0.0384 - val_mse: 0.0066 - learning_rate: 3.1017e-04 - lr: 3.1017e-04

Epoch 128/150
715/715 - 3s - 4ms/step - loss: 0.0102 - mae: 0.0590 - mse: 0.0102 - val_loss: 0.0065 - val_mae: 0.0382 - val_mse: 0.0065 - learning_rate: 2.8543e-04 - lr: 2.8543e-04

Epoch 129/150
715/715 - 3s - 4ms/step - loss: 0.0102 - mae: 0.0588 - mse: 0.0102 - val_loss: 0.0064 - val_mae: 0.0380 - val_mse: 0.0064 - learning_rate: 2.6167e-04 - lr: 2.6167e-04

Epoch 130/150
715/715 - 3s - 4ms/step - loss: 0.0101 - mae: 0.0587 - mse: 0.0101 - val_loss: 0.0063 - val_mae: 0.0379 - val_mse: 0.0063 - learning_rate: 2.3888e-04 - lr: 2.3888e-04

Epoch 131/150
715/715 - 3s - 4ms/step - loss: 0.0101 - mae: 0.0586 - mse: 0.0101 - val_loss: 0.0062 - val_mae: 0.0377 - val_mse: 0.0062 - learning_rate: 2.1709e-04 - lr: 2.1709e-04

Epoch 132/150
715/715 - 3s - 4ms/step - loss: 0.0100 - mae: 0.0586 - mse: 0.0100 - val_loss: 0.0060 - val_mae: 0.0375 - val_mse: 0.0060 - learning_rate: 1.9630e-04 - lr: 1.9630e-04

Epoch 133/150
715/715 - 3s - 4ms/step - loss: 0.0100 - mae: 0.0585 - mse: 0.0100 - val_loss: 0.0060 - val_mae: 0.0374 - val_mse: 0.0060 - learning_rate: 1.7652e-04 - lr: 1.7652e-04

Epoch 134/150
715/715 - 3s - 4ms/step - loss: 0.0100 - mae: 0.0584 - mse: 0.0100 - val_loss: 0.0059 - val_mae: 0.0373 - val_mse: 0.0059 - learning_rate: 1.5776e-04 - lr: 1.5776e-04

Epoch 135/150
715/715 - 3s - 4ms/step - loss: 0.0099 - mae: 0.0583 - mse: 0.0099 - val_loss: 0.0058 - val_mae: 0.0372 - val_mse: 0.0058 - learning_rate: 1.4003e-04 - lr: 1.4003e-04

Epoch 136/150
715/715 - 3s - 4ms/step - loss: 0.0099 - mae: 0.0582 - mse: 0.0099 - val_loss: 0.0057 - val_mae: 0.0370 - val_mse: 0.0057 - learning_rate: 1.2333e-04 - lr: 1.2333e-04

Epoch 137/150
715/715 - 3s - 4ms/step - loss: 0.0099 - mae: 0.0582 - mse: 0.0099 - val_loss: 0.0056 - val_mae: 0.0368 - val_mse: 0.0056 - learning_rate: 1.0768e-04 - lr: 1.0768e-04

Epoch 138/150
715/715 - 3s - 4ms/step - loss: 0.0099 - mae: 0.0581 - mse: 0.0099 - val_loss: 0.0055 - val_mae: 0.0367 - val_mse: 0.0055 - learning_rate: 9.3075e-05 - lr: 9.3075e-05

Epoch 139/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0580 - mse: 0.0098 - val_loss: 0.0054 - val_mae: 0.0366 - val_mse: 0.0054 - learning_rate: 7.9526e-05 - lr: 7.9526e-05

Epoch 140/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0580 - mse: 0.0098 - val_loss: 0.0054 - val_mae: 0.0365 - val_mse: 0.0054 - learning_rate: 6.7040e-05 - lr: 6.7040e-05

Epoch 141/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0579 - mse: 0.0098 - val_loss: 0.0053 - val_mae: 0.0365 - val_mse: 0.0053 - learning_rate: 5.5620e-05 - lr: 5.5620e-05

Epoch 142/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0579 - mse: 0.0098 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 4.5273e-05 - lr: 4.5273e-05

Epoch 143/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0578 - mse: 0.0098 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 3.6003e-05 - lr: 3.6003e-05

Epoch 144/150
715/715 - 3s - 4ms/step - loss: 0.0098 - mae: 0.0578 - mse: 0.0098 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 2.7814e-05 - lr: 2.7814e-05

Epoch 145/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0577 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 2.0709e-05 - lr: 2.0709e-05

Epoch 146/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0577 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 1.4693e-05 - lr: 1.4693e-05

Epoch 147/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0577 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 9.7661e-06 - lr: 9.7661e-06

Epoch 148/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0577 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 5.9322e-06 - lr: 5.9322e-06

Epoch 149/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0576 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 3.1925e-06 - lr: 3.1925e-06

Epoch 150/150
715/715 - 3s - 4ms/step - loss: 0.0097 - mae: 0.0576 - mse: 0.0097 - val_loss: 0.0053 - val_mae: 0.0364 - val_mse: 0.0053 - learning_rate: 1.5482e-06 - lr: 1.5482e-06

Restoring model weights from the end of the best epoch: 144.

============================================================
Model Evaluation:
Test Loss (MSE): 0.005302
Test MAE: 0.036418
Test MSE: 0.005302

Metrics on original scale:
MSE: 0.000160
RMSE: 0.012649
MAE: 0.006326
R² Score: 0.994517
Correlation Coefficient: 0.997259

============================================================
Testing at known minimum [4, 4, 4, 4]:
Predicted: -8.6552
Expected: ~-10.5363

Training history plot saved as 'training_history.png'
Ignoring fixed x limits to fulfill fixed data aspect with adjustable data limits.
Ignoring fixed x limits to fulfill fixed data aspect with adjustable data limits.

Correlation plot saved as 'correlation_plot.png'

Model saved to shekel_model.keras
Scalers saved to scaler_X.pkl and scaler_y.pkl

============================================================
Training complete!
PASSED                               [ 62%]
shekel/py/test_keras2onnx.py::test_keras2onnx 
-------------------------------- live log call ---------------------------------
Using tensorflow=2.20.0, onnx=1.17.0, tf2onnx=1.16.1/15c810
Using opset <onnx, 13>
Computed 0 values for constant folding
Optimizing ONNX model
After optimization: Identity -2 (2->0)
PASSED                     [ 75%]
shekel/py/test_shgo_keras.py::test_shgo_nn 
-------------------------------- live log call ---------------------------------
============================================================

============================================================
Testing trained model on new points:
============================================================

============================================================
Comparing Model Predictions vs Actual Function
============================================================
Model loaded from shekel_model_expected.keras
Scalers loaded

Point                                    Actual       Predicted    Error     
--------------------------------------------------------------------------------
['4.00', '4.00', '4.00', '4.00']         -10.536284   -8.655185    1.881099  
['1.00', '1.00', '1.00', '1.00']         -5.128471    -3.301217    1.827254  
['8.00', '8.00', '8.00', '8.00']         -5.175617    -3.962699    1.212918  
['3.50', '4.20', '3.80', '4.10']         -2.705209    -2.463345    0.241863  
['6.00', '5.50', '6.20', '5.80']         -1.774179    -1.777872    0.003693  

--------------------------------------------------------------------------------
Mean Absolute Error (MAE): 1.033366
Root Mean Squared Error (RMSE): 1.296694
Max Error: 1.881099

================================================================================
COMPARING SHGO OPTIMIZATION: ACTUAL FUNCTION vs NEURAL NETWORK MODEL
================================================================================

[1/2] Running SHGO with ACTUAL Shekel function...
--------------------------------------------------------------------------------

[2/2] Running SHGO with NEURAL NETWORK model...
--------------------------------------------------------------------------------
Model loaded from shekel_model_expected.keras
Scalers loaded

============================================================
Optimizing with Neural Network Model using SHGO
============================================================
[Elapsed time: 83.326] Elapsed CPU time: 85.085 seconds

Optimization Results (Model):
Success: True
Message: Optimization terminated successfully.

Optimal solution (x*):
  x = [3.75      3.75      3.7497237 3.75     ]

Optimal function value (f(x*)):
  f(x*) = -3.7842361927

Number of function evaluations: 1059
Number of iterations: 5

================================================================================
COMPARISON RESULTS
================================================================================

Metric                         Actual Function      NN Model             Difference     
-------------------------------------------------------------------------------------
Optimal X:                    
  x[0]                         4.000747             3.750000             0.250747       
  x[1]                         3.999509             3.750000             0.249509       
  x[2]                         4.000747             3.749724             0.251023       
  x[3]                         3.999509             3.750000             0.249509       

Optimal f(x):                 
  Function result             -10.5364431535       -3.7842361927        6.7522069608   
  Actual Shekel at x*         -10.5364431535       -3.3562372557        7.1802058978   

Performance:                  
  Function evaluations        2254                 1059                 1195           
  Iterations                  5                    5                    0              
  Success                     True                 True                

================================================================================
SUMMARY
================================================================================
Distance between optimal solutions (L2 norm): 0.500396
Difference in actual function values: 7.1802058978
✗ Neural network model solution differs significantly from actual optimum.
⚡ Speedup: 2.13x fewer function evaluations with NN model
PASSED                        [ 87%]
shekel/py/test_shgo_shekel_keras2onnx.py::test_shgo_shekel_onnx 
-------------------------------- live log call ---------------------------------
Using tensorflow=2.20.0, onnx=1.17.0, tf2onnx=1.16.1/15c810
Using opset <onnx, 13>
Computed 0 values for constant folding
Optimizing ONNX model
After optimization: Identity -2 (2->0)
============================================================

============================================================
Testing trained ONNX model on new points:
============================================================

============================================================
Comparing Model Predictions vs Actual Function
============================================================
ONNX model loaded from shekel_model.onnx
Scalers loaded

Point                                    Actual       Predicted    Error     
--------------------------------------------------------------------------------
['4.00', '4.00', '4.00', '4.00']         -10.536284   -8.655186    1.881098  
['1.00', '1.00', '1.00', '1.00']         -5.128471    -3.301217    1.827254  
['8.00', '8.00', '8.00', '8.00']         -5.175617    -3.962699    1.212919  
['3.50', '4.20', '3.80', '4.10']         -2.705209    -2.463345    0.241863  
['6.00', '5.50', '6.20', '5.80']         -1.774179    -1.777872    0.003693  

--------------------------------------------------------------------------------
Mean Absolute Error (MAE): 1.033365
Root Mean Squared Error (RMSE): 1.296693
Max Error: 1.881098

================================================================================
COMPARING SHGO OPTIMIZATION: ACTUAL FUNCTION vs ONNX NEURAL NETWORK MODEL
================================================================================

[1/2] Running SHGO with ACTUAL Shekel function...
--------------------------------------------------------------------------------

[2/2] Running SHGO with ONNX NEURAL NETWORK model...
--------------------------------------------------------------------------------
ONNX model loaded from shekel_model.onnx
Scalers loaded

============================================================
Optimizing with ONNX Neural Network Model using SHGO
============================================================
[Elapsed time: 0.438] Elapsed CPU time: 0.510 seconds

Optimization Results (ONNX Model):
Success: True
Message: Optimization terminated successfully.

Optimal solution (x*):
  x = [3.75      3.75      3.7499218 3.75     ]

Optimal function value (f(x*)):
  f(x*) = -3.7852702141

Number of function evaluations: 1052
Number of iterations: 5

================================================================================
COMPARISON RESULTS
================================================================================

Metric                         Actual Function      NN Model             Difference     
-------------------------------------------------------------------------------------
Optimal X:                    
  x[0]                         4.000747             3.750000             0.250747       
  x[1]                         3.999509             3.750000             0.249509       
  x[2]                         4.000747             3.749922             0.250825       
  x[3]                         3.999509             3.750000             0.249509       

Optimal f(x):                 
  Function result             -10.5364431535       -3.7852702141        6.7511729394   
  Actual Shekel at x*         -10.5364431535       -3.3570769339        7.1793662196   

Performance:                  
  Function evaluations        2254                 1052                 1202           
  Iterations                  5                    5                    0              
  Success                     True                 True                

================================================================================
SUMMARY
================================================================================
Distance between optimal solutions (L2 norm): 0.500297
Difference in actual function values: 7.1793662196
Distance from [4,4,4,4]             0.001264             0.500039            
✗ ONNX neural network model solution differs significantly from actual optimum.
⚡ Speedup: 2.14x fewer function evaluations with ONNX NN model
PASSED   [100%]

=============================== warnings summary ===============================
../../../../../usr/lib/python3/dist-packages/z3/z3core.py:5
  /usr/lib/python3/dist-packages/z3/z3core.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    import pkg_resources

bnh/py/test_z3.py::test_z3
bnh/py/test_z3_gradient.py::test_z3_gradient
bnh/py/test_z3_minimax.py::test_z3_minimax
constraint_dora/py/test_z3_nonlinear.py::test_constraint_dora
shekel/py/test_keras.py::test_keras
shekel/py/test_keras2onnx.py::test_keras2onnx
shekel/py/test_shgo_keras.py::test_shgo_nn
shekel/py/test_shgo_shekel_keras2onnx.py::test_shgo_shekel_onnx
  /usr/lib/python3/dist-packages/py/_process/forkedfunc.py:45: DeprecationWarning: This process (pid=494339) is multi-threaded, use of fork() may lead to deadlocks in the child.
    pid = os.fork()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========== 8 passed, 18 deselected, 9 warnings in 563.58s (0:09:23) ===========
env PYTHONDONTWRITEBYTECODE=1 /usr/bin/pytest -v -s -m "not forked" /tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples
/usr/lib/python3/dist-packages/_pytest/config/__init__.py:482: PytestConfigWarning: pytest-catchlog plugin has been merged into the core, please remove it from your requirements.
  warnings.warn(
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-7.4.4, pluggy-1.4.0 -- /usr/bin/python3
cachedir: .pytest_cache
rootdir: /tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples
plugins: anyio-4.12.0, catchlog-1.2.2, typeguard-4.4.4, forked-1.6.0, mock-3.12.0
collecting ... 
----------------------------- live log collection ------------------------------
NumExpr defaulting to 4 threads.
collected 26 items / 8 deselected / 18 selected

bnh/py/test_nsga2.py::test_nsga2 
## Problem definition
```
"Boundary definition"
    def __init__(self):
        super().__init__(n_var=2, n_obj=2, n_ieq_constr=2, vtype=float)
        self.xl = np.zeros(self.n_var)
        self.xu = np.array([5.0, 3.0])

"Functions (F) and inequality constraints (G) definitions"
    def _evaluate(self, x, out, *args, **kwargs):
        f1 = 4 * x[:, 0] ** 2 + 4 * x[:, 1] ** 2
        f2 = (x[:, 0] - 5) ** 2 + (x[:, 1] - 5) ** 2
        g1 = (1 / 25) * ((x[:, 0] - 5) ** 2 + x[:, 1] ** 2 - 25)
        g2 = -1 / 7.7 * ((x[:, 0] - 8) ** 2 + (x[:, 1] + 3) ** 2 - 7.7)

        out["F"] = anp.column_stack([f1, f2])
        out["G"] = anp.column_stack([g1, g2])

"Expected Pareto front"
    def _calc_pareto_front(self, n_points=100):
        x1 = np.linspace(0, 5, n_points)
        x2 = np.linspace(0, 5, n_points)
        x2[x1 >= 3] = 3

        X = np.column_stack([x1, x2])
        return self.evaluate(X, return_values_of=["F"])

```
PASSED
c3dtlz4/py/test_gpsampler.py::test_gpsampler [I 2026-01-17 14:31:14,276] A new study created in RDB with name: my_study
[I 2026-01-17 14:31:14,362] Trial 0 finished with values: [1.2569645750397653, 4.4185249455598885e-43] and parameters: {'x0': 0.3745401188473625, 'x1': 0.9507143064099162, 'x2': 0.7319939418114051}.
[I 2026-01-17 14:31:14,430] Trial 1 finished with values: [1.236662945761789, 1.0145778959154135e-22] and parameters: {'x0': 0.5986584841970366, 'x1': 0.15601864044243652, 'x2': 0.15599452033620265}.
[I 2026-01-17 14:31:14,495] Trial 2 finished with values: [1.1443092153344159, 4.571144650098741e-124] and parameters: {'x0': 0.05808361216819946, 'x1': 0.8661761457749352, 'x2': 0.6011150117432088}.
[I 2026-01-17 14:31:14,579] Trial 3 finished with values: [1.4506544962685188, 2.3198580035410804e-15] and parameters: {'x0': 0.7080725777960455, 'x1': 0.020584494295802447, 'x2': 0.9699098521619943}.
[I 2026-01-17 14:31:14,645] Trial 4 finished with values: [1.1839841387381274, 2.017890472548048e-08] and parameters: {'x0': 0.8324426408004217, 'x1': 0.21233911067827616, 'x2': 0.18182496720710062}.
[I 2026-01-17 14:31:14,740] Trial 5 finished with values: [1.0389339803486743, 3.578564831812836e-74] and parameters: {'x0': 0.18340450985343382, 'x1': 0.3042422429595377, 'x2': 0.5247564316322378}.
[I 2026-01-17 14:31:14,825] Trial 6 finished with values: [1.0560963419602245, 5.789891256550866e-37] and parameters: {'x0': 0.43194501864211576, 'x1': 0.2912291401980419, 'x2': 0.6118528947223795}.
[I 2026-01-17 14:31:14,896] Trial 7 finished with values: [1.0610630040604079, 4.7574085951972936e-86] and parameters: {'x0': 0.13949386065204183, 'x1': 0.29214464853521815, 'x2': 0.3663618432936917}.
[I 2026-01-17 14:31:14,961] Trial 8 finished with values: [1.171521166079494, 1.4723771522333772e-34] and parameters: {'x0': 0.45606998421703593, 'x1': 0.7851759613930136, 'x2': 0.19967378215835974}.
[I 2026-01-17 14:31:15,026] Trial 9 finished with values: [1.2142476806598177, 2.491970914729005e-29] and parameters: {'x0': 0.5142344384136116, 'x1': 0.5924145688620425, 'x2': 0.046450412719997725}.
[I 2026-01-17 14:31:19,829] Trial 10 finished with values: [1.5, 3.249132054563685e-24] and parameters: {'x0': 0.5772920306987445, 'x1': 1.0, 'x2': 1.0}.
[I 2026-01-17 14:31:20,265] Trial 11 finished with values: [1.4810590572975793, 2.2471687400140787e-21] and parameters: {'x0': 0.616381271404474, 'x1': 0.0, 'x2': 0.9806860277744498}.
[I 2026-01-17 14:31:20,745] Trial 12 finished with values: [1.3962865006473393, 0.0] and parameters: {'x0': 0.0, 'x1': 0.0, 'x2': 0.8824741829814651}.
[I 2026-01-17 14:31:21,233] Trial 13 finished with values: [1.4319600251053506, 4.613008316428876e-65] and parameters: {'x0': 0.22548587130879116, 'x1': 0.0, 'x2': 0.9265677262819476}.
[I 2026-01-17 14:31:21,683] Trial 14 finished with values: [1.4657790846195586, 3.1399921722830447e-18] and parameters: {'x0': 0.6627464371294791, 'x1': 1.0, 'x2': 0.9645202736367473}.
[I 2026-01-17 14:31:22,110] Trial 15 finished with values: [1.445114846117425, 2.5479755591876564e-80] and parameters: {'x0': 0.1586725240275914, 'x1': 1.0, 'x2': 0.9417180617966907}.
[I 2026-01-17 14:31:22,449] Trial 16 finished with values: [1.4458575475275077, 6.445293888426763e-57] and parameters: {'x0': 0.2719756187229194, 'x1': 1.0, 'x2': 0.9425579595120934}.
[I 2026-01-17 14:31:22,973] Trial 17 finished with values: [1.255194971651199, 6.912014307771002e-107] and parameters: {'x0': 0.08618818508327279, 'x1': 0.3403540168938965, 'x2': 0.9792787620261143}.
[I 2026-01-17 14:31:23,669] Trial 18 finished with values: [1.4448556966461257, 0.0] and parameters: {'x0': 0.0, 'x1': 0.9998164559591163, 'x2': 0.9416324342692624}.
[I 2026-01-17 14:31:24,370] Trial 19 finished with values: [1.4445869046211222, 4.585494496743527e-44] and parameters: {'x0': 0.36564128475868873, 'x1': 0.0030928341581690846, 'x2': 0.9446011394004309}.
[I 2026-01-17 14:31:24,966] Trial 20 finished with values: [1.4443633775253015, 3.095705331360937e-36] and parameters: {'x0': 0.43787447810613506, 'x1': 0.9996722436046344, 'x2': 0.941238061024219}.
[I 2026-01-17 14:31:25,674] Trial 21 finished with values: [1.443408516454606, 4.3094579972876635e-45] and parameters: {'x0': 0.35709941202732837, 'x1': 0.9997098583444072, 'x2': 0.9401120015723482}.
[I 2026-01-17 14:31:26,257] Trial 22 finished with values: [1.463130590964571, 2.888001049597235e-17] and parameters: {'x0': 0.6776289374052729, 'x1': 0.03833931187010174, 'x2': 1.0}.
[I 2026-01-17 14:31:26,776] Trial 23 finished with values: [1.443973196351162, 1.7300030254632066e-25] and parameters: {'x0': 0.5608202394978651, 'x1': 0.9994244463105117, 'x2': 0.9410764319010944}.
[I 2026-01-17 14:31:27,415] Trial 24 finished with values: [1.3849349118817273, 8.362307825117555e-13] and parameters: {'x0': 0.7513594778738962, 'x1': 0.8673348770287508, 'x2': 1.0}.
[I 2026-01-17 14:31:27,989] Trial 25 finished with values: [1.5, 0.0] and parameters: {'x0': 0.0, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:31:28,591] Trial 26 finished with values: [1.4381781661444666, 5.444554551490792e-68] and parameters: {'x0': 0.21077558233539917, 'x1': 0.9674237804847422, 'x2': 0.03128561406564639}.
[I 2026-01-17 14:31:29,189] Trial 27 finished with values: [1.4450624040166224, 8.973847889825106e-96] and parameters: {'x0': 0.1111654223264895, 'x1': 0.9718963397733132, 'x2': 0.028432137942827843}.
[I 2026-01-17 14:31:29,860] Trial 28 finished with values: [1.4545308740819558, 0.0] and parameters: {'x0': 0.0, 'x1': 0.04774910272951829, 'x2': 1.0}.
[I 2026-01-17 14:31:30,465] Trial 29 finished with values: [1.443456628945614, 1.3257021548609495e-118] and parameters: {'x0': 0.06571571287991912, 'x1': 0.9997015324402695, 'x2': 0.9401761095544151}.
[I 2026-01-17 14:31:31,238] Trial 30 finished with values: [1.4443394761144561, 1.1982523729722923e-29] and parameters: {'x0': 0.5095978551114642, 'x1': 0.009069976469102108, 'x2': 0.9509181612115531}.
[I 2026-01-17 14:31:31,857] Trial 31 finished with values: [1.4466735412988014, 6.783807214776015e-54] and parameters: {'x0': 0.2915746955448456, 'x1': 0.05652109261115761, 'x2': 1.0}.
[I 2026-01-17 14:31:32,512] Trial 32 finished with values: [1.4442558247808832, 1.4721363098379749e-49] and parameters: {'x0': 0.3221972580606223, 'x1': 0.9695064368543467, 'x2': 0.026904311440889787}.
[I 2026-01-17 14:31:33,079] Trial 33 finished with values: [1.4462906705701832, 4.271496767328079e-15] and parameters: {'x0': 0.7124297492536091, 'x1': 0.9430470297498714, 'x2': 0.9999999999999999}.
[I 2026-01-17 14:31:33,729] Trial 34 finished with values: [8.874509324427002e-17, 1.449317359193815] and parameters: {'x0': 1.0, 'x1': 0.05355027248993058, 'x2': 0.9999999999999999}.
[I 2026-01-17 14:31:34,541] Trial 35 finished with values: [1.4421472900270447, 1.8370418696915614e-05] and parameters: {'x0': 0.8893851946560436, 'x1': 0.061653915103546945, 'x2': 0.9999999999999999}.
[I 2026-01-17 14:31:35,014] Trial 36 finished with values: [1.2860347520190483, 0.00732682699752418] and parameters: {'x0': 0.9453560035560985, 'x1': 0.2333758939789026, 'x2': 0.963645564221677}.
[I 2026-01-17 14:31:35,540] Trial 37 finished with values: [1.3979215950049932, 0.013402146762977827] and parameters: {'x0': 0.950288740884737, 'x1': 0.0, 'x2': 0.8846892746428605}.
[I 2026-01-17 14:31:36,018] Trial 38 finished with values: [1.499773926794213, 0.026041668692059296] and parameters: {'x0': 0.9559491578123709, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-17 14:31:36,680] Trial 39 finished with values: [1.2507159928328349, 0.10107387451756286] and parameters: {'x0': 0.970742794965871, 'x1': 0.12552098979677392, 'x2': 0.8384654331851219}.
[I 2026-01-17 14:31:37,396] Trial 40 finished with values: [1.3328017906425575, 0.24152549034686965] and parameters: {'x0': 0.9785294451073929, 'x1': 0.1767211714132525, 'x2': 1.0}.
[I 2026-01-17 14:31:38,133] Trial 41 finished with values: [1.4328849389260425, 0.14666799744849854] and parameters: {'x0': 0.9730271148458519, 'x1': 0.06368389887687556, 'x2': 1.0}.
[I 2026-01-17 14:31:38,641] Trial 42 finished with values: [1.333992159994511, 0.2629425225833838] and parameters: {'x0': 0.9793333800058809, 'x1': 0.0, 'x2': 0.8311487010167008}.
[I 2026-01-17 14:31:39,194] Trial 43 finished with values: [1.412236542216437, 0.5055570678257422] and parameters: {'x0': 0.9849211636388685, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-17 14:31:39,795] Trial 44 finished with values: [1.1974161959488412, 0.5105739589446882] and parameters: {'x0': 0.9864893950301072, 'x1': 0.0756990415680327, 'x2': 0.8488480742575549}.
[I 2026-01-17 14:31:40,643] Trial 45 finished with values: [1.318172221905542, 0.4928733534366517] and parameters: {'x0': 0.9853155741934119, 'x1': 0.040451134193926895, 'x2': 0.942852105669598}.
[I 2026-01-17 14:31:41,258] Trial 46 finished with values: [1.2613904163758827, 0.6637045325742482] and parameters: {'x0': 0.9883039032603425, 'x1': 0.08125736674161979, 'x2': 1.0}.
[I 2026-01-17 14:31:41,858] Trial 47 finished with values: [1.354312703621413, 0.539278551869976] and parameters: {'x0': 0.9858812308189343, 'x1': 0.04422311000879493, 'x2': 1.0}.
[I 2026-01-17 14:31:42,479] Trial 48 finished with values: [1.1762519994276803, 0.7403325974990232] and parameters: {'x0': 0.9897699973990214, 'x1': 0.05972497173503773, 'x2': 0.9427188954676303}.
[I 2026-01-17 14:31:43,066] Trial 49 finished with values: [1.4086850958794073, 0.014756195786579529] and parameters: {'x0': 0.9511308040150737, 'x1': 0.10155002762709123, 'x2': 1.0}.
[I 2026-01-17 14:31:43,665] Trial 50 finished with values: [1.5, 5.448261629847529e-09] and parameters: {'x0': 0.8196726952048367, 'x1': 0.0, 'x2': 0.9999999999999999}.
[I 2026-01-17 14:31:44,305] Trial 51 finished with values: [1.2502094463894986, 3.2056528914080544e-22] and parameters: {'x0': 0.6055195066980048, 'x1': 0.5144722627636019, 'x2': 1.0}.
[I 2026-01-17 14:31:45,038] Trial 52 finished with values: [1.2397226711499003, 0.160165830969946] and parameters: {'x0': 0.9752753876226731, 'x1': 0.49488554639808036, 'x2': 1.0}.
[I 2026-01-17 14:31:45,596] Trial 53 finished with values: [0.8925557742864271, 1.172973175213934] and parameters: {'x0': 0.994668248743633, 'x1': 0.026768851874987096, 'x2': 1.0}.
[I 2026-01-17 14:31:46,283] Trial 54 finished with values: [1.4381504402274703, 0.017328076151746984] and parameters: {'x0': 0.9524628631173203, 'x1': 0.0542626208461052, 'x2': 0.9894619669217432}.
[I 2026-01-17 14:31:47,442] Trial 55 finished with values: [7.806015666202219e-17, 1.2748191023954125] and parameters: {'x0': 1.0, 'x1': 1.0, 'x2': 0.6575407959717496}.
[I 2026-01-17 14:31:48,239] Trial 56 finished with values: [7.115937412943625e-17, 1.1621207711314017] and parameters: {'x0': 1.0, 'x1': 0.45467107825829617, 'x2': 0.9000825664598947}.
[I 2026-01-17 14:31:48,879] Trial 57 finished with values: [8.996755200178027e-17, 1.4692816257621248] and parameters: {'x0': 1.0, 'x1': 0.03172483969131484, 'x2': 0.0}.
[I 2026-01-17 14:31:49,859] Trial 58 finished with values: [9.184850993605148e-17, 1.5] and parameters: {'x0': 1.0, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:31:50,658] Trial 59 finished with values: [1.4999999999999603, 3.4534186058961785e-07] and parameters: {'x0': 0.8543983104594208, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-17 14:31:51,407] Trial 60 finished with values: [1.4999738186944656, 0.008862461912037569] and parameters: {'x0': 0.9457001495716729, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:31:52,095] Trial 61 finished with values: [1.49597040326911, 3.344106942159469e-11] and parameters: {'x0': 0.7789908792292021, 'x1': 0.0, 'x2': 0.9959540334235724}.
[I 2026-01-17 14:31:52,850] Trial 62 finished with values: [1.4999999999892142, 5.688385951871503e-06] and parameters: {'x0': 0.8786740068618826, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:31:53,596] Trial 63 finished with values: [1.5, 3.1557294531968846e-58] and parameters: {'x0': 0.2637964132348673, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-17 14:31:54,476] Trial 64 finished with values: [1.5, 7.480230743629189e-32] and parameters: {'x0': 0.48419136339291113, 'x1': 1.0, 'x2': 0.9999999999999999}.
[I 2026-01-17 14:31:55,309] Trial 65 finished with values: [1.5, 4.004719557329522e-10] and parameters: {'x0': 0.7985527502075315, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:31:56,159] Trial 66 finished with values: [1.5, 2.5241077370801873e-13] and parameters: {'x0': 0.7418207335935786, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-17 14:31:56,849] Trial 67 finished with values: [1.5, 1.3226336189709205e-145] and parameters: {'x0': 0.03527705191526801, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-17 14:31:57,815] Trial 68 finished with values: [1.4999999999999991, 5.551585722354227e-08] and parameters: {'x0': 0.8389229187114745, 'x1': 0.9999999999999999, 'x2': 0.0}.
[I 2026-01-17 14:31:58,654] Trial 69 finished with values: [1.5, 1.3724523623977077e-16] and parameters: {'x0': 0.6881020654125343, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:31:59,265] Trial 70 finished with values: [1.5, 8.983496600899047e-21] and parameters: {'x0': 0.6249026159709564, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:32:00,245] Trial 71 finished with values: [1.336043780977482, 0.124397321047044] and parameters: {'x0': 0.9721117701189588, 'x1': 0.9610055863319994, 'x2': 0.1404219549443894}.
[I 2026-01-17 14:32:01,265] Trial 72 finished with values: [1.4999999788104759, 0.0002521280861328005] and parameters: {'x0': 0.9126286553499107, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:32:01,963] Trial 73 finished with values: [1.072000874398448, 0.7904366008633232] and parameters: {'x0': 0.9909892833775172, 'x1': 0.9738930081119688, 'x2': 0.17238394272467716}.
[I 2026-01-17 14:32:02,936] Trial 74 finished with values: [1.5, 1.5695849473508265e-33] and parameters: {'x0': 0.4658388842343704, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-17 14:32:03,860] Trial 75 finished with values: [1.5, 5.927498373813773e-54] and parameters: {'x0': 0.2910761373354068, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:32:04,809] Trial 76 finished with values: [1.5, 1.2627738906243954e-26] and parameters: {'x0': 0.5461238517270743, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-17 14:32:05,827] Trial 77 finished with values: [1.5, 7.203672798991653e-120] and parameters: {'x0': 0.06380481127393717, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-17 14:32:06,987] Trial 78 finished with values: [1.5, 1.663176751155159e-138] and parameters: {'x0': 0.04154198768916644, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:32:07,818] Trial 79 finished with values: [1.5, 1.560279321730471e-39] and parameters: {'x0': 0.4057045815733842, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-17 14:32:08,625] Trial 80 finished with values: [1.5, 3.290381459105341e-36] and parameters: {'x0': 0.4379760385622165, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-17 14:32:09,760] Trial 81 finished with values: [1.5, 7.585351771336779e-38] and parameters: {'x0': 0.421771903722286, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-17 14:32:10,634] Trial 82 finished with values: [1.5, 1.3680269982517941e-77] and parameters: {'x0': 0.16890357243740514, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-17 14:32:11,539] Trial 83 finished with values: [1.5, 3.909612633123792e-72] and parameters: {'x0': 0.19151342620137854, 'x1': 0.0, 'x2': 1.0}.
[I 2026-01-17 14:32:12,394] Trial 84 finished with values: [1.3687430359131785, 6.414029890087453e-74] and parameters: {'x0': 0.18396995062675522, 'x1': 0.8445911140949204, 'x2': 0.0}.
[I 2026-01-17 14:32:13,206] Trial 85 finished with values: [1.5, 7.819442505506539e-94] and parameters: {'x0': 0.11620094141293441, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-17 14:32:14,059] Trial 86 finished with values: [1.379885224505081, 1.0941322233841578e-84] and parameters: {'x0': 0.14355921327533042, 'x1': 0.13960407257422963, 'x2': 1.0}.
[I 2026-01-17 14:32:14,942] Trial 87 finished with values: [1.2554295222525873, 0.038709388718459053] and parameters: {'x0': 0.961452128574098, 'x1': 0.5776283133241384, 'x2': 0.0}.
[I 2026-01-17 14:32:15,870] Trial 88 finished with values: [1.4321320512165352, 2.938231920314661e-73] and parameters: {'x0': 0.18670666919912798, 'x1': 0.9267693185041952, 'x2': 1.0}.
[I 2026-01-17 14:32:16,639] Trial 89 finished with values: [1.5, 4.8262143033342457e-45] and parameters: {'x0': 0.35736659538234455, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-17 14:32:17,483] Trial 90 finished with values: [1.5, 4.6056286165763486e-71] and parameters: {'x0': 0.1962956962124501, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-17 14:32:18,276] Trial 91 finished with values: [1.5, 6.601783912696714e-62] and parameters: {'x0': 0.24236756614607313, 'x1': 0.9999999999999999, 'x2': 0.0}.
[I 2026-01-17 14:32:19,171] Trial 92 finished with values: [1.5, 1.950422013946868e-38] and parameters: {'x0': 0.4160822358998056, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:32:19,994] Trial 93 finished with values: [0.20525503916164173, 1.1971236475934526] and parameters: {'x0': 0.9988566226104079, 'x1': 0.692211354047187, 'x2': 0.9214821219840579}.
[I 2026-01-17 14:32:20,962] Trial 94 finished with values: [1.4599570050671307, 5.603607912891833e-36] and parameters: {'x0': 0.4404332295834488, 'x1': 0.04178934422349951, 'x2': 1.0}.
[I 2026-01-17 14:32:21,901] Trial 95 finished with values: [0.3191923584352629, 1.2632719324649218] and parameters: {'x0': 0.9982869675896483, 'x1': 0.21187132672016926, 'x2': 0.96899391157699}.
[I 2026-01-17 14:32:23,112] Trial 96 finished with values: [1.5, 6.401043742659036e-32] and parameters: {'x0': 0.48343756813630157, 'x1': 0.0, 'x2': 0.0}.
[I 2026-01-17 14:32:24,173] Trial 97 finished with values: [1.5, 1.1721616017060081e-91] and parameters: {'x0': 0.12217089727820314, 'x1': 1.0, 'x2': 1.0}.
[I 2026-01-17 14:32:25,193] Trial 98 finished with values: [1.5, 5.705075290167587e-12] and parameters: {'x0': 0.7653154880513249, 'x1': 1.0, 'x2': 0.0}.
[I 2026-01-17 14:32:26,135] Trial 99 finished with values: [1.5, 2.234490778578826e-25] and parameters: {'x0': 0.5620431696261835, 'x1': 1.0, 'x2': 0.0}.

my_study /tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples/c3dtlz4/py/example.db 100
Number of trials = 100
Trial: 10 [Elapsed time: 13.8] Elapsed CPU time: 9.2 seconds
Trial: 20 [Elapsed time: 23.2] Elapsed CPU time: 30.7 seconds
Trial: 30 [Elapsed time: 29.3] Elapsed CPU time: 54.1 seconds
Trial: 40 [Elapsed time: 35.5] Elapsed CPU time: 77.8 seconds
Trial: 50 [Elapsed time: 41.9] Elapsed CPU time: 101.9 seconds
Trial: 60 [Elapsed time: 49.5] Elapsed CPU time: 129.8 seconds
Trial: 70 [Elapsed time: 57.5] Elapsed CPU time: 158.6 seconds
Trial: 80 [Elapsed time: 66.6] Elapsed CPU time: 192.2 seconds
Trial: 90 [Elapsed time: 75.5] Elapsed CPU time: 223.8 seconds
Trial: 100 [Elapsed time: 85.0] Elapsed CPU time: 258.6 seconds
================================================================================
DATAFRAME COMPARISON REPORT
================================================================================

================================================================================
SUMMARY:
  Total mismatches found: 0
  Float threshold: 0.01%
  ✓ DataFrames match!
================================================================================
PASSED
c3dtlz4/py/test_variables_transformation.py::test_variables_transformation 
================================================================================
DATAFRAME COMPARISON REPORT
================================================================================

────────────────────────────────────────────────────────────────────────────────
Column: 'F1' - 4 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: 1.4445869046211222 != 1.447670173156161 (rel_diff=2.134e-03)
  Line 78: 1.495003685794626 != 1.497532147728052 (rel_diff=1.691e-03)
  Line 117: 1.47682371956128 != 1.478969064161009 (rel_diff=1.453e-03)
  Line 327: 1.4454059271875428 != 1.447548460110874 (rel_diff=1.482e-03)

────────────────────────────────────────────────────────────────────────────────
Column: 'F2' - 4 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: 4.585494496743527e-44 != 4.595281592870508e-44 (rel_diff=2.134e-03)
  Line 78: 1.8417450089905839e-121 != 1.844859905756681e-121 (rel_diff=1.691e-03)
  Line 117: 1.1232288126754917e-25 != 1.124860498864991e-25 (rel_diff=1.453e-03)
  Line 327: 1.171471360456363e-11 != 1.173207838708808e-11 (rel_diff=1.482e-03)

────────────────────────────────────────────────────────────────────────────────
Column: 'C1' - 11 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: -0.0033911465369933 != -0.009996300378869 (rel_diff=1.948e+00)
  Line 78: -0.1137832561935472 != -0.119453422682953 (rel_diff=4.983e-02)
  Line 82: -0.1203673055076137 != -0.1211223916803213 (rel_diff=6.273e-03)
  Line 98: -0.1221311482578988 != -0.125 (rel_diff=2.349e-02)
  Line 105: -0.124484267356677 != -0.125 (rel_diff=4.143e-03)
  Line 106: -0.1126779959521606 != -0.112797464282956 (rel_diff=1.060e-03)
  Line 107: -0.1119495107572825 != -0.1134573372909615 (rel_diff=1.347e-02)
  Line 117: -0.0733905089878543 != -0.078122694624534 (rel_diff=6.448e-02)
  Line 279: -0.4356605212037681 != -0.4370790202635428 (rel_diff=3.256e-03)
  Line 311: -0.5384930707665228 != -0.5406742204840589 (rel_diff=4.050e-03)
  Line 327: -0.0051438489582229 != -0.0097351992862068 (rel_diff=8.926e-01)

────────────────────────────────────────────────────────────────────────────────
Column: 'C2' - 8 mismatch(es)
────────────────────────────────────────────────────────────────────────────────
  Line 9: -1.086831325002835 != -1.09574893024599 (rel_diff=8.205e-03)
  Line 78: -1.2350360205395168 != -1.242602533478992 (rel_diff=6.127e-03)
  Line 98: -1.246174321814996 != -1.249999999999629 (rel_diff=3.070e-03)
  Line 107: -1.2325880753094638 != -1.234600970274428 (rel_diff=1.633e-03)
  Line 117: -1.1810082986588144 != -1.187349492745291 (rel_diff=5.369e-03)
  Line 279: -0.0043119169509244 != -0.0055482887859028 (rel_diff=2.867e-01)
  Line 311: -0.0012322408512295 != -0.0030787929590615 (rel_diff=1.499e+00)
  Line 327: -1.0891982943400942 != -1.095396544360563 (rel_diff=5.691e-03)

================================================================================
SUMMARY:
  Total mismatches found: 27
  Float threshold: 0.1%
================================================================================
PASSED
cattlefeed/py/test_shgo_with_constraints.py::test_optimization_ex 
x1=0.636,x2=0.000,x3=0.313,x4=0.052,f=29.894
g1=-0.000,g2=-0.000,g3=0.000
PASSED
constraint_dora/py/test_slsqp.py::test_constraint_dora 
============================================================
Brute force result: x1 = 0.89039 x2 = 0.45495 f(x*) = 1.52831
============================================================
Brute force result polar: r = 0.99989 theta = 0.47238 f(x*) = 1.52831
Brute force result polar Cartesian coordinate system: x1 = 0.89039 x2 = 0.45496 f(x*) = 1.52831
============================================================
============================================================
CONSTRAINED OPTIMIZATION RESULTS
============================================================
Success: True
Message: Optimization terminated successfully

Constraint value: -0.000000 (should be >= 0)
Constraint satisfied: True

Optimal solution: x1 = 0.894429 x2 = 0.447211 f(x*) = 1.527864

Expected result (analytical solution):
  2/√5 = 0.894427, 1/√5 = 0.447214
  (√5 - 1)² = 1.527864
x1 error    = 0.000154%
x2 error    = -0.000614%
f(x*) error = -0.000001%
============================================================
PASSED
deap/py/test_easimple.py::test_easimple 
======================================================================
GENETIC ALGORITHM OPTIMIZATION: Production Planning with DEAP
======================================================================

Genetic Algorithm Parameters:
  Population Size: 100
  Generations: 50
  Crossover Probability: 0.7
  Mutation Probability: 0.2

Running optimization...

gen	nevals	avg      	std     	min      	max      
0  	100   	-17,882.9	5,345.78	-34,403.7	-9,793.94
1  	78    	-13,706.9	3,803.25	-25,887.8	-9,229.37
2  	79    	-10,920.4	1,631.75	-17,608.7	-9,166.56
3  	69    	-10,021.5	918.106 	-15,691.2	-9,163.4 
4  	77    	-9,608.92	439.015 	-11,981  	-9,136.48
5  	79    	-9,110.25	1,758.07	-10,351.2	849.448  
6  	71    	-8,623.82	2,588.86	-10,257.4	849.448  
7  	75    	-7,798.73	3,611.4 	-10,205.1	849.448  
8  	71    	-7,482.8 	3,884.68	-9,826.9 	849.448  
9  	71    	-6,980.22	4,251.6 	-10,087.7	849.448  
10 	78    	-5,799.1 	4,829.73	-10,281.5	857.208  
11 	73    	-4,452.96	5,031.82	-9,646.03	860.065  
12 	72    	-3,548.58	4,991.52	-9,617.57	861.499  
13 	75    	-3,224.47	4,930.94	-9,711.18	861.499  
14 	64    	-2,110.89	4,570.22	-9,597.36	854.187  
15 	81    	-2,821.48	4,859.18	-9,744.03	854.289  
16 	68    	-1,702.78	4,393.7 	-9,930.19	858.274  
17 	82    	-1,377.2 	4,176.56	-9,543.52	859.407  
18 	72    	-1,071.61	3,961.87	-9,713.61	859.442  
19 	69    	-1,167.8 	4,032.96	-9,805.4 	859.442  
20 	73    	-1,365.25	4,174.43	-9,455.29	860.465  
21 	76    	-1,060.88	3,950.93	-9,571   	860.465  
22 	71    	-1,965.19	4,521.65	-9,557.11	860.505  
23 	72    	-1,060.48	3,946.77	-9,677.02	860.505  
24 	74    	-949.666 	3,854.22	-9,370.66	860.541  
25 	70    	-2,261   	4,648.15	-9,492.27	860.537  
26 	73    	-1,554.44	4,291.31	-9,331.48	860.547  
27 	82    	-1,970.13	4,537.99	-10,059  	860.571  
28 	83    	-1,550   	4,282.05	-9,505.49	860.59   
29 	77    	-1,467.99	4,258.49	-9,643.01	860.595  
30 	71    	-1,160.52	4,037.29	-9,548.6 	860.595  
31 	71    	-1,661.77	4,366.03	-9,516.29	860.595  
32 	84    	-2,257.76	4,651.06	-9,417.8 	860.595  
33 	73    	-1,258.37	4,102.59	-9,564.84	860.595  
34 	82    	-1,467.6 	4,256.54	-9,583.28	860.595  
35 	79    	-1,565.68	4,311.56	-9,890.08	860.585  
36 	73    	-1,051.86	3,941.49	-9,405.88	860.597  
37 	74    	-1,452.29	4,225.93	-9,493.4 	860.597  
38 	71    	-1,848.21	4,454.11	-9,502.58	860.597  
39 	80    	-2,861.49	4,854.62	-9,843.34	860.597  
40 	87    	-1,750.6 	4,401.63	-9,378.1 	860.585  
41 	82    	-2,758.03	4,823.83	-9,617.04	860.587  
42 	81    	-2,161.99	4,614.21	-9,791.68	860.589  
43 	66    	-1,048.29	3,937.38	-9,417.34	860.586  
44 	75    	-1,046.67	3,935.72	-9,568.58	860.592  
45 	87    	-2,555.23	4,758.67	-9,560.92	860.592  
46 	78    	-1,556.01	4,292.61	-9,575.45	860.592  
47 	74    	-1,453.36	4,229.98	-9,672.97	860.593  
48 	73    	-1,551.09	4,291.74	-9,380.59	860.594  
49 	78    	-1,553.38	4,292.55	-9,404.5 	860.594  
50 	67    	-1,751.95	4,405.98	-9,525.98	860.594  

======================================================================
OPTIMIZATION RESULTS
======================================================================

Best Profit: $861.50
Constraints Satisfied: True

----------------------------------------------------------------------
Optimal Production Quantities:
----------------------------------------------------------------------
Product  Quantity  Profit_per_Unit  Total_Profit
      A      7.79               30        233.75
      B      6.55               45        294.77
      C      2.19               50        109.30
      D      6.39               35        223.68

----------------------------------------------------------------------
Resource Utilization:
----------------------------------------------------------------------
     Resource  Used  Capacity  Utilization_%
  labor_hours 59.96       100          59.96
machine_hours 35.94        80          44.93
storage_space 15.16        50          30.31

----------------------------------------------------------------------
Constraint Verification:
----------------------------------------------------------------------
Total Weight: 59.96 (limit: 60)
Product B / Product A ratio: 84.07% (min: 30%)
All resource constraints: ✓ Satisfied
Minimum quantities: ✓ Satisfied

----------------------------------------------------------------------
Evolution Statistics:
----------------------------------------------------------------------
Generation 0 - Max Fitness: $-9793.94, Avg: $-17882.88
Generation 25 - Max Fitness: $860.54, Avg: $-2261.00
Generation 50 - Max Fitness: $860.59, Avg: $-1751.95

======================================================================
To use with your own CSV files:
  products_data = pd.read_csv('your_products.csv')
  resources_data = pd.read_csv('your_resources.csv')
  requirements_data = pd.read_csv('your_requirements.csv')
======================================================================
PASSED
eggholder/py/test_shgo.py::test_optimization_ex 
The first element of the sorted dataset:     512.0 404.00 -959.5797
Analytical solution [1]:                     512.0 404.23 -959.6407
Simplicial homology global optimization [2]: 512.0 404.23 -959.6407
Dual annealing [3]:                          512.0 404.23 -959.6407
Difference between SHGO and DA methods: -2.37e-14 %
[1] EGGHOLDER: https://www.sfu.ca/~ssurjano/egg.html
[2] SHGO: https://link.springer.com/article/10.1007/s10898-018-0645-y
[3] DA: https://www.jstatsoft.org/article/view/v060i06
[4] SCIPY: https://docs.scipy.org/doc/scipy/tutorial/optimize.html#global-optimization
PASSED
pyomo/py/test_bnh_csv.py::test_bnh_csv 
================================================================================
PARETO FRONT FROM CSV DATA (NO ANALYTICAL FUNCTIONS)
================================================================================

IMPORTANT: F1 and F2 are read ONLY from the DataFrame/CSV
No analytical functions are used for objectives!

================================================================================
RUNNING NSGA-II OPTIMIZATION
================================================================================

Loaded 10201 data points from CSV:
    X1    X2      F1       F2
0  0.0  0.00  0.0000  50.0000
1  0.0  0.03  0.0036  49.7009
2  0.0  0.06  0.0144  49.4036
3  0.0  0.09  0.0324  49.1081
4  0.0  0.12  0.0576  48.8144
5  0.0  0.15  0.0900  48.5225
6  0.0  0.18  0.1296  48.2324
7  0.0  0.21  0.1764  47.9441
8  0.0  0.24  0.2304  47.6576
9  0.0  0.27  0.2916  47.3729

================================================================================
CREATING INTERPOLATORS FROM CSV DATA
================================================================================

Optimization Settings:
  Objectives: F1 and F2 from CSV data (interpolated)
  Variables: X1 ∈ [0, 5], X2 ∈ [0, 3]
  Constraints:
    C1: (x1-5)² + x2² ≤ 25
    C2: (x1-8)² + (x2+3)² ≥ 7.7
  Population: 100
  Generations: 200

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |     33 |  0.000000E+00 |  0.1286804155 |             - |             -
     2 |      200 |     55 |  0.000000E+00 |  0.000000E+00 |  0.0094193678 |         ideal
     3 |      300 |     80 |  0.000000E+00 |  0.000000E+00 |  0.0133917184 |         ideal
     4 |      400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0049512504 |         ideal
     5 |      500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0346550020 |         nadir
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0042590207 |         ideal
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0264060472 |         nadir
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013496195 |             f
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029223176 |             f
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0602502458 |         nadir
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013677011 |             f
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024541785 |             f
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031088355 |             f
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030604978 |         nadir
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016427562 |             f
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026902471 |             f
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012212615 |             f
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024523804 |             f
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030806621 |             f
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020401815 |             f
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028333888 |             f
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017196336 |             f
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0034415200 |             f
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014990912 |             f
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023137096 |             f
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030067257 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0034426328 |         nadir
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016875754 |             f
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031067282 |             f
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019428838 |             f
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025044898 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015208656 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029258606 |             f
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016167522 |             f
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029026766 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015656362 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025949558 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011032953 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024195891 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0037000220 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014403648 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027243194 |             f
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013623455 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024324181 |             f
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033621633 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012273873 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018310109 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028007010 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018818039 |             f
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028060430 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012789861 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023066908 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0034180714 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014101766 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019330304 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027034354 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018232295 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032566788 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019233244 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028909781 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012305381 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023523093 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031773596 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021074091 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028248819 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014316787 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031257715 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015879427 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029896085 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016088139 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024748758 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029771257 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016266051 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022378814 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031184031 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014639635 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022377123 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027497942 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013219945 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024414493 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030531216 |             f
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016585290 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024319308 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029043670 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018893253 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027001371 |             f
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015681522 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022553954 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033454595 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022275939 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033037730 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014704339 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025391655 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011058446 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023136897 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028730885 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015509714 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031055775 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018220131 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0033070451 |             f
   101 |    10100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016620725 |             f
   102 |    10200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030799063 |             f
   103 |    10300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015678476 |             f
   104 |    10400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025405400 |             f
   105 |    10500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020755373 |             f
   106 |    10600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032823534 |             f
   107 |    10700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010310932 |             f
   108 |    10800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018420800 |             f
   109 |    10900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030569520 |             f
   110 |    11000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012545302 |             f
   111 |    11100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022705865 |             f
   112 |    11200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030301488 |             f
   113 |    11300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014763647 |             f
   114 |    11400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023794887 |             f
   115 |    11500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027770762 |             f
   116 |    11600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018711824 |             f
   117 |    11700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029580051 |             f
   118 |    11800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015117153 |             f
   119 |    11900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022026519 |             f
   120 |    12000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029406710 |             f
   121 |    12100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017212980 |             f
   122 |    12200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025636759 |             f
   123 |    12300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014956680 |             f
   124 |    12400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028512845 |             f
   125 |    12500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013339962 |             f
   126 |    12600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026413103 |             f
   127 |    12700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014638281 |             f
   128 |    12800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029077235 |             f
   129 |    12900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011488020 |             f
   130 |    13000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020592693 |             f
   131 |    13100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028587589 |             f
   132 |    13200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010471028 |             f
   133 |    13300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025621935 |             f
   134 |    13400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013511135 |             f
   135 |    13500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021734668 |             f
   136 |    13600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030967327 |             f
   137 |    13700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014339744 |             f
   138 |    13800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022370097 |             f
   139 |    13900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031350592 |             f
   140 |    14000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013799313 |             f
   141 |    14100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024094746 |             f
   142 |    14200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0035562441 |             f
   143 |    14300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014658387 |             f
   144 |    14400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027588815 |             f
   145 |    14500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012150657 |             f
   146 |    14600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025219646 |             f
   147 |    14700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010186729 |             f
   148 |    14800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025620430 |             f
   149 |    14900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012961613 |             f
   150 |    15000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022772545 |             f
   151 |    15100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031359253 |             f
   152 |    15200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012463746 |             f
   153 |    15300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024226216 |             f
   154 |    15400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030005391 |             f
   155 |    15500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016712303 |             f
   156 |    15600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020544577 |             f
   157 |    15700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032833995 |             f
   158 |    15800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018285467 |             f
   159 |    15900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028244138 |             f
   160 |    16000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016832680 |             f
   161 |    16100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026202319 |             f
   162 |    16200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020147649 |             f
   163 |    16300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032114292 |             f
   164 |    16400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018578464 |             f
   165 |    16500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025752397 |             f
   166 |    16600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016368045 |             f
   167 |    16700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027822098 |             f
   168 |    16800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013437895 |             f
   169 |    16900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022574945 |             f
   170 |    17000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030589683 |             f
   171 |    17100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014711390 |             f
   172 |    17200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021399322 |             f
   173 |    17300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029197988 |             f
   174 |    17400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016159070 |             f
   175 |    17500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027599366 |             f
   176 |    17600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018863532 |             f
   177 |    17700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028668031 |             f
   178 |    17800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013986234 |             f
   179 |    17900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029365441 |             f
   180 |    18000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007667067 |             f
   181 |    18100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026868655 |             f
   182 |    18200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014725721 |             f
   183 |    18300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024614301 |             f
   184 |    18400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030987642 |             f
   185 |    18500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016168394 |             f
   186 |    18600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027080733 |             f
   187 |    18700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015221964 |             f
   188 |    18800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027041198 |             f
   189 |    18900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014591066 |             f
   190 |    19000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024765200 |             f
   191 |    19100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0032100936 |             f
   192 |    19200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011812047 |             f
   193 |    19300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019472959 |             f
   194 |    19400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029805970 |             f
   195 |    19500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017631852 |             f
   196 |    19600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029766829 |             f
   197 |    19700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014623804 |             f
   198 |    19800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021882647 |             f
   199 |    19900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029620974 |             f
   200 |    20000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017576242 |             f

================================================================================
PARETO FRONT RESULTS
================================================================================

Number of Pareto optimal solutions: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (First 15):
--------------------------------------------------------------------------------
 Solution           X1           X2           F1        F2  All_constraints_OK
        1 5.000000e+00 3.000000e+00 1.360000e+02  4.000000                True
        2 1.218467e-08 2.046647e-10 2.461494e-09 50.000000                True
        3 8.711329e-02 1.309883e-01 1.017353e-01 47.844418                True
        4 8.492474e-01 8.269027e-01 5.620991e+00 34.643747                True
        5 1.657728e+00 1.791393e+00 2.383065e+01 21.466449                True
        6 2.198225e+00 2.300924e+00 4.050690e+01 15.135225                True
        7 5.854720e-01 6.804507e-01 3.226005e+00 38.147275                True
        8 2.145566e-01 1.816493e-01 3.183748e-01 46.117535                True
        9 7.311700e-01 6.804507e-01 3.993620e+00 36.882198                True
       10 5.063401e-01 6.804507e-01 2.879462e+00 38.851958                True
       11 1.491069e+00 1.272618e+00 1.537372e+01 26.206555                True
       12 1.172314e+00 9.889348e-01 9.411846e+00 30.740470                True
       13 6.187570e-03 1.958578e-10 1.237514e-03 49.938434                True
       14 4.639126e+00 2.993578e+00 1.219343e+02  4.156536                True
       15 7.960221e-01 1.029604e+00 6.776461e+00 33.437850                True

--------------------------------------------------------------------------------
Summary Statistics:
--------------------------------------------------------------------------------
X1 range: [0.000, 5.000]
X2 range: [0.000, 3.000]
F1 range: [0.000, 136.000]
F2 range: [4.000, 50.000]
Feasible solutions: 100 / 100

✓ Pareto front saved to 'pareto_front_results_tab.csv'

================================================================================
ANALYZING ORIGINAL CSV DATA
================================================================================

Original CSV data points:
  Total points: 10201
  Feasible points: 9500
  Infeasible points: 701

================================================================================
GENERATING VISUALIZATIONS
================================================================================
✓ Visualization saved as 'pareto_front_two_plots.png'

================================================================================
OPTIMIZATION COMPLETE
================================================================================

Generated files:
  1. pareto_front_results.csv - Pareto optimal solutions
  2. pareto_front_two_plots.png - Objective and variable space plots

✓ F1 and F2 were obtained ONLY from CSV data (no analytical functions)
✓ Interpolation used to estimate objectives for new X1, X2 combinations
✓ The Pareto front shows optimal trade-offs based purely on your data
================================================================================
PASSED
pyomo/py/test_bnh_csv_decision_tree.py::test_bnh_csv_decision_tree 
================================================================================
PARETO FRONT FROM CSV DATA (NO ANALYTICAL FUNCTIONS)
================================================================================

IMPORTANT: F1 and F2 are read ONLY from the DataFrame/CSV
No analytical functions are used for objectives!

================================================================================
RUNNING NSGA-II OPTIMIZATION
================================================================================

Loaded 10201 data points from CSV:
    X1    X2      F1       F2
0  0.0  0.00  0.0000  50.0000
1  0.0  0.03  0.0036  49.7009
2  0.0  0.06  0.0144  49.4036
3  0.0  0.09  0.0324  49.1081
4  0.0  0.12  0.0576  48.8144
5  0.0  0.15  0.0900  48.5225
6  0.0  0.18  0.1296  48.2324
7  0.0  0.21  0.1764  47.9441
8  0.0  0.24  0.2304  47.6576
9  0.0  0.27  0.2916  47.3729

================================================================================
CREATING INTERPOLATORS FROM CSV DATA
================================================================================
✓ Decision Tree Regressors trained from CSV data
  - Using 10201 data points for training
  - F1 model R² score: 0.9999
  - F2 model R² score: 0.9998
  - Max depth: 15, min samples split: 5, min samples leaf 2

Optimization Settings:
  Objectives: F1 and F2 from CSV data (interpolated)
  Variables: X1 ∈ [0, 5], X2 ∈ [0, 3]
  Constraints:
    C1: (x1-5)² + x2² ≤ 25
    C2: (x1-8)² + (x2+3)² ≥ 7.7
  Population: 100
  Generations: 200

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |     30 |  0.000000E+00 |  0.1286804155 |             - |             -
     2 |      200 |     44 |  0.000000E+00 |  0.000000E+00 |  0.0094302091 |         ideal
     3 |      300 |     62 |  0.000000E+00 |  0.000000E+00 |  0.0115410372 |         ideal
     4 |      400 |     89 |  0.000000E+00 |  0.000000E+00 |  0.0061365191 |         ideal
     5 |      500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0300713395 |         nadir
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025561371 |             f
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015821736 |             f
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0297960831 |         nadir
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0108426833 |         nadir
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006788078 |             f
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0307062109 |         nadir
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014237947 |             f
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0108341875 |         nadir
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0150329387 |         nadir
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010560022 |             f
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019146881 |             f
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023726160 |             f
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030623037 |             f
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010578016 |             f
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016716443 |             f
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021191422 |             f
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023290155 |             f
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111668636 |         nadir
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010925231 |             f
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016465655 |             f
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021847536 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027851710 |             f
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006823772 |             f
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011927399 |             f
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017389549 |             f
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018724726 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021919856 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023709962 |             f
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024149542 |             f
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024035939 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022656496 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026557657 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009055364 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014577361 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018502008 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024833898 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027665856 |             f
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007471782 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012179709 |             f
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019106063 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021880251 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024661914 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028030292 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009365372 |             f
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015955219 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017193171 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019994100 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021971399 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024016195 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025639223 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004218840 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011111038 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016110061 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016471329 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022822365 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025625011 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008594448 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018278516 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022253151 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029859610 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012168281 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016950571 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019811428 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022772639 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023808916 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025392180 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014131601 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018904458 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023862001 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029757585 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0005680436 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012114554 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018563322 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024784091 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027363714 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009636381 |             f
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017950013 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024910679 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028316150 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006911416 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012981216 |             f
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016537096 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023898157 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024944587 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027214293 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007986760 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010235565 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018140861 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022474642 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024911912 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026784074 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010610107 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016484846 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016449763 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020142413 |             f
   101 |    10100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025197123 |             f
   102 |    10200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004739624 |             f
   103 |    10300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012848794 |             f
   104 |    10400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016134634 |             f
   105 |    10500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016769258 |             f
   106 |    10600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019495074 |             f
   107 |    10700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024710987 |             f
   108 |    10800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028336154 |             f
   109 |    10900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007508756 |             f
   110 |    11000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011630096 |             f
   111 |    11100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017813123 |             f
   112 |    11200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020449588 |             f
   113 |    11300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022858351 |             f
   114 |    11400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025476706 |             f
   115 |    11500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006637029 |             f
   116 |    11600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008507181 |             f
   117 |    11700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014203787 |             f
   118 |    11800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025757795 |             f
   119 |    11900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006784762 |             f
   120 |    12000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012305567 |             f
   121 |    12100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014054034 |             f
   122 |    12200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018129463 |             f
   123 |    12300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021187618 |             f
   124 |    12400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023458375 |             f
   125 |    12500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024770517 |             f
   126 |    12600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023647830 |             f
   127 |    12700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027742671 |             f
   128 |    12800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008684354 |             f
   129 |    12900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015718511 |             f
   130 |    13000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018028645 |             f
   131 |    13100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020828005 |             f
   132 |    13200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023065896 |             f
   133 |    13300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026442110 |             f
   134 |    13400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009565265 |             f
   135 |    13500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017484165 |             f
   136 |    13600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024778599 |             f
   137 |    13700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025357670 |             f
   138 |    13800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007356919 |             f
   139 |    13900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010125807 |             f
   140 |    14000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014830162 |             f
   141 |    14100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018609696 |             f
   142 |    14200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021999137 |             f
   143 |    14300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022909521 |             f
   144 |    14400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024972844 |             f
   145 |    14500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023083013 |             f
   146 |    14600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023275379 |             f
   147 |    14700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027334576 |             f
   148 |    14800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008782173 |             f
   149 |    14900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012196709 |             f
   150 |    15000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013377808 |             f
   151 |    15100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017875654 |             f
   152 |    15200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021855626 |             f
   153 |    15300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023852722 |             f
   154 |    15400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027229499 |             f
   155 |    15500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007864234 |             f
   156 |    15600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011437087 |             f
   157 |    15700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016642774 |             f
   158 |    15800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020555117 |             f
   159 |    15900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023847449 |             f
   160 |    16000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027321414 |             f
   161 |    16100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017599639 |             f
   162 |    16200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023187083 |             f
   163 |    16300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027357618 |             f
   164 |    16400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011798045 |             f
   165 |    16500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016501768 |             f
   166 |    16600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022323287 |             f
   167 |    16700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028373689 |             f
   168 |    16800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006842937 |             f
   169 |    16900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009955446 |             f
   170 |    17000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016654359 |             f
   171 |    17100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022280324 |             f
   172 |    17200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026450557 |             f
   173 |    17300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004320553 |             f
   174 |    17400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013340689 |             f
   175 |    17500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020168706 |             f
   176 |    17600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024108760 |             f
   177 |    17700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026507340 |             f
   178 |    17800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007556377 |             f
   179 |    17900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017429982 |             f
   180 |    18000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021978117 |             f
   181 |    18100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026884756 |             f
   182 |    18200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008297853 |             f
   183 |    18300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013081424 |             f
   184 |    18400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017253512 |             f
   185 |    18500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017203848 |             f
   186 |    18600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018644606 |             f
   187 |    18700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019356770 |             f
   188 |    18800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022375102 |             f
   189 |    18900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023290506 |             f
   190 |    19000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024683832 |             f
   191 |    19100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028476087 |             f
   192 |    19200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009989903 |             f
   193 |    19300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019895200 |             f
   194 |    19400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022559882 |             f
   195 |    19500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026615906 |             f
   196 |    19600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0002334188 |             f
   197 |    19700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012037161 |             f
   198 |    19800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017147930 |             f
   199 |    19900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022623139 |             f
   200 |    20000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025946461 |             f

================================================================================
PARETO FRONT RESULTS
================================================================================

Number of Pareto optimal solutions: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (First 15):
--------------------------------------------------------------------------------
 Solution       X1       X2         F1       F2  All_constraints_OK
        1 0.081992 0.132386   0.038267 48.11870                True
        2 4.797486 2.994308 127.092600  4.01500                True
        3 4.820903 2.993248 127.092600  4.01500                True
        4 0.124493 0.108412   0.038267 48.11870                True
        5 3.060195 2.942259  70.828400  7.92400                True
        6 0.200072 0.180240   0.234800 46.17870                True
        7 4.496139 2.993707 115.932600  4.18375                True
        8 4.677925 2.993113 123.292600  4.07625                True
        9 0.323048 0.196272   0.508400 44.74820                True
       10 3.430052 2.931841  81.148400  6.72485                True
       11 1.202342 0.973858   9.567000 30.76220                True
       12 3.277230 2.808117  74.378000  7.49370                True
       13 2.212310 2.438640  43.276600 14.36915                True
       14 2.203604 2.580823  45.372400 13.77035                True
       15 0.772330 0.853866   4.828400 35.12000                True

--------------------------------------------------------------------------------
Summary Statistics:
--------------------------------------------------------------------------------
X1 range: [0.082, 4.821]
X2 range: [0.021, 3.000]
F1 range: [0.038, 127.093]
F2 range: [4.015, 48.119]
Feasible solutions: 100 / 100

✓ Pareto front saved to 'pareto_front_results_dt.csv'

================================================================================
ANALYZING ORIGINAL CSV DATA
================================================================================

Original CSV data points:
  Total points: 10201
  Feasible points: 9500
  Infeasible points: 701

================================================================================
GENERATING VISUALIZATIONS
================================================================================
✓ Visualization saved as 'pareto_front_two_plots_dt.png'

================================================================================
OPTIMIZATION COMPLETE
================================================================================

Generated files:
  1. pareto_front_results_dt.csv - Pareto optimal solutions
  2. pareto_front_two_plots_dt.png - Objective and variable space plots

✓ F1 and F2 were obtained ONLY from CSV data (no analytical functions)
✓ Interpolation used to estimate objectives for new X1, X2 combinations
✓ The Pareto front shows optimal trade-offs based purely on your data
================================================================================
PASSED
pyomo/py/test_bnh_csv_decision_tree_generated_constraints.py::test_bnh_csv_decision_tree_generated_constraints 
================================================================================
PARETO FRONT FROM CSV DATA (NO ANALYTICAL FUNCTIONS)
================================================================================

IMPORTANT: F1 and F2 are read ONLY from the DataFrame/CSV
No analytical functions are used for objectives!
================================================================================
GENERIC CONSTRAINT FUNCTION GENERATOR FROM JSON
================================================================================

# ============================================================================
# GENERATED CONSTRAINT FUNCTIONS FROM JSON ALPHA ATTRIBUTE
# ============================================================================
# Source: /tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples/pyomo/py/bnh.json
# Variables: X1, X2
# Alpha: (X1-5)*(X1-5)+X2*X2-25 and -(X1-8)*(X1-8)-(X2+3)*(X2+3)+7.7
# Number of constraints: 2
# ============================================================================

def constraint_C1(X1, X2):
    """
    C1: (X1-5)*(X1-5)+X2*X2-25
    Returns: True if constraint is satisfied, False otherwise
    """
    return (X1-5)*(X1-5)+X2*X2-25

def constraint_C2(X1, X2):
    """
    C2: -(X1-8)*(X1-8)-(X2+3)*(X2+3)+7.7
    Returns: True if constraint is satisfied, False otherwise
    """
    return -(X1-8)*(X1-8)-(X2+3)*(X2+3)+7.7


================================================================================
✓ Constraints successfully generated and saved to '/tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples/pyomo/py/generated_constraints_claude.py'
================================================================================

================================================================================
RUNNING NSGA-II OPTIMIZATION
================================================================================

Loaded 10201 data points from CSV:
    X1    X2      F1       F2
0  0.0  0.00  0.0000  50.0000
1  0.0  0.03  0.0036  49.7009
2  0.0  0.06  0.0144  49.4036
3  0.0  0.09  0.0324  49.1081
4  0.0  0.12  0.0576  48.8144
5  0.0  0.15  0.0900  48.5225
6  0.0  0.18  0.1296  48.2324
7  0.0  0.21  0.1764  47.9441
8  0.0  0.24  0.2304  47.6576
9  0.0  0.27  0.2916  47.3729

================================================================================
CREATING MODEL FROM CSV DATA
================================================================================
✓ Decision Tree Regressors trained from CSV data
  - Using 10201 data points for training
  - F1 model R² score: 0.9999
  - F2 model R² score: 0.9998
  - Max depth: 15, min samples split: 5, min samples leaf 2

Optimization Settings:
  Objectives: F1 and F2 from CSV data (interpolated)
  Variables: X1 ∈ [0, 5], X2 ∈ [0, 3]
  Constraints:
    C1: (x1-5)² + x2² ≤ 25
    C2: (x1-8)² + (x2+3)² ≥ 7.7
  Population: 100
  Generations: 200

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |     30 |  0.000000E+00 |  0.1286804155 |             - |             -
     2 |      200 |     44 |  0.000000E+00 |  0.000000E+00 |  0.0094302091 |         ideal
     3 |      300 |     62 |  0.000000E+00 |  0.000000E+00 |  0.0115410372 |         ideal
     4 |      400 |     89 |  0.000000E+00 |  0.000000E+00 |  0.0061365191 |         ideal
     5 |      500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0300713395 |         nadir
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025561371 |             f
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015821736 |             f
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0297960831 |         nadir
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0108426833 |         nadir
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006788078 |             f
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0307062109 |         nadir
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014237947 |             f
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0108341875 |         nadir
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0150329387 |         nadir
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010560022 |             f
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019146881 |             f
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023726160 |             f
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0030623037 |             f
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010578016 |             f
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016716443 |             f
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021191422 |             f
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023290155 |             f
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111668636 |         nadir
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010925231 |             f
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016465655 |             f
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021847536 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027851710 |             f
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006823772 |             f
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011927399 |             f
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017389549 |             f
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018724726 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021919856 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023709962 |             f
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024149542 |             f
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024035939 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022656496 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026557657 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009055364 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014577361 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018502008 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024833898 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027665856 |             f
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007471782 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012179709 |             f
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019106063 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021880251 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024661914 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028030292 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009365372 |             f
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015955219 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017193171 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019994100 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021971399 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024016195 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025639223 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004218840 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011111038 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016110061 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016471329 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022822365 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025625011 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008594448 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018278516 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022253151 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029859610 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012168281 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016950571 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019811428 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022772639 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023808916 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025392180 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014131601 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018904458 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023862001 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0029757585 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0005680436 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012114554 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018563322 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024784091 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027363714 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009636381 |             f
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017950013 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024910679 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028316150 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006911416 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012981216 |             f
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016537096 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023898157 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024944587 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027214293 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007986760 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010235565 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018140861 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022474642 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024911912 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026784074 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010610107 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016484846 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016449763 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020142413 |             f
   101 |    10100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025197123 |             f
   102 |    10200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004739624 |             f
   103 |    10300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012848794 |             f
   104 |    10400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016134634 |             f
   105 |    10500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016769258 |             f
   106 |    10600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019495074 |             f
   107 |    10700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024710987 |             f
   108 |    10800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028336154 |             f
   109 |    10900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007508756 |             f
   110 |    11000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011630096 |             f
   111 |    11100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017813123 |             f
   112 |    11200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020449588 |             f
   113 |    11300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022858351 |             f
   114 |    11400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025476706 |             f
   115 |    11500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006637029 |             f
   116 |    11600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008507181 |             f
   117 |    11700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014203787 |             f
   118 |    11800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025757795 |             f
   119 |    11900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006784762 |             f
   120 |    12000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012305567 |             f
   121 |    12100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014054034 |             f
   122 |    12200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018129463 |             f
   123 |    12300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021187618 |             f
   124 |    12400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023458375 |             f
   125 |    12500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024770517 |             f
   126 |    12600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023647830 |             f
   127 |    12700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027742671 |             f
   128 |    12800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008684354 |             f
   129 |    12900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0015718511 |             f
   130 |    13000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018028645 |             f
   131 |    13100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020828005 |             f
   132 |    13200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023065896 |             f
   133 |    13300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026442110 |             f
   134 |    13400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009565265 |             f
   135 |    13500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017484165 |             f
   136 |    13600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024778599 |             f
   137 |    13700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025357670 |             f
   138 |    13800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007356919 |             f
   139 |    13900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0010125807 |             f
   140 |    14000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0014830162 |             f
   141 |    14100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018609696 |             f
   142 |    14200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021999137 |             f
   143 |    14300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022909521 |             f
   144 |    14400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024972844 |             f
   145 |    14500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023083013 |             f
   146 |    14600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023275379 |             f
   147 |    14700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027334576 |             f
   148 |    14800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008782173 |             f
   149 |    14900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012196709 |             f
   150 |    15000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013377808 |             f
   151 |    15100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017875654 |             f
   152 |    15200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021855626 |             f
   153 |    15300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023852722 |             f
   154 |    15400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027229499 |             f
   155 |    15500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007864234 |             f
   156 |    15600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011437087 |             f
   157 |    15700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016642774 |             f
   158 |    15800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020555117 |             f
   159 |    15900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023847449 |             f
   160 |    16000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027321414 |             f
   161 |    16100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017599639 |             f
   162 |    16200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023187083 |             f
   163 |    16300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0027357618 |             f
   164 |    16400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0011798045 |             f
   165 |    16500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016501768 |             f
   166 |    16600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022323287 |             f
   167 |    16700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028373689 |             f
   168 |    16800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0006842937 |             f
   169 |    16900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009955446 |             f
   170 |    17000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0016654359 |             f
   171 |    17100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022280324 |             f
   172 |    17200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026450557 |             f
   173 |    17300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0004320553 |             f
   174 |    17400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013340689 |             f
   175 |    17500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0020168706 |             f
   176 |    17600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024108760 |             f
   177 |    17700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026507340 |             f
   178 |    17800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0007556377 |             f
   179 |    17900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017429982 |             f
   180 |    18000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0021978117 |             f
   181 |    18100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026884756 |             f
   182 |    18200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0008297853 |             f
   183 |    18300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0013081424 |             f
   184 |    18400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017253512 |             f
   185 |    18500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017203848 |             f
   186 |    18600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0018644606 |             f
   187 |    18700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019356770 |             f
   188 |    18800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022375102 |             f
   189 |    18900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0023290506 |             f
   190 |    19000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0024683832 |             f
   191 |    19100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0028476087 |             f
   192 |    19200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0009989903 |             f
   193 |    19300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0019895200 |             f
   194 |    19400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022559882 |             f
   195 |    19500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0026615906 |             f
   196 |    19600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0002334188 |             f
   197 |    19700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0012037161 |             f
   198 |    19800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0017147930 |             f
   199 |    19900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0022623139 |             f
   200 |    20000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0025946461 |             f

================================================================================
PARETO FRONT RESULTS
================================================================================

Number of Pareto optimal solutions: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (First 15):
--------------------------------------------------------------------------------
 Solution       X1       X2         F1       F2  All_constraints_OK
        1 0.081992 0.132386   0.038267 48.11870                True
        2 4.797486 2.994308 127.092600  4.01500                True
        3 4.820903 2.993248 127.092600  4.01500                True
        4 0.124493 0.108412   0.038267 48.11870                True
        5 3.060195 2.942259  70.828400  7.92400                True
        6 0.200072 0.180240   0.234800 46.17870                True
        7 4.496139 2.993707 115.932600  4.18375                True
        8 4.677925 2.993113 123.292600  4.07625                True
        9 0.323048 0.196272   0.508400 44.74820                True
       10 3.430052 2.931841  81.148400  6.72485                True
       11 1.202342 0.973858   9.567000 30.76220                True
       12 3.277230 2.808117  74.378000  7.49370                True
       13 2.212310 2.438640  43.276600 14.36915                True
       14 2.203604 2.580823  45.372400 13.77035                True
       15 0.772330 0.853866   4.828400 35.12000                True

--------------------------------------------------------------------------------
Summary Statistics:
--------------------------------------------------------------------------------
X1 range: [0.082, 4.821]
X2 range: [0.021, 3.000]
F1 range: [0.038, 127.093]
F2 range: [4.015, 48.119]
Feasible solutions: 100 / 100

✓ Pareto front saved to 'pareto_front_results_dt_gc.csv'

================================================================================
ANALYZING ORIGINAL CSV DATA
================================================================================

Original CSV data points:
  Total points: 10201
  Feasible points: 9500
  Infeasible points: 701

================================================================================
GENERATING VISUALIZATIONS
================================================================================
✓ Visualization saved as 'pareto_front_two_plots_dt_gc.png'

================================================================================
OPTIMIZATION COMPLETE
================================================================================

Generated files:
  1. pareto_front_results_dt_gc.csv - Pareto optimal solutions
  2. pareto_front_two_plots_dt_gc.png - Objective and variable space plots

✓ F1 and F2 were obtained ONLY from CSV data (no analytical functions)
✓ Interpolation used to estimate objectives for new X1, X2 combinations
✓ The Pareto front shows optimal trade-offs based purely on your data
================================================================================
PASSED
pyomo/py/test_bnh_models_comparison.py::test_bnh_models_comparison 
PASSED
pyomo/py/test_glpk.py::test_glpk 
============================================================
OPTIMIZATION PROBLEM: Production Planning
============================================================

Solving optimization problem...

============================================================
OPTIMIZATION RESULTS
============================================================

Solver Status: ok
Termination Condition: optimal

Optimal Objective Value (Max Profit): $870.00

------------------------------------------------------------
Production Quantities:
------------------------------------------------------------
Product  Quantity  Profit  Total_Profit
      A  5.000000      30         150.0
      B 10.666667      45         480.0
      C  2.000000      50         100.0
      D  4.000000      35         140.0

------------------------------------------------------------
Resource Utilization:
------------------------------------------------------------
     Resource      Used  Capacity  Utilization_%
  labor_hours 60.000000       100      60.000000
machine_hours 37.333333        80      46.666667
storage_space 15.433333        50      30.866667

------------------------------------------------------------
Constraint Verification:
------------------------------------------------------------
Total Weight: 60.00 (limit: 60)
Product B / Product A ratio: 213.33% (min: 30%)

============================================================
To use with your own CSV files:
  products_data = pd.read_csv('your_products.csv')
  resources_data = pd.read_csv('your_resources.csv')
  requirements_data = pd.read_csv('your_requirements.csv')
============================================================
PASSED
pyomo/py/test_nsga2_df.py::test_nsga2 
================================================================================
NSGA-II MULTI-OBJECTIVE OPTIMIZATION with Pymoo
================================================================================

Setting up NSGA-II algorithm...
Running NSGA-II optimization...
  Population Size: 100
  Generations: 100
  Objectives: 3 (Profit, Emissions, Quality)
  Constraints: 9

==========================================================================================
n_gen  |  n_eval  | n_nds  |     cv_min    |     cv_avg    |      eps      |   indicator  
==========================================================================================
     1 |      100 |      1 |  0.000000E+00 |  1.405063E+02 |             - |             -
     2 |      200 |      1 |  0.000000E+00 |  5.871091E+01 |  0.000000E+00 |             f
     3 |      300 |      3 |  0.000000E+00 |  2.968131E+01 |  1.0000000000 |         ideal
     4 |      400 |     15 |  0.000000E+00 |  1.438383E+01 |  0.8663739049 |         ideal
     5 |      500 |     46 |  0.000000E+00 |  3.1895751178 |  0.0213632413 |         ideal
     6 |      600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0633385179 |         ideal
     7 |      700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.1675341084 |         ideal
     8 |      800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0971151477 |         nadir
     9 |      900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0173319830 |         ideal
    10 |     1000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0074915452 |         ideal
    11 |     1100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0831973273 |         ideal
    12 |     1200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0434253341 |         ideal
    13 |     1300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0177580761 |             f
    14 |     1400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0119907993 |             f
    15 |     1500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0292705914 |         ideal
    16 |     1600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0130508966 |         ideal
    17 |     1700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0365704115 |         ideal
    18 |     1800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0064704558 |         nadir
    19 |     1900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0099130916 |         ideal
    20 |     2000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0044571499 |         ideal
    21 |     2100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0127607467 |         ideal
    22 |     2200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0079197994 |         ideal
    23 |     2300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111494029 |             f
    24 |     2400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0110656748 |             f
    25 |     2500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0345565169 |         ideal
    26 |     2600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0150300846 |             f
    27 |     2700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0039750073 |         ideal
    28 |     2800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0056948934 |         nadir
    29 |     2900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0035248970 |         ideal
    30 |     3000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0117494893 |         nadir
    31 |     3100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0154072744 |             f
    32 |     3200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0115748516 |             f
    33 |     3300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0072931827 |         ideal
    34 |     3400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0094452834 |         ideal
    35 |     3500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0110703187 |             f
    36 |     3600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0132012233 |             f
    37 |     3700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0128807759 |             f
    38 |     3800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111748974 |             f
    39 |     3900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0134096917 |             f
    40 |     4000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0085329387 |             f
    41 |     4100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0121686302 |             f
    42 |     4200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0062587191 |         nadir
    43 |     4300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0107251897 |             f
    44 |     4400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0031146258 |         nadir
    45 |     4500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0130376021 |             f
    46 |     4600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0098589653 |             f
    47 |     4700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0128495561 |             f
    48 |     4800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0099560026 |             f
    49 |     4900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0042195308 |         ideal
    50 |     5000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0122619070 |             f
    51 |     5100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0120548593 |             f
    52 |     5200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0113353003 |             f
    53 |     5300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0107073630 |             f
    54 |     5400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0114403677 |             f
    55 |     5500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0109685667 |             f
    56 |     5600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0098758049 |             f
    57 |     5700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0122097522 |             f
    58 |     5800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0132137424 |             f
    59 |     5900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0116049992 |             f
    60 |     6000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0110956267 |             f
    61 |     6100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0098383333 |             f
    62 |     6200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0114612549 |             f
    63 |     6300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0141964468 |             f
    64 |     6400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0128061911 |             f
    65 |     6500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0101193021 |             f
    66 |     6600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0118854748 |             f
    67 |     6700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0138770225 |             f
    68 |     6800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0112770188 |             f
    69 |     6900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0141636146 |             f
    70 |     7000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0138517275 |             f
    71 |     7100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0101291967 |             f
    72 |     7200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0100351919 |             f
    73 |     7300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0138315250 |             f
    74 |     7400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0126369839 |             f
    75 |     7500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0127177096 |             f
    76 |     7600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0123849642 |             f
    77 |     7700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0094258837 |             f
    78 |     7800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0096888419 |             f
    79 |     7900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0144853994 |             f
    80 |     8000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0102574943 |             f
    81 |     8100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0138367986 |         nadir
    82 |     8200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0094277306 |             f
    83 |     8300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0083293585 |             f
    84 |     8400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0105128488 |             f
    85 |     8500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0157448417 |             f
    86 |     8600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0101479509 |         nadir
    87 |     8700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0101823888 |             f
    88 |     8800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0136433955 |             f
    89 |     8900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0118741224 |             f
    90 |     9000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0111319930 |             f
    91 |     9100 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0100575579 |             f
    92 |     9200 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0118796126 |             f
    93 |     9300 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0089559400 |             f
    94 |     9400 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0095083385 |             f
    95 |     9500 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0102531959 |             f
    96 |     9600 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0084108972 |             f
    97 |     9700 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0105037920 |             f
    98 |     9800 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0090552867 |             f
    99 |     9900 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0134454852 |             f
   100 |    10000 |    100 |  0.000000E+00 |  0.000000E+00 |  0.0386629614 |         nadir

================================================================================
OPTIMIZATION RESULTS - PARETO FRONT
================================================================================

Number of Pareto optimal solutions found: 100

--------------------------------------------------------------------------------
Pareto Optimal Solutions (Sample - First 10):
--------------------------------------------------------------------------------
 Solution     Profit  Carbon_Emissions  Avg_Quality
        1 868.291985        154.036554    87.399855
        2 827.334450        144.739932    83.924250
        3 812.221362        150.430919    88.665592
        4 525.153169         93.029359    86.072987
        5 868.911019        154.033937    87.386937
        6 764.078671        138.987875    87.833355
        7 801.278654        148.021527    88.544441
        8 806.617586        141.526360    84.204452
        9 854.619795        151.506018    87.329735
       10 738.401730        135.407842    88.028520

================================================================================
DETAILED ANALYSIS OF KEY SOLUTIONS
================================================================================

--------------------------------------------------------------------------------
Maximum Profit Solution:
--------------------------------------------------------------------------------
Profit: $868.91
Carbon Emissions: 154.03 units
Average Quality: 87.39

Production Quantities:
Product  Quantity  Profit  Emissions
      A      5.38  161.54      26.92
      B     10.36  466.05      82.85
      C      2.02  101.08      20.22
      D      4.01  140.25      24.04

Resource Utilization:
  labor_hours: 59.94 / 100 (59.9%)
  machine_hours: 37.16 / 80 (46.5%)
  storage_space: 15.40 / 50 (30.8%)

--------------------------------------------------------------------------------
Minimum Emissions Solution:
--------------------------------------------------------------------------------
Profit: $525.15
Carbon Emissions: 93.03 units
Average Quality: 86.07

Production Quantities:
Product  Quantity  Profit  Emissions
      A       5.0  150.00      25.00
      B       3.0  135.05      24.01
      C       2.0  100.10      20.02
      D       4.0  140.00      24.00

Resource Utilization:
  labor_hours: 37.01 / 100 (37.0%)
  machine_hours: 22.01 / 80 (27.5%)
  storage_space: 9.30 / 50 (18.6%)

--------------------------------------------------------------------------------
Maximum Quality Solution:
--------------------------------------------------------------------------------
Profit: $812.22
Carbon Emissions: 150.43 units
Average Quality: 88.67

Production Quantities:
Product  Quantity  Profit  Emissions
      A      5.00  150.09      25.02
      B      3.01  135.45      24.08
      C      7.73  386.67      77.33
      D      4.00  140.01      24.00

Resource Utilization:
  labor_hours: 59.97 / 100 (60.0%)
  machine_hours: 36.36 / 80 (45.4%)
  storage_space: 15.04 / 50 (30.1%)

================================================================================
TRADE-OFF ANALYSIS
================================================================================

Profit Range: $525.15 - $868.91
Emissions Range: 93.03 - 154.04 units
Quality Range: 83.92 - 88.67

Key Insights:
  • Maximizing profit results in 154.03 emissions
  • Minimizing emissions reduces profit to $525.15
  • High quality solution achieves 88.67 quality score

================================================================================
RECOMMENDATION
================================================================================
Use the Pareto front to select a solution based on your priorities:
  • High profit with acceptable environmental impact
  • Balance between all three objectives
  • Environmentally friendly with reasonable profit

Export pareto_df to CSV for further analysis or visualization.
================================================================================

================================================================================
GENERATING PARETO FRONT VISUALIZATIONS
================================================================================

✓ Pareto front visualizations saved as 'pareto_front_analysis.png'
✓ Optimal solutions comparison saved as 'optimal_solutions_comparison.png'

Visualization complete! Two PNG files have been generated:
  1. pareto_front_analysis.png - Multi-view Pareto front analysis
  2. optimal_solutions_comparison.png - Detailed comparison of key solutions
================================================================================
PASSED
pyomo/py/test_nsga2_mixed.py::test_nsga2_mixed 
==========================================================
n_gen  |  n_eval  | n_nds  |      eps      |   indicator  
==========================================================
     1 |      200 |     60 |             - |             -
     2 |      400 |    108 |  0.0042049841 |             f
     3 |      600 |    175 |  0.0029542472 |         nadir
     4 |      800 |    200 |  0.0087476680 |         nadir
     5 |     1000 |    200 |  0.0005978725 |             f
     6 |     1200 |    200 |  0.0011942070 |             f
     7 |     1400 |    200 |  0.0014457724 |             f
     8 |     1600 |    200 |  0.0018036864 |             f
     9 |     1800 |    200 |  0.0020209954 |             f
    10 |     2000 |    200 |  0.0020984509 |             f
    11 |     2200 |    200 |  0.0021674494 |             f
    12 |     2400 |    200 |  0.0022506652 |             f
    13 |     2600 |    200 |  0.0022396065 |             f
    14 |     2800 |    200 |  0.0023025077 |             f
    15 |     3000 |    200 |  0.0022226597 |             f
    16 |     3200 |    200 |  0.0021475266 |             f
    17 |     3400 |    200 |  0.0023017864 |             f
    18 |     3600 |    200 |  0.0022372250 |             f
    19 |     3800 |    200 |  0.0022157910 |             f
    20 |     4000 |    200 |  0.0023951164 |             f
    21 |     4200 |    200 |  0.0023439753 |             f
    22 |     4400 |    200 |  0.0022568071 |             f
    23 |     4600 |    200 |  0.0022619142 |             f
    24 |     4800 |    200 |  0.0022679477 |             f
    25 |     5000 |    200 |  0.0022303665 |             f
    26 |     5200 |    200 |  0.0022049692 |             f
    27 |     5400 |    200 |  0.0023266123 |             f
    28 |     5600 |    200 |  0.0023976508 |             f
    29 |     5800 |    200 |  0.0024228821 |             f
    30 |     6000 |    200 |  0.0024951809 |             f
    31 |     6200 |    200 |  0.0024960498 |             f
    32 |     6400 |    200 |  0.0024461566 |             f
    33 |     6600 |    200 |  0.0025842198 |             f
    34 |     6800 |    200 |  0.0003501300 |             f
    35 |     7000 |    200 |  0.0005665319 |             f
    36 |     7200 |    200 |  0.0007819341 |             f
    37 |     7400 |    200 |  0.0009748304 |             f
    38 |     7600 |    200 |  0.0010447385 |             f
    39 |     7800 |    200 |  0.0011152461 |             f
    40 |     8000 |    200 |  0.0011728017 |             f
    41 |     8200 |    200 |  0.0012879648 |             f
    42 |     8400 |    200 |  0.0013597746 |             f
    43 |     8600 |    200 |  0.0014055846 |             f
    44 |     8800 |    200 |  0.0014009590 |             f
    45 |     9000 |    200 |  0.0015084065 |             f
    46 |     9200 |    200 |  0.0015804164 |             f
    47 |     9400 |    200 |  0.0015767791 |             f
    48 |     9600 |    200 |  0.0016105678 |             f
    49 |     9800 |    200 |  0.0016724679 |             f
    50 |    10000 |    200 |  0.0016729697 |             f
    51 |    10200 |    200 |  0.0016229921 |             f
    52 |    10400 |    200 |  0.0016910234 |             f
    53 |    10600 |    200 |  0.0016701758 |             f
    54 |    10800 |    200 |  0.0016345966 |             f
    55 |    11000 |    200 |  0.0015908178 |             f
    56 |    11200 |    200 |  0.0016554186 |             f
    57 |    11400 |    200 |  0.0017518635 |             f
    58 |    11600 |    200 |  0.0016406202 |             f
    59 |    11800 |    200 |  0.0015967262 |             f
    60 |    12000 |    200 |  0.0016065238 |             f
    61 |    12200 |    200 |  0.0016432181 |             f
    62 |    12400 |    200 |  0.0016357686 |             f
    63 |    12600 |    200 |  0.0016861599 |             f
    64 |    12800 |    200 |  0.0017355475 |             f
    65 |    13000 |    200 |  0.0016305885 |             f
    66 |    13200 |    200 |  0.0016743403 |             f
    67 |    13400 |    200 |  0.0016765833 |             f
    68 |    13600 |    200 |  0.0016128383 |             f
    69 |    13800 |    200 |  0.0016082325 |             f
    70 |    14000 |    200 |  0.0016651272 |             f
    71 |    14200 |    200 |  0.0016807775 |             f
    72 |    14400 |    200 |  0.0016838505 |             f
    73 |    14600 |    200 |  0.0015880587 |             f
    74 |    14800 |    200 |  0.0015638964 |             f
    75 |    15000 |    200 |  0.0015795946 |             f
    76 |    15200 |    200 |  0.0016116861 |             f
    77 |    15400 |    200 |  0.0015844354 |             f
    78 |    15600 |    200 |  0.0015963635 |             f
    79 |    15800 |    200 |  0.0016438301 |             f
    80 |    16000 |    200 |  0.0016744381 |             f
    81 |    16200 |    200 |  0.0016930107 |             f
    82 |    16400 |    200 |  0.0015552151 |             f
    83 |    16600 |    200 |  0.0015334283 |             f
    84 |    16800 |    200 |  0.0016070666 |             f
    85 |    17000 |    200 |  0.0016267605 |             f
    86 |    17200 |    200 |  0.0015474487 |             f
    87 |    17400 |    200 |  0.0015544784 |             f
    88 |    17600 |    200 |  0.0015712639 |             f
    89 |    17800 |    200 |  0.0016166501 |             f
    90 |    18000 |    200 |  0.0016232793 |             f
    91 |    18200 |    200 |  0.0016160165 |             f
    92 |    18400 |    200 |  0.0014671493 |             f
    93 |    18600 |    200 |  0.0014845553 |             f
    94 |    18800 |    200 |  0.0015083051 |             f
    95 |    19000 |    200 |  0.0015714766 |             f
    96 |    19200 |    200 |  0.0014922642 |             f
    97 |    19400 |    200 |  0.0015699272 |             f
    98 |    19600 |    200 |  0.0014983372 |             f
    99 |    19800 |    200 |  0.0015446214 |             f
   100 |    20000 |    200 |  0.0015616296 |             f
   101 |    20200 |    200 |  0.0015901148 |             f
   102 |    20400 |    200 |  0.0015701406 |             f
   103 |    20600 |    200 |  0.0015718828 |             f
   104 |    20800 |    200 |  0.0016102288 |             f
   105 |    21000 |    200 |  0.0016649448 |             f
   106 |    21200 |    200 |  0.0017322247 |             f
   107 |    21400 |    200 |  0.0017242285 |             f
   108 |    21600 |    200 |  0.0017255144 |             f
   109 |    21800 |    200 |  0.0016686634 |             f
   110 |    22000 |    200 |  0.0016261893 |             f
   111 |    22200 |    200 |  0.0016879618 |             f
   112 |    22400 |    200 |  0.0017458972 |             f
   113 |    22600 |    200 |  0.0016950529 |             f
   114 |    22800 |    200 |  0.0016304107 |             f
   115 |    23000 |    200 |  0.0015853259 |             f
   116 |    23200 |    200 |  0.0015467562 |             f
   117 |    23400 |    200 |  0.0016582081 |             f
   118 |    23600 |    200 |  0.0015998071 |             f
   119 |    23800 |    200 |  0.0016337372 |             f
   120 |    24000 |    200 |  0.0016661636 |             f
   121 |    24200 |    200 |  0.0016839355 |             f
   122 |    24400 |    200 |  0.0016883097 |             f
   123 |    24600 |    200 |  0.0017053530 |             f
   124 |    24800 |    200 |  0.0016743775 |             f
   125 |    25000 |    200 |  0.0016190857 |             f
   126 |    25200 |    200 |  0.0016342496 |             f
   127 |    25400 |    200 |  0.0016560330 |             f
   128 |    25600 |    200 |  0.0015755668 |             f
   129 |    25800 |    200 |  0.0016078766 |             f
   130 |    26000 |    200 |  0.0016369152 |             f
   131 |    26200 |    200 |  0.0015820111 |             f
   132 |    26400 |    200 |  0.0016002532 |             f
   133 |    26600 |    200 |  0.0015789140 |             f
   134 |    26800 |    200 |  0.0015913541 |             f
   135 |    27000 |    200 |  0.0015412312 |             f
   136 |    27200 |    200 |  0.0015403721 |             f
   137 |    27400 |    200 |  0.0015369193 |             f
   138 |    27600 |    200 |  0.0015520522 |             f
   139 |    27800 |    200 |  0.0015514826 |             f
   140 |    28000 |    200 |  0.0015896001 |             f
   141 |    28200 |    200 |  0.0016216211 |             f
   142 |    28400 |    200 |  0.0015880663 |             f
   143 |    28600 |    200 |  0.0016167843 |             f
   144 |    28800 |    200 |  0.0016461931 |             f
   145 |    29000 |    200 |  0.0016400516 |             f
   146 |    29200 |    200 |  0.0016625656 |             f
   147 |    29400 |    200 |  0.0015879008 |             f
   148 |    29600 |    200 |  0.0015960134 |             f
   149 |    29800 |    200 |  0.0016064131 |             f
   150 |    30000 |    200 |  0.0016119688 |             f
   151 |    30200 |    200 |  0.0016139802 |             f
   152 |    30400 |    200 |  0.0015988627 |             f
   153 |    30600 |    200 |  0.0016119974 |             f
   154 |    30800 |    200 |  0.0017040823 |             f
   155 |    31000 |    200 |  0.0016403129 |             f
   156 |    31200 |    200 |  0.0016583403 |             f
   157 |    31400 |    200 |  0.0016923945 |             f
   158 |    31600 |    200 |  0.0016461758 |             f
   159 |    31800 |    200 |  0.0015933194 |             f
   160 |    32000 |    200 |  0.0015336003 |             f
   161 |    32200 |    200 |  0.0014858073 |             f
   162 |    32400 |    200 |  0.0014486090 |             f
   163 |    32600 |    200 |  0.0015538812 |             f
   164 |    32800 |    200 |  0.0015779931 |             f
   165 |    33000 |    200 |  0.0015672054 |             f
   166 |    33200 |    200 |  0.0015934514 |             f
   167 |    33400 |    200 |  0.0015553133 |             f
   168 |    33600 |    200 |  0.0015700553 |             f
   169 |    33800 |    200 |  0.0015326715 |             f
   170 |    34000 |    200 |  0.0014760015 |             f
   171 |    34200 |    200 |  0.0014416320 |             f
   172 |    34400 |    200 |  0.0015117406 |             f
   173 |    34600 |    200 |  0.0014733734 |             f
   174 |    34800 |    200 |  0.0014848592 |             f
   175 |    35000 |    200 |  0.0014908640 |             f
   176 |    35200 |    200 |  0.0014814745 |             f
   177 |    35400 |    200 |  0.0015161258 |             f
   178 |    35600 |    200 |  0.0014810795 |             f
   179 |    35800 |    200 |  0.0014323076 |             f
   180 |    36000 |    200 |  0.0014887668 |             f
   181 |    36200 |    200 |  0.0015353075 |             f
   182 |    36400 |    200 |  0.0016036803 |             f
   183 |    36600 |    200 |  0.0016489457 |             f
   184 |    36800 |    200 |  0.0016350151 |             f
   185 |    37000 |    200 |  0.0016063964 |             f
   186 |    37200 |    200 |  0.0016832150 |             f
   187 |    37400 |    200 |  0.0017379206 |             f
   188 |    37600 |    200 |  0.0016713278 |             f
   189 |    37800 |    200 |  0.0015874822 |             f
   190 |    38000 |    200 |  0.0015932995 |             f
   191 |    38200 |    200 |  0.0014546722 |             f
   192 |    38400 |    200 |  0.0015887585 |             f
   193 |    38600 |    200 |  0.0015215466 |             f
   194 |    38800 |    200 |  0.0015423558 |             f
   195 |    39000 |    200 |  0.0015215547 |             f
   196 |    39200 |    200 |  0.0016065630 |             f
   197 |    39400 |    200 |  0.0015456258 |             f
   198 |    39600 |    200 |  0.0016135564 |             f
   199 |    39800 |    200 |  0.0016729092 |             f
   200 |    40000 |    200 |  0.0016503253 |             f
   201 |    40200 |    200 |  0.0016615084 |             f
   202 |    40400 |    200 |  0.0017061426 |             f
   203 |    40600 |    200 |  0.0016682311 |             f
   204 |    40800 |    200 |  0.0017029948 |             f
   205 |    41000 |    200 |  0.0016533659 |             f
   206 |    41200 |    200 |  0.0016700925 |             f
   207 |    41400 |    200 |  0.0017516611 |             f
   208 |    41600 |    200 |  0.0017328575 |             f
   209 |    41800 |    200 |  0.0016867957 |             f
   210 |    42000 |    200 |  0.0016404901 |             f
   211 |    42200 |    200 |  0.0015964862 |             f
   212 |    42400 |    200 |  0.0015787529 |             f
   213 |    42600 |    200 |  0.0016260093 |             f
   214 |    42800 |    200 |  0.0016305090 |             f
   215 |    43000 |    200 |  0.0016469764 |             f
   216 |    43200 |    200 |  0.0016540204 |             f
   217 |    43400 |    200 |  0.0015191264 |             f
   218 |    43600 |    200 |  0.0016039113 |             f
   219 |    43800 |    200 |  0.0015327281 |             f
   220 |    44000 |    200 |  0.0015924137 |             f
   221 |    44200 |    200 |  0.0016536925 |             f
   222 |    44400 |    200 |  0.0016173207 |             f
   223 |    44600 |    200 |  0.0016283930 |             f
   224 |    44800 |    200 |  0.0016040633 |             f
   225 |    45000 |    200 |  0.0015763778 |             f
   226 |    45200 |    200 |  0.0015685508 |             f
   227 |    45400 |    200 |  0.0015880584 |             f
   228 |    45600 |    200 |  0.0015280517 |             f
   229 |    45800 |    200 |  0.0015101177 |             f
   230 |    46000 |    200 |  0.0015657568 |             f
   231 |    46200 |    200 |  0.0015099021 |             f
   232 |    46400 |    200 |  0.0015836673 |             f
   233 |    46600 |    200 |  0.0016308552 |             f
   234 |    46800 |    200 |  0.0016238428 |             f
   235 |    47000 |    200 |  0.0016398258 |             f
   236 |    47200 |    200 |  0.0015420645 |             f
   237 |    47400 |    200 |  0.0015592705 |             f
   238 |    47600 |    200 |  0.0015430983 |             f
   239 |    47800 |    200 |  0.0015940483 |             f
   240 |    48000 |    200 |  0.0014828229 |             f
   241 |    48200 |    200 |  0.0015821291 |             f
   242 |    48400 |    200 |  0.0015635239 |             f
   243 |    48600 |    200 |  0.0015491296 |             f
   244 |    48800 |    200 |  0.0015602531 |             f
   245 |    49000 |    200 |  0.0015443552 |             f
   246 |    49200 |    200 |  0.0015273844 |             f
   247 |    49400 |    200 |  0.0014763486 |             f
   248 |    49600 |    200 |  0.0015326284 |             f
   249 |    49800 |    200 |  0.0015546954 |             f
   250 |    50000 |    200 |  0.0015545825 |             f

======================================================================
OPTIMIZATION RESULTS
======================================================================

Total Pareto front solutions: 200

======================================================================
SOLUTIONS BY CATEGORY
======================================================================

Category A: 76 solutions
  X1= 0.000  →  F1=  2.000, F2= 40.000
  X1= 0.100  →  F1=  2.003, F2= 39.749
  X1= 0.264  →  F1=  2.021, F2= 39.339
  X1= 0.358  →  F1=  2.039, F2= 39.104
  X1= 0.459  →  F1=  2.063, F2= 38.853
  X1= 0.488  →  F1=  2.071, F2= 38.780
  X1= 0.544  →  F1=  2.089, F2= 38.639
  X1= 0.599  →  F1=  2.108, F2= 38.503
  ... and 68 more solutions

Category B: 54 solutions
  X1= 3.000  →  F1=  5.000, F2= 29.000
  X1= 3.062  →  F1=  5.001, F2= 28.876
  X1= 3.188  →  F1=  5.005, F2= 28.625
  X1= 3.324  →  F1=  5.016, F2= 28.351
  X1= 3.390  →  F1=  5.023, F2= 28.220
  X1= 3.501  →  F1=  5.038, F2= 27.997
  X1= 3.549  →  F1=  5.045, F2= 27.902
  X1= 3.648  →  F1=  5.063, F2= 27.704
  ... and 46 more solutions

Category C: 35 solutions
  X1= 7.000  →  F1=  7.000, F2= 19.500
  X1= 7.296  →  F1=  7.013, F2= 19.056
  X1= 7.507  →  F1=  7.039, F2= 18.739
  X1= 7.703  →  F1=  7.074, F2= 18.445
  X1= 7.894  →  F1=  7.120, F2= 18.159
  X1= 7.980  →  F1=  7.144, F2= 18.030
  X1= 8.062  →  F1=  7.169, F2= 17.907
  X1= 8.158  →  F1=  7.201, F2= 17.763
  ... and 27 more solutions

Category D: 35 solutions
  X1=10.000  →  F1= 10.000, F2=  3.200
  X1= 9.934  →  F1= 10.066, F2=  3.122
  X1= 9.919  →  F1= 10.081, F2=  3.104
  X1= 9.865  →  F1= 10.135, F2=  3.044
  X1= 9.797  →  F1= 10.203, F2=  2.969
  X1= 9.744  →  F1= 10.256, F2=  2.912
  X1= 9.704  →  F1= 10.296, F2=  2.871
  X1= 9.659  →  F1= 10.341, F2=  2.826
  ... and 27 more solutions
✓ Optimal solutions comparison saved as /tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples/pyomo/py/nsga2_mixed_models_comparison.png

======================================================================
STATISTICAL SUMMARY
======================================================================

Category A: 76/200 solutions (38.0%)
  X1  → min:  0.00, max:  3.16, mean:  1.89, std:  0.87
  F1  → min:   2.00, max:   5.00, mean:   3.30
  F2  → min:  32.10, max:  40.00, mean:  35.28

Category B: 54/200 solutions (27.0%)
  X1  → min:  3.00, max:  6.65, mean:  5.01, std:  1.06
  F1  → min:   5.00, max:   7.00, mean:   5.78
  F2  → min:  21.70, max:  29.00, mean:  24.98

Category C: 35/200 solutions (17.5%)
  X1  → min:  7.00, max: 10.00, mean:  8.86, std:  0.80
  F1  → min:   7.00, max:   8.35, mean:   7.62
  F2  → min:  15.00, max:  19.50, mean:  16.71

Category D: 35/200 solutions (17.5%)
  X1  → min:  8.00, max: 10.00, mean:  9.06, std:  0.57
  F1  → min:  10.00, max:  12.00, mean:  10.94
  F2  → min:   2.00, max:   3.20, mean:   2.44

======================================================================
PROBLEM DESIGN - HOW CATEGORIES COMPETE
======================================================================
Category A: Dominates LEFT side (low F1, high F2)
Category B: Competitive in MID-LEFT region
Category C: Competitive in MID-RIGHT region
Category D: Dominates RIGHT side (high F1, low F2)

Each category is non-dominated in its specialized region!
PASSED
pyomo/py/test_nsga3_mixed.py::test_nsga3_mixed 
Number of reference directions: 496
WARNING: pop_size=300 is less than the number of reference directions ref_dirs=496.
This might cause unwanted behavior of the algorithm. 
Please make sure pop_size is equal or larger than the number of reference directions. 
==========================================================
n_gen  |  n_eval  | n_nds  |      eps      |   indicator  
==========================================================
     1 |      300 |     39 |             - |             -
     2 |      600 |     44 |  0.0367877232 |         nadir
     3 |      900 |     44 |  0.0023070974 |             f
     4 |     1200 |     45 |  0.0035211125 |             f
     5 |     1500 |     45 |  0.0002588012 |             f
     6 |     1800 |     45 |  0.0045527970 |         nadir
     7 |     2100 |     45 |  0.0010665891 |             f
     8 |     2400 |     45 |  0.0004567215 |             f
     9 |     2700 |     45 |  0.0006302142 |             f
    10 |     3000 |     45 |  0.0008078675 |             f
    11 |     3300 |     45 |  0.0009062124 |             f
    12 |     3600 |     45 |  0.0010105192 |             f
    13 |     3900 |     45 |  0.0011465905 |             f
    14 |     4200 |     46 |  0.0014756638 |             f
    15 |     4500 |     46 |  0.0015875214 |             f
    16 |     4800 |     46 |  0.0015736692 |             f
    17 |     5100 |     46 |  0.0014561735 |             f
    18 |     5400 |     46 |  0.0014205093 |             f
    19 |     5700 |     46 |  0.0013584048 |             f
    20 |     6000 |     46 |  0.0016013582 |             f
    21 |     6300 |     46 |  0.0016759459 |             f
    22 |     6600 |     46 |  0.0016560576 |             f
    23 |     6900 |     46 |  0.0015481683 |             f
    24 |     7200 |     46 |  0.0016176573 |             f
    25 |     7500 |     46 |  0.0019166319 |             f
    26 |     7800 |     46 |  0.0018274352 |             f
    27 |     8100 |     46 |  0.0015496038 |             f
    28 |     8400 |     46 |  0.0015530363 |             f
    29 |     8700 |     46 |  0.0015629162 |             f
    30 |     9000 |     46 |  0.0015252842 |             f
    31 |     9300 |     46 |  0.0014853959 |             f
    32 |     9600 |     46 |  0.0014853959 |             f
    33 |     9900 |     46 |  0.0015135522 |             f
    34 |    10200 |     46 |  0.0016201857 |             f
    35 |    10500 |     46 |  0.0016700199 |             f
    36 |    10800 |     46 |  0.0016692338 |             f
    37 |    11100 |     46 |  0.0016343289 |             f
    38 |    11400 |     46 |  0.0016316568 |             f
    39 |    11700 |     46 |  0.0016500208 |             f
    40 |    12000 |     46 |  0.0015665913 |             f
    41 |    12300 |     46 |  0.0022594546 |             f
    42 |    12600 |     46 |  0.0023660138 |             f
    43 |    12900 |     46 |  0.0016301250 |             f
    44 |    13200 |     46 |  0.0017648354 |             f
    45 |    13500 |     46 |  0.0021761102 |             f
    46 |    13800 |     46 |  0.0017023525 |             f
    47 |    14100 |     46 |  0.0016610817 |             f
    48 |    14400 |     46 |  0.0015453881 |             f
    49 |    14700 |     46 |  0.0014737412 |             f
    50 |    15000 |     46 |  0.0015200052 |             f
    51 |    15300 |     46 |  0.0015156335 |             f
    52 |    15600 |     46 |  0.0015324574 |             f
    53 |    15900 |     46 |  0.0015246837 |             f
    54 |    16200 |     46 |  0.0016105324 |             f
    55 |    16500 |     46 |  0.0015776779 |             f
    56 |    16800 |     46 |  0.0015809102 |             f
    57 |    17100 |     46 |  0.0015809913 |             f
    58 |    17400 |     46 |  0.0015605588 |             f
    59 |    17700 |     46 |  0.0017063410 |             f
    60 |    18000 |     46 |  0.0017272036 |             f
    61 |    18300 |     46 |  0.0016520806 |             f
    62 |    18600 |     46 |  0.0016548055 |             f
    63 |    18900 |     46 |  0.0016773627 |             f
    64 |    19200 |     46 |  0.0016092955 |             f
    65 |    19500 |     46 |  0.0016168405 |             f
    66 |    19800 |     46 |  0.0015565709 |             f
    67 |    20100 |     46 |  0.0016979611 |             f
    68 |    20400 |     46 |  0.0016399444 |             f
    69 |    20700 |     46 |  0.0018661643 |             f
    70 |    21000 |     46 |  0.0016330749 |             f
    71 |    21300 |     46 |  0.0016494453 |             f
    72 |    21600 |     46 |  0.0015073257 |             f
    73 |    21900 |     46 |  0.0014400363 |             f
    74 |    22200 |     46 |  0.0014930726 |             f
    75 |    22500 |     46 |  0.0014494946 |             f
    76 |    22800 |     46 |  0.0014338615 |             f
    77 |    23100 |     46 |  0.0014808133 |             f
    78 |    23400 |     46 |  0.0014629162 |             f
    79 |    23700 |     46 |  0.0014947193 |             f
    80 |    24000 |     46 |  0.0014340734 |             f
    81 |    24300 |     46 |  0.0014340734 |             f
    82 |    24600 |     46 |  0.0025364323 |             f
    83 |    24900 |     46 |  0.0012447383 |             f
    84 |    25200 |     46 |  0.0012328370 |             f
    85 |    25500 |     46 |  0.0010175412 |             f
    86 |    25800 |     46 |  0.0010701498 |             f
    87 |    26100 |     46 |  0.0012902771 |             f
    88 |    26400 |     46 |  0.0012859992 |             f
    89 |    26700 |     46 |  0.0008316357 |             f
    90 |    27000 |     46 |  0.0012158959 |             f
    91 |    27300 |     46 |  0.0012753951 |             f
    92 |    27600 |     46 |  0.0012968897 |             f
    93 |    27900 |     46 |  0.0010608956 |             f
    94 |    28200 |     46 |  0.0012640060 |             f
    95 |    28500 |     46 |  0.0012852629 |             f
    96 |    28800 |     46 |  0.0013147510 |             f
    97 |    29100 |     46 |  0.0010473550 |             f
    98 |    29400 |     46 |  0.0005656195 |             f
    99 |    29700 |     46 |  0.0010579516 |             f
   100 |    30000 |     46 |  0.0011782627 |             f
   101 |    30300 |     46 |  0.0011687199 |             f
   102 |    30600 |     46 |  0.0011164488 |             f
   103 |    30900 |     46 |  0.0011210855 |             f
   104 |    31200 |     46 |  0.0007926707 |             f
   105 |    31500 |     46 |  0.0010683340 |             f
   106 |    31800 |     46 |  0.0012095387 |             f
   107 |    32100 |     46 |  0.0011368988 |             f
   108 |    32400 |     46 |  0.0011422944 |             f
   109 |    32700 |     46 |  0.0011434130 |             f
   110 |    33000 |     46 |  0.0011434130 |             f
   111 |    33300 |     46 |  0.0011128038 |             f
   112 |    33600 |     46 |  0.0011128038 |             f
   113 |    33900 |     46 |  0.0011226669 |             f
   114 |    34200 |     46 |  0.0010345609 |             f
   115 |    34500 |     46 |  0.0006264288 |             f
   116 |    34800 |     46 |  0.0006725700 |             f
   117 |    35100 |     46 |  0.0009520528 |             f
   118 |    35400 |     46 |  0.0006200654 |             f
   119 |    35700 |     46 |  0.0005550373 |             f
   120 |    36000 |     46 |  0.0008920785 |             f
   121 |    36300 |     46 |  0.0009064358 |             f
   122 |    36600 |     46 |  0.0008956362 |             f
   123 |    36900 |     46 |  0.0008973969 |             f
   124 |    37200 |     46 |  0.0008825462 |             f
   125 |    37500 |     46 |  0.0008902132 |             f
   126 |    37800 |     46 |  0.0009480907 |             f
   127 |    38100 |     46 |  0.0013382278 |             f
   128 |    38400 |     46 |  0.0012578132 |             f
   129 |    38700 |     46 |  0.0014125891 |             f
   130 |    39000 |     46 |  0.0013939285 |             f
   131 |    39300 |     46 |  0.0014015469 |             f
   132 |    39600 |     46 |  0.0013654022 |             f
   133 |    39900 |     46 |  0.0013781044 |             f
   134 |    40200 |     46 |  0.0013969025 |             f
   135 |    40500 |     46 |  0.0013646166 |             f
   136 |    40800 |     46 |  0.0012570711 |             f
   137 |    41100 |     46 |  0.0012901289 |             f
   138 |    41400 |     46 |  0.0012519292 |             f
   139 |    41700 |     46 |  0.0012834063 |             f
   140 |    42000 |     46 |  0.0006647590 |             f
   141 |    42300 |     46 |  0.0011822516 |             f
   142 |    42600 |     46 |  0.0005390969 |             f
   143 |    42900 |     46 |  0.0011319068 |             f
   144 |    43200 |     46 |  0.0011636616 |             f
   145 |    43500 |     46 |  0.0012064205 |             f
   146 |    43800 |     46 |  0.0011716834 |             f
   147 |    44100 |     46 |  0.0010626471 |             f
   148 |    44400 |     46 |  0.0014415253 |             f
   149 |    44700 |     46 |  0.0014420687 |             f
   150 |    45000 |     46 |  0.0013730785 |             f
   151 |    45300 |     46 |  0.0012711345 |             f
   152 |    45600 |     46 |  0.0012397543 |             f
   153 |    45900 |     46 |  0.0007766753 |             f
   154 |    46200 |     46 |  0.0009570425 |             f
   155 |    46500 |     46 |  0.0006843118 |             f
   156 |    46800 |     46 |  0.0006843118 |             f
   157 |    47100 |     46 |  0.0011511462 |             f
   158 |    47400 |     46 |  0.0011511462 |             f
   159 |    47700 |     46 |  0.0011324964 |             f
   160 |    48000 |     46 |  0.0010871108 |             f
   161 |    48300 |     46 |  0.0010489748 |             f
   162 |    48600 |     46 |  0.0010057935 |             f
   163 |    48900 |     46 |  0.0012065121 |             f
   164 |    49200 |     46 |  0.0012280625 |             f
   165 |    49500 |     46 |  0.0012026463 |             f
   166 |    49800 |     46 |  0.0012114676 |             f
   167 |    50100 |     46 |  0.0013449695 |             f
   168 |    50400 |     46 |  0.0010337396 |             f
   169 |    50700 |     46 |  0.0009926450 |             f
   170 |    51000 |     46 |  0.0009870506 |             f
   171 |    51300 |     46 |  0.0008035187 |             f
   172 |    51600 |     46 |  0.0010128048 |             f
   173 |    51900 |     46 |  0.0010143565 |             f
   174 |    52200 |     46 |  0.0010047561 |             f
   175 |    52500 |     46 |  0.0012381813 |             f
   176 |    52800 |     46 |  0.0012408373 |             f
   177 |    53100 |     46 |  0.0011785313 |             f
   178 |    53400 |     46 |  0.0007695675 |             f
   179 |    53700 |     46 |  0.0012478106 |             f
   180 |    54000 |     46 |  0.0011783255 |             f
   181 |    54300 |     46 |  0.0014339288 |             f
   182 |    54600 |     46 |  0.0010375418 |             f
   183 |    54900 |     46 |  0.0010397665 |             f
   184 |    55200 |     46 |  0.0010577322 |             f
   185 |    55500 |     46 |  0.0011056501 |             f
   186 |    55800 |     46 |  0.0011015840 |             f
   187 |    56100 |     46 |  0.0011099389 |             f
   188 |    56400 |     46 |  0.0011101310 |             f
   189 |    56700 |     46 |  0.0009737514 |             f
   190 |    57000 |     46 |  0.0010159831 |             f
   191 |    57300 |     46 |  0.0008680937 |             f
   192 |    57600 |     46 |  0.0008706271 |             f
   193 |    57900 |     46 |  0.0014384755 |             f
   194 |    58200 |     46 |  0.0014309141 |             f
   195 |    58500 |     46 |  0.0013884342 |             f
   196 |    58800 |     46 |  0.0010111501 |             f
   197 |    59100 |     46 |  0.0010111501 |             f
   198 |    59400 |     46 |  0.0009869646 |             f
   199 |    59700 |     46 |  0.0014952559 |             f
   200 |    60000 |     46 |  0.0014263084 |             f
   201 |    60300 |     46 |  0.0013621052 |             f
   202 |    60600 |     46 |  0.0013651931 |             f
   203 |    60900 |     46 |  0.0013310257 |             f
   204 |    61200 |     46 |  0.0012626045 |             f
   205 |    61500 |     46 |  0.0012693543 |             f
   206 |    61800 |     46 |  0.0011441628 |             f
   207 |    62100 |     46 |  0.0010207100 |             f
   208 |    62400 |     46 |  0.0009782996 |             f
   209 |    62700 |     46 |  0.0012633389 |             f
   210 |    63000 |     46 |  0.0013235404 |             f
   211 |    63300 |     46 |  0.0012633389 |             f
   212 |    63600 |     46 |  0.0013222494 |             f
   213 |    63900 |     46 |  0.0012650444 |             f
   214 |    64200 |     46 |  0.0013376445 |             f
   215 |    64500 |     46 |  0.0012856680 |             f
   216 |    64800 |     46 |  0.0013870067 |             f
   217 |    65100 |     46 |  0.0014378990 |             f
   218 |    65400 |     46 |  0.0013867401 |             f
   219 |    65700 |     46 |  0.0013141429 |             f
   220 |    66000 |     46 |  0.0008749459 |             f
   221 |    66300 |     46 |  0.0013190917 |             f
   222 |    66600 |     46 |  0.0009502098 |             f
   223 |    66900 |     46 |  0.0009846626 |             f
   224 |    67200 |     46 |  0.0011712286 |             f
   225 |    67500 |     46 |  0.0011542957 |             f
   226 |    67800 |     46 |  0.0011693778 |             f
   227 |    68100 |     46 |  0.0011404627 |             f
   228 |    68400 |     46 |  0.0011774367 |             f
   229 |    68700 |     46 |  0.0008625515 |             f
   230 |    69000 |     46 |  0.0007798842 |             f
   231 |    69300 |     46 |  0.0013864984 |             f
   232 |    69600 |     46 |  0.0013396409 |             f
   233 |    69900 |     46 |  0.0011004505 |             f
   234 |    70200 |     46 |  0.0013905947 |             f
   235 |    70500 |     46 |  0.0013521965 |             f
   236 |    70800 |     46 |  0.0013570311 |             f
   237 |    71100 |     46 |  0.0010781287 |             f
   238 |    71400 |     46 |  0.0013187361 |             f
   239 |    71700 |     46 |  0.0013421438 |             f
   240 |    72000 |     46 |  0.0012921592 |             f
   241 |    72300 |     46 |  0.0013091531 |             f
   242 |    72600 |     46 |  0.0013381189 |             f
   243 |    72900 |     46 |  0.0013253034 |             f
   244 |    73200 |     46 |  0.0013225646 |             f
   245 |    73500 |     46 |  0.0013225646 |             f
   246 |    73800 |     46 |  0.0013458922 |             f
   247 |    74100 |     46 |  0.0013134070 |             f
   248 |    74400 |     46 |  0.0014722060 |             f
   249 |    74700 |     46 |  0.0014722060 |             f
   250 |    75000 |     46 |  0.0014385116 |             f

======================================================================
OPTIMIZATION RESULTS
======================================================================

Total Pareto front solutions: 46

======================================================================
SOLUTIONS BY CATEGORY
======================================================================

Category A: 12 solutions
  X1= 0.000  →  F1=  2.000, F2= 32.500, F3= 40.000
  X1= 1.253  →  F1=  2.471, F2= 27.022, F3= 36.869
  X1= 2.313  →  F1=  3.605, F2= 23.610, F3= 34.218
  X1= 3.146  →  F1=  4.970, F2= 21.718, F3= 32.134
  X1= 3.750  →  F1=  6.219, F2= 20.781, F3= 30.625
  X1= 4.374  →  F1=  7.740, F2= 20.196, F3= 29.064
  X1= 4.759  →  F1=  8.793, F2= 20.029, F3= 28.103
  X1= 4.959  →  F1=  9.378, F2= 20.001, F3= 27.602
  ... and 4 more solutions

Category B: 17 solutions
  X1= 3.159  →  F1= 15.005, F2=  6.993, F3= 28.681
  X1= 2.552  →  F1= 15.040, F2=  5.606, F3= 29.895
  X1= 3.476  →  F1= 15.045, F2=  7.834, F3= 28.047
  X1= 3.689  →  F1= 15.095, F2=  8.444, F3= 27.622
  X1= 3.979  →  F1= 15.192, F2=  9.333, F3= 27.042
  X1= 4.178  →  F1= 15.278, F2=  9.984, F3= 26.643
  X1= 1.776  →  F1= 15.299, F2=  4.262, F3= 31.447
  X1= 4.450  →  F1= 15.420, F2= 10.920, F3= 26.101
  ... and 9 more solutions

Category C: 8 solutions
  X1= 7.634  →  F1= 18.548, F2= 18.801, F3= 22.399
  X1= 7.338  →  F1= 18.993, F2= 18.537, F3= 20.846
  X1= 6.787  →  F1= 19.820, F2= 18.186, F3= 18.120
  X1= 4.012  →  F1= 23.982, F2= 19.186, F3=  7.634
  X1= 3.230  →  F1= 25.155, F2= 20.301, F3=  5.652
  X1= 2.550  →  F1= 26.176, F2= 21.572, F3=  4.275
  X1= 1.425  →  F1= 27.862, F2= 24.278, F3=  2.711
  X1= 0.000  →  F1= 30.000, F2= 28.800, F3=  2.000

Category D: 9 solutions
  X1= 7.373  →  F1= 20.035, F2= 18.728, F3=  8.414
  X1= 6.511  →  F1= 20.060, F2= 20.281, F3=  6.891
  X1= 7.863  →  F1= 20.186, F2= 17.847, F3=  9.476
  X1= 8.098  →  F1= 20.301, F2= 17.424, F3= 10.038
  X1= 8.525  →  F1= 20.581, F2= 16.656, F3= 11.142
  X1= 5.243  →  F1= 20.772, F2= 22.563, F3=  5.463
  X1= 8.761  →  F1= 20.775, F2= 16.231, F3= 11.799
  X1= 9.150  →  F1= 21.156, F2= 15.530, F3= 12.958
  ... and 1 more solutions
✓ Optimal solutions comparison saved as /tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples/pyomo/py/nsga3_mixed_models_comparison.png
PASSED
s2_rx_anonym/py/test_s2_rx_anonym.py::test_s2_rx_anonym_csv 
Data shape: (6809448, 11)
Columns: ['CH', 'RANK', 'Byte', 'p0', 'p1', 'p2', 'p3', 'p4', 'p5', 'o0', 'o1']

First few rows:
   CH  RANK  Byte  p0  p1  p2  p3  p4  p5   o0    o1
0   0     0     0  40  56  60   9   0   0 -1.0   0.0
1   0     0     0  40  56  60   9   0   0 -1.0   5.0
2   0     0     0  40  56  60   9   0   0 -1.0  10.0
3   0     0     0  40  56  60   9   0   0 -1.0  15.0
4   0     0     0  40  56  60   9   0   0 -1.0  20.0

Number of unique fixed parameter combinations: 32

Training surrogate models...
max_depth=17; n_estimators=18; min_samples_split=20
Model o0 R² score: 0.2315
Model o1 R² score: 0.1007
PASSED
s2_tx_anonym/py/test_s2_tx_anonym.py::test_s2_tx_anonym_csv 
Data shape: (2240060, 9)
Columns: ['CH', 'RANK', 'Byte', 'p0', 'p1', 'p2', 'p3', 'o0', 'o1']

First few rows:
   CH  RANK  Byte  p0  p1  p2  p3    o0    o1
0   0     0     0  15   3  60  40 -8.25   0.0
1   0     0     0  15   3  60  40 -8.25   4.0
2   0     0     0  15   3  60  40 -8.25   8.0
3   0     0     0  15   3  60  40 -8.25  12.0
4   0     0     0  15   3  60  40 -8.25  16.0

Number of unique fixed parameter combinations: 32

Training surrogate models...
max_depth=17; n_estimators=18; min_samples_split=20
Model o0 R² score: 0.3885
Model o1 R² score: 0.0994
PASSED
shekel/py/test_shgo_shekel.py::test_optimization_ex2 
Optimizing Shekel function using SHGO algorithm...
============================================================

Optimization Results:
Success: True
Message: Optimization terminated successfully.

Optimal solution (x*):
  x = [4.00074686 3.99950947 4.00074686 3.99950947]

Optimal function value (f(x*)):
  f(x*) = -10.5364431535

Number of function evaluations: 2254
Number of iterations: 5

============================================================
Note: The known global minimum is approximately:
  x* ≈ [4, 4, 4, 4]
  f(x*) ≈ -10.5363

============================================================
Function evaluations at test points:
  f([4.0, 4.0, 4.0, 4.0]) = -10.536284
  f([1.2, 2.0, 3.2, 4.0]) = -0.323428
  f([8.0, 7.2, 6.0, 4.8]) = -0.416895
Creating CSV with 456976 points...
Grid size: 26 points per dimension
Total points: 456976

CSV file '/tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples/shekel/py/shekel_meshgrid_26.csv' created successfully!
Total rows: 456977 (including header)

Minimum value found in grid:
  X = [4. 4. 4. 4.]
  Y = -10.5362837262
PASSED

=============================== warnings summary ===============================
../../../../../usr/lib/python3/dist-packages/z3/z3core.py:5
  /usr/lib/python3/dist-packages/z3/z3core.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    import pkg_resources

c3dtlz4/py/test_gpsampler.py::test_gpsampler
  /tmp/smlp_tutorial_mdmitry_494180/smlp/tutorial/examples/c3dtlz4/py/c3dtlz4_ex.py:38: ExperimentalWarning: GPSampler is experimental (supported from v3.6.0). The interface can change in the future.
    sampler=optuna.samplers.GPSampler(seed=42, constraints_func=c3dtlz4.constraints_func, deterministic_objective=True),

c3dtlz4/py/test_gpsampler.py::test_gpsampler
  /usr/local/lib/python3.12/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``constraints_func`` is an experimental feature. The interface can change in the future.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========== 18 passed, 8 deselected, 3 warnings in 568.99s (0:09:28) ===========
